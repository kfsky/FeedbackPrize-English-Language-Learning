Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|                                                                                                                                                                                                                                              | 0/3911 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3911/3911 [00:04<00:00, 948.41it/s]
[2022-11-07 12:43:55] - comment: deberta-v2-xlarge exp039 10fold
[2022-11-07 12:43:55] - max_len: 2048
[2022-11-07 12:43:55] - ========== fold: 0 training ==========
[2022-11-07 12:43:55] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v2-xlarge",
  "attention_dropout": 0.0,
  "attention_head_size": 64,
  "attention_probs_dropout_prob": 0.0,
  "conv_act": "gelu",
  "conv_kernel_size": 3,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 24,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1536,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Reinitializing Last 1 Layers ...
Done.!
Epoch: [1][0/220] Elapsed 0m 5s (remain 18m 54s) Loss: 3.2085(3.2085) Grad: inf  LR: 0.00000200
Epoch: [1][100/220] Elapsed 4m 45s (remain 5m 36s) Loss: 0.0950(0.7231) Grad: 36343.4570  LR: 0.00000200
Epoch: [1][200/220] Elapsed 9m 23s (remain 0m 53s) Loss: 0.1052(0.4225) Grad: 45766.5352  LR: 0.00000200
Epoch: [1][219/220] Elapsed 10m 18s (remain 0m 0s) Loss: 0.1510(0.3964) Grad: 64610.0273  LR: 0.00000129
EVAL: [0/13] Elapsed 0m 3s (remain 0m 40s) Loss: 0.1126(0.1126)
EVAL: [12/13] Elapsed 1m 2s (remain 0m 0s) Loss: 0.1589(0.1222)
[2022-11-07 12:55:25] - Epoch 1 - avg_train_loss: 0.3964  avg_val_loss: 0.1222  time: 681s
[2022-11-07 12:55:25] - Epoch 1 - Score: 0.4956  Scores: [0.5216025134728105, 0.4750579904394675, 0.4327129720544014, 0.46659856087852175, 0.559766907717641, 0.5179494324622224]
[2022-11-07 12:55:25] - Epoch 1 - Save Best Score: 0.4956 Model
Epoch: [2][0/220] Elapsed 0m 2s (remain 7m 18s) Loss: 0.1678(0.1678) Grad: 510528.2812  LR: 0.00000123
Epoch: [2][100/220] Elapsed 4m 47s (remain 5m 38s) Loss: 0.1611(0.1085) Grad: 128290.0547  LR: 0.00000123
Epoch: [2][200/220] Elapsed 9m 24s (remain 0m 53s) Loss: 0.0637(0.1067) Grad: 87823.6016  LR: 0.00000123
Epoch: [2][219/220] Elapsed 10m 14s (remain 0m 0s) Loss: 0.0984(0.1061) Grad: 47186.0898  LR: 0.00000025
EVAL: [0/13] Elapsed 0m 3s (remain 0m 41s) Loss: 0.0977(0.0977)
[2022-11-07 13:06:47] - Epoch 2 - avg_train_loss: 0.1061  avg_val_loss: 0.1085  time: 677s
[2022-11-07 13:06:47] - Epoch 2 - Score: 0.4662  Scores: [0.5104806859088827, 0.4639117374558588, 0.4230246548928552, 0.4316570373598811, 0.5050162851319839, 0.46329377128706833]
EVAL: [12/13] Elapsed 1m 2s (remain 0m 0s) Loss: 0.1458(0.1085)
[2022-11-07 13:06:47] - Epoch 2 - Save Best Score: 0.4662 Model
Epoch: [3][0/220] Elapsed 0m 3s (remain 14m 18s) Loss: 0.1069(0.1069) Grad: 260589.9688  LR: 0.00000022
Epoch: [3][100/220] Elapsed 4m 33s (remain 5m 22s) Loss: 0.1255(0.1032) Grad: 124498.5547  LR: 0.00000022
Epoch: [3][200/220] Elapsed 9m 11s (remain 0m 52s) Loss: 0.1077(0.1028) Grad: 178901.3906  LR: 0.00000022
Epoch: [3][219/220] Elapsed 10m 14s (remain 0m 0s) Loss: 0.1321(0.1025) Grad: 210029.0938  LR: 0.00000032
EVAL: [0/13] Elapsed 0m 3s (remain 0m 40s) Loss: 0.0979(0.0979)
[2022-11-07 13:18:11] - Epoch 3 - avg_train_loss: 0.1025  avg_val_loss: 0.1070  time: 677s
[2022-11-07 13:18:11] - Epoch 3 - Score: 0.4631  Scores: [0.5068071003068645, 0.4641430300838595, 0.41769299349656225, 0.43233476951334493, 0.5014733817005511, 0.45616107138644685]
[2022-11-07 13:18:11] - Epoch 3 - Save Best Score: 0.4631 Model
EVAL: [12/13] Elapsed 1m 2s (remain 0m 0s) Loss: 0.1396(0.1070)
Epoch: [4][0/220] Elapsed 0m 2s (remain 8m 24s) Loss: 0.0969(0.0969) Grad: 196807.1094  LR: 0.00000036
Epoch: [4][100/220] Elapsed 4m 30s (remain 5m 18s) Loss: 0.0848(0.1010) Grad: 99778.6797  LR: 0.00000036
Epoch: [4][200/220] Elapsed 9m 17s (remain 0m 52s) Loss: 0.1053(0.1013) Grad: 90208.9062  LR: 0.00000036
Epoch: [4][219/220] Elapsed 10m 5s (remain 0m 0s) Loss: 0.1045(0.1009) Grad: 190535.0156  LR: 0.00000140
EVAL: [0/13] Elapsed 0m 3s (remain 0m 40s) Loss: 0.0952(0.0952)
[2022-11-07 13:29:27] - Epoch 4 - avg_train_loss: 0.1009  avg_val_loss: 0.1056  time: 668s
[2022-11-07 13:29:27] - Epoch 4 - Score: 0.4600  Scores: [0.5003629523139654, 0.4596119024476594, 0.4115430573564811, 0.4255982815052061, 0.5020022613182842, 0.46101489496089454]
[2022-11-07 13:29:27] - Epoch 4 - Save Best Score: 0.4600 Model
EVAL: [12/13] Elapsed 1m 2s (remain 0m 0s) Loss: 0.1306(0.1056)
Epoch: [5][0/220] Elapsed 0m 4s (remain 17m 25s) Loss: 0.1207(0.1207) Grad: 367502.2812  LR: 0.00000146
Epoch: [5][100/220] Elapsed 4m 26s (remain 5m 13s) Loss: 0.1116(0.1002) Grad: 58194.3594  LR: 0.00000146
Epoch: [5][200/220] Elapsed 9m 21s (remain 0m 53s) Loss: 0.1365(0.1012) Grad: 157028.4844  LR: 0.00000146
Epoch: [5][219/220] Elapsed 10m 9s (remain 0m 0s) Loss: 0.1017(0.1015) Grad: 50279.1797  LR: 0.00000200
EVAL: [0/13] Elapsed 0m 3s (remain 0m 40s) Loss: 0.1032(0.1032)
EVAL: [12/13] Elapsed 1m 2s (remain 0m 0s) Loss: 0.1643(0.1142)
[2022-11-07 13:40:46] - Epoch 5 - avg_train_loss: 0.1015  avg_val_loss: 0.1142  time: 672s
[2022-11-07 13:40:46] - Epoch 5 - Score: 0.4792  Scores: [0.5314523560858058, 0.4582917885102979, 0.46558089804410163, 0.44987171883515426, 0.4982730535360607, 0.4719367438467127]
[2022-11-07 13:40:48] - ========== fold: 0 result ==========
[2022-11-07 13:40:48] - Score: 0.4600  Scores: [0.5003629523139654, 0.4596119024476594, 0.4115430573564811, 0.4255982815052061, 0.5020022613182842, 0.46101489496089454]
[2022-11-07 13:40:48] - ========== fold: 1 training ==========
[2022-11-07 13:40:48] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v2-xlarge",
  "attention_dropout": 0.0,
  "attention_head_size": 64,
  "attention_probs_dropout_prob": 0.0,
  "conv_act": "gelu",
  "conv_kernel_size": 3,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 24,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1536,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Reinitializing Last 1 Layers ...
Done.!
Epoch: [1][0/220] Elapsed 0m 2s (remain 8m 51s) Loss: 2.8431(2.8431) Grad: inf  LR: 0.00000200
Epoch: [1][100/220] Elapsed 4m 43s (remain 5m 33s) Loss: 0.1340(0.6413) Grad: 130134.4844  LR: 0.00000200
Epoch: [1][200/220] Elapsed 9m 23s (remain 0m 53s) Loss: 0.1019(0.3823) Grad: 69824.4922  LR: 0.00000200
Epoch: [1][219/220] Elapsed 10m 15s (remain 0m 0s) Loss: 0.0993(0.3598) Grad: 68152.6953  LR: 0.00000129
EVAL: [0/13] Elapsed 0m 6s (remain 1m 22s) Loss: 0.1165(0.1165)
[2022-11-07 13:52:11] - Epoch 1 - avg_train_loss: 0.3598  avg_val_loss: 0.1200  time: 674s
[2022-11-07 13:52:11] - Epoch 1 - Score: 0.4919  Scores: [0.5193276903536246, 0.46798182996705606, 0.4580223289283545, 0.47139577219292134, 0.49374816324262644, 0.5411592105335986]
EVAL: [12/13] Elapsed 0m 58s (remain 0m 0s) Loss: 0.0773(0.1200)
[2022-11-07 13:52:11] - Epoch 1 - Save Best Score: 0.4919 Model
Epoch: [2][0/220] Elapsed 0m 2s (remain 7m 53s) Loss: 0.1459(0.1459) Grad: 308834.7188  LR: 0.00000123
Epoch: [2][100/220] Elapsed 4m 48s (remain 5m 39s) Loss: 0.1076(0.1098) Grad: 135998.2344  LR: 0.00000123
Epoch: [2][200/220] Elapsed 9m 34s (remain 0m 54s) Loss: 0.0861(0.1095) Grad: 69897.3984  LR: 0.00000123
Epoch: [2][219/220] Elapsed 10m 27s (remain 0m 0s) Loss: 0.1229(0.1085) Grad: 52558.5625  LR: 0.00000025
EVAL: [0/13] Elapsed 0m 6s (remain 1m 15s) Loss: 0.1110(0.1110)
EVAL: [12/13] Elapsed 0m 57s (remain 0m 0s) Loss: 0.1011(0.1141)
[2022-11-07 14:03:40] - Epoch 2 - avg_train_loss: 0.1085  avg_val_loss: 0.1141  time: 685s
[2022-11-07 14:03:40] - Epoch 2 - Score: 0.4803  Scores: [0.516979630842069, 0.5016674104480467, 0.4316668821074671, 0.4643049698934673, 0.4993333606922775, 0.4675624730788254]
[2022-11-07 14:03:40] - Epoch 2 - Save Best Score: 0.4803 Model
Epoch: [3][0/220] Elapsed 0m 3s (remain 11m 30s) Loss: 0.1387(0.1387) Grad: 387246.6875  LR: 0.00000022
Epoch: [3][100/220] Elapsed 4m 47s (remain 5m 38s) Loss: 0.1145(0.1037) Grad: 196592.5469  LR: 0.00000022
Epoch: [3][200/220] Elapsed 9m 22s (remain 0m 53s) Loss: 0.1400(0.1013) Grad: 155222.7031  LR: 0.00000022
Epoch: [3][219/220] Elapsed 10m 24s (remain 0m 0s) Loss: 0.0997(0.1012) Grad: 37316.3477  LR: 0.00000032
EVAL: [0/13] Elapsed 0m 6s (remain 1m 15s) Loss: 0.1040(0.1040)
[2022-11-07 14:15:07] - Epoch 3 - avg_train_loss: 0.1012  avg_val_loss: 0.1066  time: 682s
[2022-11-07 14:15:07] - Epoch 3 - Score: 0.4636  Scores: [0.5046957457923534, 0.4537315042922323, 0.4296839410845008, 0.46543679010350114, 0.4720674724686337, 0.45582459100429923]
[2022-11-07 14:15:07] - Epoch 3 - Save Best Score: 0.4636 Model
EVAL: [12/13] Elapsed 0m 57s (remain 0m 0s) Loss: 0.0770(0.1066)
Epoch: [4][0/220] Elapsed 0m 3s (remain 13m 36s) Loss: 0.1159(0.1159) Grad: 403921.7500  LR: 0.00000036
Epoch: [4][100/220] Elapsed 4m 40s (remain 5m 30s) Loss: 0.0792(0.1095) Grad: 32454.2090  LR: 0.00000036
Epoch: [4][200/220] Elapsed 9m 18s (remain 0m 52s) Loss: 0.1019(0.1061) Grad: 67528.0625  LR: 0.00000036
Epoch: [4][219/220] Elapsed 10m 13s (remain 0m 0s) Loss: 0.0998(0.1054) Grad: 41967.3672  LR: 0.00000140
EVAL: [0/13] Elapsed 0m 6s (remain 1m 15s) Loss: 0.1174(0.1174)
EVAL: [12/13] Elapsed 0m 57s (remain 0m 0s) Loss: 0.0640(0.1180)
[2022-11-07 14:26:23] - Epoch 4 - avg_train_loss: 0.1054  avg_val_loss: 0.1180  time: 672s
[2022-11-07 14:26:23] - Epoch 4 - Score: 0.4870  Scores: [0.5404536585041656, 0.45991999163856806, 0.43754901457865797, 0.5224317792908096, 0.4825531868846383, 0.47932386390138]
Epoch: [5][0/220] Elapsed 0m 4s (remain 15m 19s) Loss: 0.0985(0.0985) Grad: 553720.9375  LR: 0.00000146
Epoch: [5][100/220] Elapsed 4m 51s (remain 5m 43s) Loss: 0.0908(0.0981) Grad: 103381.5078  LR: 0.00000146
Epoch: [5][200/220] Elapsed 9m 33s (remain 0m 54s) Loss: 0.0747(0.0955) Grad: 88265.3203  LR: 0.00000146
Epoch: [5][219/220] Elapsed 10m 27s (remain 0m 0s) Loss: 0.1031(0.0958) Grad: 117620.8672  LR: 0.00000200
EVAL: [0/13] Elapsed 0m 6s (remain 1m 15s) Loss: 0.1084(0.1084)
EVAL: [12/13] Elapsed 0m 57s (remain 0m 0s) Loss: 0.0714(0.1109)
[2022-11-07 14:37:48] - Epoch 5 - avg_train_loss: 0.0958  avg_val_loss: 0.1109  time: 685s
[2022-11-07 14:37:48] - Epoch 5 - Score: 0.4726  Scores: [0.5071378516114148, 0.45208623574575857, 0.4476469814647523, 0.4995045282721423, 0.47606617566305653, 0.4530611173497778]
[2022-11-07 14:37:50] - ========== fold: 1 result ==========
[2022-11-07 14:37:50] - Score: 0.4636  Scores: [0.5046957457923534, 0.4537315042922323, 0.4296839410845008, 0.46543679010350114, 0.4720674724686337, 0.45582459100429923]
[2022-11-07 14:37:50] - ========== fold: 2 training ==========
[2022-11-07 14:37:50] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v2-xlarge",
  "attention_dropout": 0.0,
  "attention_head_size": 64,
  "attention_probs_dropout_prob": 0.0,
  "conv_act": "gelu",
  "conv_kernel_size": 3,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 24,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1536,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Reinitializing Last 1 Layers ...
Done.!
Epoch: [1][0/220] Elapsed 0m 2s (remain 9m 13s) Loss: 3.0162(3.0162) Grad: inf  LR: 0.00000200
Epoch: [1][100/220] Elapsed 4m 35s (remain 5m 24s) Loss: 0.1071(0.6960) Grad: 50567.6484  LR: 0.00000200
Epoch: [1][200/220] Elapsed 9m 20s (remain 0m 53s) Loss: 0.1163(0.4104) Grad: 76323.9531  LR: 0.00000200
Epoch: [1][219/220] Elapsed 10m 13s (remain 0m 0s) Loss: 0.0921(0.3850) Grad: 52819.4492  LR: 0.00000129
EVAL: [0/13] Elapsed 0m 6s (remain 1m 12s) Loss: 0.1113(0.1113)
[2022-11-07 14:49:11] - Epoch 1 - avg_train_loss: 0.3850  avg_val_loss: 0.1156  time: 674s
[2022-11-07 14:49:11] - Epoch 1 - Score: 0.4827  Scores: [0.4932283391583146, 0.4795694766912236, 0.46888077858681243, 0.4763890669779143, 0.4917183069440708, 0.4862065837201468]
[2022-11-07 14:49:11] - Epoch 1 - Save Best Score: 0.4827 Model
EVAL: [12/13] Elapsed 0m 59s (remain 0m 0s) Loss: 0.1017(0.1156)
Epoch: [2][0/220] Elapsed 0m 3s (remain 11m 3s) Loss: 0.0906(0.0906) Grad: 258136.9062  LR: 0.00000123
Epoch: [2][100/220] Elapsed 4m 36s (remain 5m 25s) Loss: 0.1044(0.1137) Grad: 387232.1875  LR: 0.00000123
Epoch: [2][200/220] Elapsed 9m 22s (remain 0m 53s) Loss: 0.0933(0.1105) Grad: 176467.9062  LR: 0.00000123
Epoch: [2][219/220] Elapsed 10m 14s (remain 0m 0s) Loss: 0.0864(0.1104) Grad: 197157.9844  LR: 0.00000025
EVAL: [0/13] Elapsed 0m 5s (remain 1m 11s) Loss: 0.1065(0.1065)
[2022-11-07 15:00:29] - Epoch 2 - avg_train_loss: 0.1104  avg_val_loss: 0.1134  time: 674s
[2022-11-07 15:00:29] - Epoch 2 - Score: 0.4777  Scores: [0.5016329003816051, 0.4799292234607819, 0.4449064310362087, 0.4645157849223022, 0.48616593687245585, 0.4892965793404794]
EVAL: [12/13] Elapsed 0m 59s (remain 0m 0s) Loss: 0.1032(0.1134)
[2022-11-07 15:00:29] - Epoch 2 - Save Best Score: 0.4777 Model
Epoch: [3][0/220] Elapsed 0m 1s (remain 7m 16s) Loss: 0.0990(0.0990) Grad: 231780.1719  LR: 0.00000022
Epoch: [3][100/220] Elapsed 4m 40s (remain 5m 29s) Loss: 0.1168(0.1068) Grad: 155456.2969  LR: 0.00000022
Epoch: [3][200/220] Elapsed 9m 37s (remain 0m 54s) Loss: 0.1051(0.1055) Grad: 154361.5469  LR: 0.00000022
Epoch: [3][219/220] Elapsed 10m 25s (remain 0m 0s) Loss: 0.1270(0.1052) Grad: 146445.9219  LR: 0.00000032
EVAL: [0/13] Elapsed 0m 5s (remain 1m 11s) Loss: 0.1058(0.1058)
EVAL: [12/13] Elapsed 0m 59s (remain 0m 0s) Loss: 0.1031(0.1103)
[2022-11-07 15:11:58] - Epoch 3 - avg_train_loss: 0.1052  avg_val_loss: 0.1103  time: 685s
[2022-11-07 15:11:58] - Epoch 3 - Score: 0.4714  Scores: [0.48964929679338753, 0.4723470220579368, 0.4425664744254919, 0.4608701103193237, 0.48523508370594814, 0.4775804220136209]
[2022-11-07 15:11:58] - Epoch 3 - Save Best Score: 0.4714 Model
Epoch: [4][0/220] Elapsed 0m 3s (remain 11m 34s) Loss: 0.1169(0.1169) Grad: 211354.2812  LR: 0.00000036
Epoch: [4][100/220] Elapsed 4m 31s (remain 5m 19s) Loss: 0.1074(0.0994) Grad: 327606.5312  LR: 0.00000036
Epoch: [4][200/220] Elapsed 9m 14s (remain 0m 52s) Loss: 0.0992(0.1013) Grad: 101772.9766  LR: 0.00000036
Epoch: [4][219/220] Elapsed 10m 8s (remain 0m 0s) Loss: 0.1065(0.1014) Grad: 90212.5938  LR: 0.00000140
EVAL: [0/13] Elapsed 0m 5s (remain 1m 11s) Loss: 0.1121(0.1121)
[2022-11-07 15:23:10] - Epoch 4 - avg_train_loss: 0.1014  avg_val_loss: 0.1148  time: 668s
[2022-11-07 15:23:10] - Epoch 4 - Score: 0.4806  Scores: [0.5034554659368309, 0.4763010658535893, 0.44729856689483927, 0.4604129920853276, 0.5086449834270408, 0.4877540279284024]
EVAL: [12/13] Elapsed 0m 59s (remain 0m 0s) Loss: 0.1004(0.1148)
Epoch: [5][0/220] Elapsed 0m 3s (remain 11m 20s) Loss: 0.0973(0.0973) Grad: 206112.3906  LR: 0.00000146
Epoch: [5][100/220] Elapsed 4m 52s (remain 5m 45s) Loss: 0.0954(0.1015) Grad: 100797.8672  LR: 0.00000146
Epoch: [5][200/220] Elapsed 9m 30s (remain 0m 53s) Loss: 0.0898(0.1000) Grad: 112633.0469  LR: 0.00000146
Epoch: [5][219/220] Elapsed 10m 23s (remain 0m 0s) Loss: 0.0771(0.1000) Grad: 158383.4219  LR: 0.00000200
EVAL: [0/13] Elapsed 0m 5s (remain 1m 11s) Loss: 0.0998(0.0998)
EVAL: [12/13] Elapsed 0m 59s (remain 0m 0s) Loss: 0.1100(0.1104)
[2022-11-07 15:34:33] - Epoch 5 - avg_train_loss: 0.1000  avg_val_loss: 0.1104  time: 684s
[2022-11-07 15:34:33] - Epoch 5 - Score: 0.4711  Scores: [0.5015773051539488, 0.4647052148796184, 0.44045342851231023, 0.46125377107670995, 0.4848634519740346, 0.4738087499235334]
[2022-11-07 15:34:33] - Epoch 5 - Save Best Score: 0.4711 Model
[2022-11-07 15:34:39] - ========== fold: 2 result ==========
[2022-11-07 15:34:39] - Score: 0.4711  Scores: [0.5015773051539488, 0.4647052148796184, 0.44045342851231023, 0.46125377107670995, 0.4848634519740346, 0.4738087499235334]
[2022-11-07 15:34:39] - ========== fold: 3 training ==========
[2022-11-07 15:34:39] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v2-xlarge",
  "attention_dropout": 0.0,
  "attention_head_size": 64,
  "attention_probs_dropout_prob": 0.0,
  "conv_act": "gelu",
  "conv_kernel_size": 3,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 24,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1536,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Reinitializing Last 1 Layers ...
Done.!
Epoch: [1][0/220] Elapsed 0m 1s (remain 6m 24s) Loss: 2.7599(2.7599) Grad: inf  LR: 0.00000200
Epoch: [1][100/220] Elapsed 4m 38s (remain 5m 28s) Loss: 0.1270(0.6598) Grad: 55382.9180  LR: 0.00000200
Epoch: [1][200/220] Elapsed 9m 16s (remain 0m 52s) Loss: 0.1408(0.3912) Grad: 70229.5703  LR: 0.00000200
Epoch: [1][219/220] Elapsed 10m 10s (remain 0m 0s) Loss: 0.1171(0.3683) Grad: 34991.0742  LR: 0.00000129
EVAL: [0/13] Elapsed 0m 5s (remain 1m 1s) Loss: 0.1216(0.1216)
EVAL: [12/13] Elapsed 0m 59s (remain 0m 0s) Loss: 0.1148(0.1153)
[2022-11-07 15:45:56] - Epoch 1 - avg_train_loss: 0.3683  avg_val_loss: 0.1153  time: 671s
[2022-11-07 15:45:56] - Epoch 1 - Score: 0.4804  Scores: [0.5393585550404008, 0.4770852685898263, 0.4324397769535805, 0.4584403514000811, 0.49922217338925085, 0.4756625187389767]
[2022-11-07 15:45:56] - Epoch 1 - Save Best Score: 0.4804 Model
Epoch: [2][0/220] Elapsed 0m 2s (remain 8m 7s) Loss: 0.1349(0.1349) Grad: 320380.7500  LR: 0.00000123
Epoch: [2][100/220] Elapsed 4m 53s (remain 5m 46s) Loss: 0.1136(0.1109) Grad: 120443.4531  LR: 0.00000123
Epoch: [2][200/220] Elapsed 9m 25s (remain 0m 53s) Loss: 0.1188(0.1089) Grad: 104452.5000  LR: 0.00000123
Epoch: [2][219/220] Elapsed 10m 21s (remain 0m 0s) Loss: 0.0666(0.1090) Grad: 153394.6406  LR: 0.00000025
EVAL: [0/13] Elapsed 0m 5s (remain 1m 2s) Loss: 0.1018(0.1018)
[2022-11-07 15:57:21] - Epoch 2 - avg_train_loss: 0.1090  avg_val_loss: 0.1139  time: 681s
[2022-11-07 15:57:21] - Epoch 2 - Score: 0.4775  Scores: [0.5157949201914255, 0.48145473004690803, 0.43708289245092913, 0.47773367032104574, 0.5045274441585902, 0.4483388288695898]
[2022-11-07 15:57:21] - Epoch 2 - Save Best Score: 0.4775 Model
EVAL: [12/13] Elapsed 0m 59s (remain 0m 0s) Loss: 0.1279(0.1139)
Epoch: [3][0/220] Elapsed 0m 5s (remain 18m 40s) Loss: 0.1492(0.1492) Grad: 498387.8750  LR: 0.00000022
Epoch: [3][100/220] Elapsed 4m 52s (remain 5m 44s) Loss: 0.0735(0.1075) Grad: 153276.0469  LR: 0.00000022
Epoch: [3][200/220] Elapsed 9m 31s (remain 0m 54s) Loss: 0.0670(0.1043) Grad: 195148.7969  LR: 0.00000022
Epoch: [3][219/220] Elapsed 10m 18s (remain 0m 0s) Loss: 0.0954(0.1036) Grad: 278641.1875  LR: 0.00000032
EVAL: [0/13] Elapsed 0m 5s (remain 1m 1s) Loss: 0.1012(0.1012)
[2022-11-07 16:08:44] - Epoch 3 - avg_train_loss: 0.1036  avg_val_loss: 0.1078  time: 678s
[2022-11-07 16:08:44] - Epoch 3 - Score: 0.4641  Scores: [0.5153497820327819, 0.4613798341281142, 0.4250539214741366, 0.45839959708613354, 0.4786570507234895, 0.4457557614726977]
[2022-11-07 16:08:44] - Epoch 3 - Save Best Score: 0.4641 Model
EVAL: [12/13] Elapsed 0m 59s (remain 0m 0s) Loss: 0.1128(0.1078)
Epoch: [4][0/220] Elapsed 0m 2s (remain 8m 20s) Loss: 0.0882(0.0882) Grad: 191206.2031  LR: 0.00000036
Epoch: [4][100/220] Elapsed 4m 56s (remain 5m 49s) Loss: 0.1185(0.1032) Grad: 220629.3750  LR: 0.00000036
Epoch: [4][200/220] Elapsed 9m 24s (remain 0m 53s) Loss: 0.0824(0.1027) Grad: 54163.1211  LR: 0.00000036
Epoch: [4][219/220] Elapsed 10m 22s (remain 0m 0s) Loss: 0.1200(0.1032) Grad: 58436.0234  LR: 0.00000140
EVAL: [0/13] Elapsed 0m 5s (remain 1m 2s) Loss: 0.1003(0.1003)
EVAL: [12/13] Elapsed 0m 59s (remain 0m 0s) Loss: 0.1122(0.1080)
[2022-11-07 16:20:11] - Epoch 4 - avg_train_loss: 0.1032  avg_val_loss: 0.1080  time: 683s
[2022-11-07 16:20:11] - Epoch 4 - Score: 0.4646  Scores: [0.515923204206293, 0.4646650888308556, 0.4230189468910974, 0.4545291722303751, 0.47949026207076256, 0.45015322613541553]
Epoch: [5][0/220] Elapsed 0m 2s (remain 7m 29s) Loss: 0.1275(0.1275) Grad: 199896.2344  LR: 0.00000146
Epoch: [5][100/220] Elapsed 4m 57s (remain 5m 50s) Loss: 0.0870(0.0993) Grad: 211722.3438  LR: 0.00000146
Epoch: [5][200/220] Elapsed 9m 26s (remain 0m 53s) Loss: 0.1088(0.0995) Grad: 482536.0938  LR: 0.00000146
Epoch: [5][219/220] Elapsed 10m 19s (remain 0m 0s) Loss: 0.1070(0.0993) Grad: 586591.6250  LR: 0.00000200
EVAL: [0/13] Elapsed 0m 5s (remain 1m 1s) Loss: 0.1110(0.1110)
[2022-11-07 16:31:29] - Epoch 5 - avg_train_loss: 0.0993  avg_val_loss: 0.1102  time: 679s
[2022-11-07 16:31:29] - Epoch 5 - Score: 0.4700  Scores: [0.5063262754790543, 0.45993071141746117, 0.44519739390075763, 0.46868903475366697, 0.479645384269172, 0.46023073330500486]
EVAL: [12/13] Elapsed 0m 59s (remain 0m 0s) Loss: 0.1144(0.1102)
[2022-11-07 16:31:31] - ========== fold: 3 result ==========
[2022-11-07 16:31:31] - Score: 0.4641  Scores: [0.5153497820327819, 0.4613798341281142, 0.4250539214741366, 0.45839959708613354, 0.4786570507234895, 0.4457557614726977]
[2022-11-07 16:31:31] - ========== fold: 4 training ==========
[2022-11-07 16:31:31] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v2-xlarge",
  "attention_dropout": 0.0,
  "attention_head_size": 64,
  "attention_probs_dropout_prob": 0.0,
  "conv_act": "gelu",
  "conv_kernel_size": 3,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 24,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1536,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Reinitializing Last 1 Layers ...
Done.!
Epoch: [1][0/220] Elapsed 0m 2s (remain 7m 43s) Loss: 3.0197(3.0197) Grad: inf  LR: 0.00000200
Epoch: [1][100/220] Elapsed 4m 42s (remain 5m 33s) Loss: 0.1099(0.7410) Grad: 45760.8750  LR: 0.00000200
Epoch: [1][200/220] Elapsed 9m 23s (remain 0m 53s) Loss: 0.1068(0.4314) Grad: 34129.5898  LR: 0.00000200
Epoch: [1][219/220] Elapsed 10m 22s (remain 0m 0s) Loss: 0.1165(0.4052) Grad: 59229.6797  LR: 0.00000129
EVAL: [0/13] Elapsed 0m 5s (remain 1m 2s) Loss: 0.0990(0.0990)
[2022-11-07 16:43:01] - Epoch 1 - avg_train_loss: 0.4052  avg_val_loss: 0.1160  time: 683s
[2022-11-07 16:43:01] - Epoch 1 - Score: 0.4831  Scores: [0.5160771303101364, 0.4712790127523062, 0.4417778965516798, 0.47020695724365225, 0.5312399712566622, 0.4677959823393343]
[2022-11-07 16:43:01] - Epoch 1 - Save Best Score: 0.4831 Model
EVAL: [12/13] Elapsed 1m 0s (remain 0m 0s) Loss: 0.1136(0.1160)
Epoch: [2][0/220] Elapsed 0m 3s (remain 12m 4s) Loss: 0.1101(0.1101) Grad: 293816.2812  LR: 0.00000123
Epoch: [2][100/220] Elapsed 4m 37s (remain 5m 27s) Loss: 0.1209(0.1101) Grad: 259885.1094  LR: 0.00000123
Epoch: [2][200/220] Elapsed 9m 29s (remain 0m 53s) Loss: 0.1349(0.1083) Grad: 145530.7344  LR: 0.00000123
Epoch: [2][219/220] Elapsed 10m 26s (remain 0m 0s) Loss: 0.1020(0.1078) Grad: 68276.3438  LR: 0.00000025
EVAL: [0/13] Elapsed 0m 5s (remain 1m 0s) Loss: 0.0955(0.0955)
[2022-11-07 16:54:31] - Epoch 2 - avg_train_loss: 0.1078  avg_val_loss: 0.1099  time: 686s
[2022-11-07 16:54:31] - Epoch 2 - Score: 0.4700  Scores: [0.5055078408898015, 0.4565424537979405, 0.4309837937693435, 0.46318284317175423, 0.5024420771204751, 0.4615680395423114]
[2022-11-07 16:54:31] - Epoch 2 - Save Best Score: 0.4700 Model
EVAL: [12/13] Elapsed 0m 59s (remain 0m 0s) Loss: 0.1130(0.1099)
Epoch: [3][0/220] Elapsed 0m 2s (remain 9m 46s) Loss: 0.1403(0.1403) Grad: 498881.6250  LR: 0.00000022
Epoch: [3][100/220] Elapsed 4m 50s (remain 5m 41s) Loss: 0.1106(0.1013) Grad: 148117.7969  LR: 0.00000022
Epoch: [3][200/220] Elapsed 9m 34s (remain 0m 54s) Loss: 0.0849(0.1031) Grad: 122884.9531  LR: 0.00000022
Epoch: [3][219/220] Elapsed 10m 24s (remain 0m 0s) Loss: 0.0774(0.1029) Grad: 107339.5156  LR: 0.00000032
EVAL: [0/13] Elapsed 0m 5s (remain 1m 1s) Loss: 0.0987(0.0987)
[2022-11-07 17:06:00] - Epoch 3 - avg_train_loss: 0.1029  avg_val_loss: 0.1079  time: 685s
[2022-11-07 17:06:00] - Epoch 3 - Score: 0.4657  Scores: [0.4955342389674172, 0.45389164332894827, 0.4312275789451765, 0.4568069639907034, 0.4981262149517912, 0.45831382813761495]
[2022-11-07 17:06:00] - Epoch 3 - Save Best Score: 0.4657 Model
EVAL: [12/13] Elapsed 0m 59s (remain 0m 0s) Loss: 0.1184(0.1079)
Epoch: [4][0/220] Elapsed 0m 2s (remain 8m 9s) Loss: 0.0927(0.0927) Grad: 249922.3281  LR: 0.00000036
Epoch: [4][100/220] Elapsed 4m 53s (remain 5m 45s) Loss: 0.0938(0.0970) Grad: 240867.5938  LR: 0.00000036
Epoch: [4][200/220] Elapsed 9m 24s (remain 0m 53s) Loss: 0.1021(0.0985) Grad: 435581.0625  LR: 0.00000036
Epoch: [4][219/220] Elapsed 10m 19s (remain 0m 0s) Loss: 0.1048(0.0991) Grad: 365792.2812  LR: 0.00000140
EVAL: [0/13] Elapsed 0m 5s (remain 1m 1s) Loss: 0.0994(0.0994)
EVAL: [12/13] Elapsed 0m 59s (remain 0m 0s) Loss: 0.1247(0.1099)
[2022-11-07 17:17:24] - Epoch 4 - avg_train_loss: 0.0991  avg_val_loss: 0.1099  time: 680s
[2022-11-07 17:17:24] - Epoch 4 - Score: 0.4701  Scores: [0.508653569689572, 0.46256759074822273, 0.4302285538897091, 0.46089437025975927, 0.5020526988326013, 0.45601464171741524]
Epoch: [5][0/220] Elapsed 0m 2s (remain 8m 51s) Loss: 0.1038(0.1038) Grad: 261368.8594  LR: 0.00000146
Epoch: [5][100/220] Elapsed 5m 3s (remain 5m 57s) Loss: 0.0740(0.0981) Grad: 107521.0000  LR: 0.00000146
Epoch: [5][200/220] Elapsed 9m 32s (remain 0m 54s) Loss: 0.1003(0.1002) Grad: 150452.0938  LR: 0.00000146
Epoch: [5][219/220] Elapsed 10m 30s (remain 0m 0s) Loss: 0.0991(0.1004) Grad: 163575.4219  LR: 0.00000200
EVAL: [0/13] Elapsed 0m 5s (remain 1m 1s) Loss: 0.1002(0.1002)
EVAL: [12/13] Elapsed 0m 59s (remain 0m 0s) Loss: 0.1415(0.1190)
[2022-11-07 17:28:54] - Epoch 5 - avg_train_loss: 0.1004  avg_val_loss: 0.1190  time: 690s
[2022-11-07 17:28:54] - Epoch 5 - Score: 0.4902  Scores: [0.49820304152017636, 0.4822146129383957, 0.46130355697447656, 0.4847299467687836, 0.49727001570614776, 0.5175072869824242]
[2022-11-07 17:28:56] - ========== fold: 4 result ==========
[2022-11-07 17:28:56] - Score: 0.4657  Scores: [0.4955342389674172, 0.45389164332894827, 0.4312275789451765, 0.4568069639907034, 0.4981262149517912, 0.45831382813761495]
[2022-11-07 17:28:56] - ========== fold: 5 training ==========
[2022-11-07 17:28:56] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v2-xlarge",
  "attention_dropout": 0.0,
  "attention_head_size": 64,
  "attention_probs_dropout_prob": 0.0,
  "conv_act": "gelu",
  "conv_kernel_size": 3,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 24,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1536,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Reinitializing Last 1 Layers ...
Done.!
Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/220] Elapsed 0m 1s (remain 6m 0s) Loss: 2.2810(2.2810) Grad: inf  LR: 0.00000200
Epoch: [1][100/220] Elapsed 4m 39s (remain 5m 29s) Loss: 0.1083(0.5473) Grad: 157344.5938  LR: 0.00000200
Epoch: [1][200/220] Elapsed 9m 16s (remain 0m 52s) Loss: 0.1149(0.3328) Grad: 35668.5234  LR: 0.00000200
Epoch: [1][219/220] Elapsed 10m 10s (remain 0m 0s) Loss: 0.1543(0.3145) Grad: 34703.6328  LR: 0.00000129
EVAL: [0/13] Elapsed 0m 3s (remain 0m 43s) Loss: 0.1163(0.1163)
[2022-11-07 17:40:12] - Epoch 1 - avg_train_loss: 0.3145  avg_val_loss: 0.1136  time: 671s
[2022-11-07 17:40:12] - Epoch 1 - Score: 0.4771  Scores: [0.5144836618874029, 0.45874998231600483, 0.41947843358051295, 0.48362691102567074, 0.4886341183909135, 0.49777724400871765]
[2022-11-07 17:40:12] - Epoch 1 - Save Best Score: 0.4771 Model
EVAL: [12/13] Elapsed 0m 59s (remain 0m 0s) Loss: 0.1162(0.1136)
Epoch: [2][0/220] Elapsed 0m 4s (remain 16m 36s) Loss: 0.0936(0.0936) Grad: 285501.9375  LR: 0.00000123
Epoch: [2][100/220] Elapsed 4m 55s (remain 5m 48s) Loss: 0.1104(0.1105) Grad: 53513.9141  LR: 0.00000123
Epoch: [2][200/220] Elapsed 9m 36s (remain 0m 54s) Loss: 0.1274(0.1099) Grad: 62804.5312  LR: 0.00000123
Epoch: [2][219/220] Elapsed 10m 28s (remain 0m 0s) Loss: 0.0919(0.1092) Grad: 33712.2930  LR: 0.00000025
EVAL: [0/13] Elapsed 0m 3s (remain 0m 43s) Loss: 0.1040(0.1040)
[2022-11-07 17:51:43] - Epoch 2 - avg_train_loss: 0.1092  avg_val_loss: 0.1034  time: 687s
[2022-11-07 17:51:43] - Epoch 2 - Score: 0.4550  Scores: [0.49036425849985293, 0.4537526679482068, 0.40003773157992134, 0.4604677254670103, 0.4808405867251158, 0.4443690394293819]
EVAL: [12/13] Elapsed 0m 58s (remain 0m 0s) Loss: 0.0963(0.1034)
[2022-11-07 17:51:43] - Epoch 2 - Save Best Score: 0.4550 Model
Epoch: [3][0/220] Elapsed 0m 2s (remain 9m 41s) Loss: 0.0729(0.0729) Grad: 213418.8750  LR: 0.00000022
Epoch: [3][100/220] Elapsed 4m 41s (remain 5m 32s) Loss: 0.0978(0.1017) Grad: 207992.5781  LR: 0.00000022
Epoch: [3][200/220] Elapsed 9m 27s (remain 0m 53s) Loss: 0.0980(0.1017) Grad: 110877.7578  LR: 0.00000022
Epoch: [3][219/220] Elapsed 10m 20s (remain 0m 0s) Loss: 0.0473(0.1013) Grad: 53807.0820  LR: 0.00000032
EVAL: [0/13] Elapsed 0m 3s (remain 0m 43s) Loss: 0.0979(0.0979)
[2022-11-07 18:03:07] - Epoch 3 - avg_train_loss: 0.1013  avg_val_loss: 0.1017  time: 680s
[2022-11-07 18:03:07] - Epoch 3 - Score: 0.4513  Scores: [0.4909633951735386, 0.4438528054714559, 0.39271551226653695, 0.46141636499670174, 0.471024355596214, 0.4475803035521211]
[2022-11-07 18:03:07] - Epoch 3 - Save Best Score: 0.4513 Model
EVAL: [12/13] Elapsed 0m 58s (remain 0m 0s) Loss: 0.0919(0.1017)
Epoch: [4][0/220] Elapsed 0m 3s (remain 11m 17s) Loss: 0.1001(0.1001) Grad: 338031.0312  LR: 0.00000036
Epoch: [4][100/220] Elapsed 4m 45s (remain 5m 36s) Loss: 0.0848(0.0976) Grad: 265079.0312  LR: 0.00000036
Epoch: [4][200/220] Elapsed 9m 21s (remain 0m 53s) Loss: 0.0985(0.0977) Grad: 266996.7812  LR: 0.00000036
Epoch: [4][219/220] Elapsed 10m 12s (remain 0m 0s) Loss: 0.1237(0.0973) Grad: 135609.9531  LR: 0.00000140
EVAL: [0/13] Elapsed 0m 3s (remain 0m 43s) Loss: 0.1018(0.1018)
[2022-11-07 18:14:24] - Epoch 4 - avg_train_loss: 0.0973  avg_val_loss: 0.1015  time: 672s
[2022-11-07 18:14:24] - Epoch 4 - Score: 0.4508  Scores: [0.48683911691577003, 0.44613938974752815, 0.39272581369457166, 0.4596236799662394, 0.46996840482778607, 0.44930119758212717]
[2022-11-07 18:14:24] - Epoch 4 - Save Best Score: 0.4508 Model
EVAL: [12/13] Elapsed 0m 58s (remain 0m 0s) Loss: 0.0897(0.1015)
Epoch: [5][0/220] Elapsed 0m 2s (remain 9m 10s) Loss: 0.0885(0.0885) Grad: 204207.3750  LR: 0.00000146
Epoch: [5][100/220] Elapsed 4m 42s (remain 5m 33s) Loss: 0.0950(0.0971) Grad: 118882.6797  LR: 0.00000146
Epoch: [5][200/220] Elapsed 9m 26s (remain 0m 53s) Loss: 0.0967(0.0972) Grad: 137687.5156  LR: 0.00000146
Epoch: [5][219/220] Elapsed 10m 19s (remain 0m 0s) Loss: 0.1326(0.0973) Grad: 190217.5156  LR: 0.00000200
