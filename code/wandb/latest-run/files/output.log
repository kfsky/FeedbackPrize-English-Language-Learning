(15594, 9)
(15142, 9)
Downloading tokenizer_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 52.0/52.0 [00:00<00:00, 89.2kB/s]
Downloading config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 579/579 [00:00<00:00, 1.08MB/s]
Downloading spm.model: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.35M/2.35M [00:00<00:00, 39.8MB/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3911/3911 [00:03<00:00, 1008.76it/s]
[2022-11-19 06:29:20] - comment: deberta-v3-base, add 2021data kunisho data
[2022-11-19 06:29:20] - max_len: 2048
[2022-11-19 06:29:20] - ========== fold: 0 training ==========
[2022-11-19 06:29:20] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}


Downloading pytorch_model.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 354M/354M [00:05<00:00, 65.3MB/s]
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Reinitializing Last 1 Layers ...
Done.!
Epoch: [1][0/476] Elapsed 0m 3s (remain 28m 12s) Loss: 3.2934(3.2934) Grad: inf  LR: 0.00002994
Epoch: [1][100/476] Elapsed 2m 41s (remain 9m 59s) Loss: 0.0307(0.3907) Grad: 23990.6230  LR: 0.00002994
Epoch: [1][200/476] Elapsed 5m 23s (remain 7m 22s) Loss: 0.0193(0.2121) Grad: 43060.0508  LR: 0.00002994
Epoch: [1][300/476] Elapsed 8m 13s (remain 4m 46s) Loss: 0.0368(0.1507) Grad: 84480.6016  LR: 0.00002994
Epoch: [1][400/476] Elapsed 10m 49s (remain 2m 1s) Loss: 0.0290(0.1196) Grad: 30256.4531  LR: 0.00002994
Epoch: [1][475/476] Elapsed 12m 50s (remain 0m 0s) Loss: 0.0329(0.1045) Grad: 51403.3750  LR: 0.00001699
EVAL: [0/13] Elapsed 0m 2s (remain 0m 31s) Loss: 0.0985(0.0985)
[2022-11-19 06:42:46] - Epoch 1 - avg_train_loss: 0.1045  avg_val_loss: 0.1006  time: 797s
[2022-11-19 06:42:46] - Epoch 1 - Score: 0.4489  Scores: [0.48172855276781495, 0.4587847588591847, 0.40797137653586324, 0.4446532737569642, 0.46126953999421266, 0.4389283062467949]
[2022-11-19 06:42:46] - Epoch 1 - Save Best Score: 0.4489 Model
EVAL: [12/13] Elapsed 0m 26s (remain 0m 0s) Loss: 0.1050(0.1006)
