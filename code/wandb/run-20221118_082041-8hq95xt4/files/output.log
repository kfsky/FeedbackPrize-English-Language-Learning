Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3911/3911 [00:03<00:00, 999.74it/s]
[2022-11-18 08:20:50] - comment: deberta-v3-base, add 2021data(all) raw_pred, PL=ems004data
[2022-11-18 08:20:50] - max_len: 2048
[2022-11-18 08:20:50] - ========== fold: 0 training ==========
[2022-11-18 08:20:50] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Reinitializing Last 1 Layers ...
Done.!
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/975] Elapsed 0m 1s (remain 29m 15s) Loss: 3.1293(3.1293) Grad: inf  LR: 0.00002994
Epoch: [1][100/975] Elapsed 2m 12s (remain 19m 3s) Loss: 0.1274(0.3920) Grad: 59639.7305  LR: 0.00002994
Epoch: [1][200/975] Elapsed 4m 12s (remain 16m 14s) Loss: 0.1162(0.2621) Grad: 92766.9375  LR: 0.00002994
Epoch: [1][300/975] Elapsed 6m 16s (remain 14m 2s) Loss: 0.0956(0.2202) Grad: 66782.6172  LR: 0.00002994
Epoch: [1][400/975] Elapsed 8m 16s (remain 11m 51s) Loss: 0.0960(0.1989) Grad: 53798.2070  LR: 0.00002994
Epoch: [1][500/975] Elapsed 10m 27s (remain 9m 53s) Loss: 0.1335(0.1855) Grad: 74330.1016  LR: 0.00002994
Epoch: [1][600/975] Elapsed 12m 25s (remain 7m 44s) Loss: 0.1621(0.1760) Grad: 32052.8184  LR: 0.00002994
Epoch: [1][700/975] Elapsed 14m 19s (remain 5m 35s) Loss: 0.1368(0.1694) Grad: 68787.8203  LR: 0.00002994
Epoch: [1][800/975] Elapsed 16m 19s (remain 3m 32s) Loss: 0.1549(0.1644) Grad: 56715.1641  LR: 0.00002994
Epoch: [1][900/975] Elapsed 18m 24s (remain 1m 30s) Loss: 0.1385(0.1607) Grad: 39385.0703  LR: 0.00002994
Epoch: [1][974/975] Elapsed 19m 50s (remain 0m 0s) Loss: 0.0870(0.1585) Grad: 19435.4180  LR: 0.00001605
EVAL: [0/25] Elapsed 0m 1s (remain 0m 33s) Loss: 0.1868(0.1868)
EVAL: [24/25] Elapsed 0m 41s (remain 0m 0s) Loss: 0.1871(0.1541)
[2022-11-18 08:41:26] - Epoch 1 - avg_train_loss: 0.1585  avg_val_loss: 0.1541  time: 1232s
[2022-11-18 08:41:26] - Epoch 1 - Score: 0.5584  Scores: [0.6021681123371098, 0.5767642037800628, 0.48791679737279575, 0.5509906665777772, 0.5796609326787288, 0.5528359183246887]
[2022-11-18 08:41:26] - Epoch 1 - Save Best Score: 0.5584 Model
Epoch: [2][0/975] Elapsed 0m 1s (remain 17m 41s) Loss: 0.1151(0.1151) Grad: 49110.6094  LR: 0.00001699
Epoch: [2][100/975] Elapsed 2m 8s (remain 18m 29s) Loss: 0.0919(0.1207) Grad: 130770.6016  LR: 0.00001699
Epoch: [2][200/975] Elapsed 4m 9s (remain 16m 1s) Loss: 0.1131(0.1238) Grad: 107937.0781  LR: 0.00001699
Epoch: [2][300/975] Elapsed 6m 14s (remain 13m 57s) Loss: 0.1831(0.1263) Grad: 127783.4375  LR: 0.00001699
Epoch: [2][400/975] Elapsed 8m 14s (remain 11m 47s) Loss: 0.1853(0.1264) Grad: 174107.4844  LR: 0.00001699
Epoch: [2][500/975] Elapsed 10m 16s (remain 9m 43s) Loss: 0.1186(0.1266) Grad: 75305.8281  LR: 0.00001699
Epoch: [2][600/975] Elapsed 12m 24s (remain 7m 43s) Loss: 0.1613(0.1260) Grad: 70439.0781  LR: 0.00001699
Epoch: [2][700/975] Elapsed 14m 26s (remain 5m 38s) Loss: 0.1502(0.1263) Grad: 51331.3828  LR: 0.00001699
Epoch: [2][800/975] Elapsed 16m 26s (remain 3m 34s) Loss: 0.1603(0.1257) Grad: 176571.1094  LR: 0.00001699
Epoch: [2][900/975] Elapsed 18m 22s (remain 1m 30s) Loss: 0.1151(0.1251) Grad: 61764.8438  LR: 0.00001699
Epoch: [2][974/975] Elapsed 19m 52s (remain 0m 0s) Loss: 0.1165(0.1248) Grad: 85665.9219  LR: 0.00000010
EVAL: [0/25] Elapsed 0m 1s (remain 0m 32s) Loss: 0.1442(0.1442)
EVAL: [24/25] Elapsed 0m 40s (remain 0m 0s) Loss: 0.1599(0.1318)
[2022-11-18 09:02:03] - Epoch 2 - avg_train_loss: 0.1248  avg_val_loss: 0.1318  time: 1233s
[2022-11-18 09:02:03] - Epoch 2 - Score: 0.5158  Scores: [0.5407883453773417, 0.5255437391437195, 0.4643072671218856, 0.5141852690123447, 0.5199857491795846, 0.5301412589525644]
[2022-11-18 09:02:03] - Epoch 2 - Save Best Score: 0.5158 Model
Epoch: [3][0/975] Elapsed 0m 0s (remain 13m 48s) Loss: 0.1536(0.1536) Grad: 64756.2500  LR: 0.00000016
Epoch: [3][100/975] Elapsed 1m 58s (remain 17m 2s) Loss: 0.1081(0.1213) Grad: 87199.6094  LR: 0.00000016
Epoch: [3][200/975] Elapsed 3m 59s (remain 15m 24s) Loss: 0.0842(0.1189) Grad: 112685.5000  LR: 0.00000016
Epoch: [3][300/975] Elapsed 6m 2s (remain 13m 31s) Loss: 0.0855(0.1206) Grad: 92042.9062  LR: 0.00000016
Epoch: [3][400/975] Elapsed 8m 9s (remain 11m 39s) Loss: 0.1355(0.1230) Grad: 42134.2617  LR: 0.00000016
Epoch: [3][500/975] Elapsed 10m 8s (remain 9m 35s) Loss: 0.1880(0.1230) Grad: 69140.6484  LR: 0.00000016
Epoch: [3][600/975] Elapsed 12m 7s (remain 7m 32s) Loss: 0.0810(0.1229) Grad: 93021.5312  LR: 0.00000016
Epoch: [3][700/975] Elapsed 14m 6s (remain 5m 30s) Loss: 0.1709(0.1226) Grad: 182514.8750  LR: 0.00000016
Epoch: [3][800/975] Elapsed 16m 12s (remain 3m 31s) Loss: 0.1175(0.1224) Grad: 79208.7891  LR: 0.00000016
Epoch: [3][900/975] Elapsed 18m 17s (remain 1m 30s) Loss: 0.1446(0.1226) Grad: 75313.8906  LR: 0.00000016
Epoch: [3][974/975] Elapsed 19m 48s (remain 0m 0s) Loss: 0.2069(0.1229) Grad: 98613.0625  LR: 0.00001417
EVAL: [0/25] Elapsed 0m 1s (remain 0m 33s) Loss: 0.1235(0.1235)
EVAL: [24/25] Elapsed 0m 40s (remain 0m 0s) Loss: 0.1517(0.1240)
[2022-11-18 09:22:37] - Epoch 3 - avg_train_loss: 0.1229  avg_val_loss: 0.1240  time: 1230s
[2022-11-18 09:22:37] - Epoch 3 - Score: 0.5003  Scores: [0.5335335424605413, 0.49916686882417344, 0.44746764799161215, 0.5068110570107215, 0.5033838254274731, 0.5114605302839089]
[2022-11-18 09:22:37] - Epoch 3 - Save Best Score: 0.5003 Model
Epoch: [4][0/975] Elapsed 0m 1s (remain 28m 28s) Loss: 0.1319(0.1319) Grad: 79354.9453  LR: 0.00001323
Epoch: [4][100/975] Elapsed 2m 4s (remain 18m 1s) Loss: 0.0945(0.1250) Grad: 80199.7578  LR: 0.00001323
Epoch: [4][200/975] Elapsed 4m 6s (remain 15m 51s) Loss: 0.1243(0.1228) Grad: 186217.5000  LR: 0.00001323
Epoch: [4][300/975] Elapsed 6m 7s (remain 13m 42s) Loss: 0.0888(0.1237) Grad: 110503.4609  LR: 0.00001323
Epoch: [4][400/975] Elapsed 8m 6s (remain 11m 35s) Loss: 0.1286(0.1233) Grad: 91546.2266  LR: 0.00001323
Epoch: [4][500/975] Elapsed 9m 58s (remain 9m 25s) Loss: 0.1658(0.1222) Grad: 247200.0156  LR: 0.00001323
Epoch: [4][600/975] Elapsed 11m 55s (remain 7m 25s) Loss: 0.1115(0.1211) Grad: 97824.0859  LR: 0.00001323
Epoch: [4][700/975] Elapsed 13m 56s (remain 5m 26s) Loss: 0.1111(0.1203) Grad: 209916.1719  LR: 0.00001323
Epoch: [4][800/975] Elapsed 16m 0s (remain 3m 28s) Loss: 0.0844(0.1196) Grad: 145196.4688  LR: 0.00001323
Epoch: [4][900/975] Elapsed 18m 8s (remain 1m 29s) Loss: 0.1151(0.1202) Grad: 80317.6562  LR: 0.00001323
Epoch: [4][974/975] Elapsed 19m 43s (remain 0m 0s) Loss: 0.1601(0.1200) Grad: 73501.5000  LR: 0.00003003
EVAL: [0/25] Elapsed 0m 1s (remain 0m 32s) Loss: 0.1238(0.1238)
EVAL: [24/25] Elapsed 0m 40s (remain 0m 0s) Loss: 0.1581(0.1240)
Epoch: [5][0/975] Elapsed 0m 1s (remain 17m 1s) Loss: 0.1169(0.1169) Grad: 61253.3711  LR: 0.00002994
[2022-11-18 09:43:05] - Epoch 4 - avg_train_loss: 0.1200  avg_val_loss: 0.1240  time: 1224s
[2022-11-18 09:43:05] - Epoch 4 - Score: 0.5006  Scores: [0.5343099578948244, 0.5036038918345329, 0.4554634757261832, 0.4974970526692256, 0.5144126339560061, 0.4983541682688358]
Epoch: [5][100/975] Elapsed 2m 3s (remain 17m 49s) Loss: 0.1658(0.1215) Grad: 77026.3359  LR: 0.00002994
Epoch: [5][200/975] Elapsed 4m 9s (remain 15m 59s) Loss: 0.1106(0.1187) Grad: 82783.6953  LR: 0.00002994
Epoch: [5][300/975] Elapsed 6m 8s (remain 13m 45s) Loss: 0.1092(0.1193) Grad: 89546.4688  LR: 0.00002994
Epoch: [5][400/975] Elapsed 8m 11s (remain 11m 43s) Loss: 0.0790(0.1184) Grad: 43945.8672  LR: 0.00002994
Epoch: [5][500/975] Elapsed 10m 8s (remain 9m 36s) Loss: 0.1326(0.1181) Grad: 103094.3828  LR: 0.00002994
Epoch: [5][600/975] Elapsed 12m 15s (remain 7m 37s) Loss: 0.1007(0.1181) Grad: 82985.3516  LR: 0.00002994
Epoch: [5][700/975] Elapsed 14m 16s (remain 5m 34s) Loss: 0.1649(0.1180) Grad: 194727.8281  LR: 0.00002994
Traceback (most recent call last):
  File "/notebooks/code/exp058.py", line 823, in <module>
    main()
  File "/notebooks/code/exp058.py", line 761, in main
    _oof_df = train_loop(train2, train, fold) # ★★★
  File "/notebooks/code/exp058.py", line 657, in train_loop
    avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)
  File "/notebooks/code/exp058.py", line 486, in train_fn
    scaler.scale(loss).backward()
  File "/usr/local/lib/python3.9/dist-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt