Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3911/3911 [00:03<00:00, 990.76it/s]
[2022-10-28 06:57:26] - max_len: 2048
[2022-10-28 06:57:26] - ========== fold: 0 training ==========
[2022-10-28 06:57:26] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Reinitializing Last 1 Layers ...
Done.!
Epoch: [1][0/195] Elapsed 0m 2s (remain 7m 9s) Loss: 2.5898(2.5898) Grad: inf  LR: 0.00009980
Epoch: [1][100/195] Elapsed 2m 1s (remain 1m 53s) Loss: 2.6818(2.6111) Grad: nan  LR: 0.00009980
Epoch: [1][194/195] Elapsed 3m 55s (remain 0m 0s) Loss: 2.6439(2.6038) Grad: nan  LR: 0.00009863
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 2.3929(2.3929)
EVAL: [24/25] Elapsed 0m 36s (remain 0m 0s) Loss: 2.4821(2.5394)
[2022-10-28 07:02:01] - Epoch 1 - avg_train_loss: 2.6038  avg_val_loss: 2.5394  time: 272s
[2022-10-28 07:02:01] - Epoch 1 - Score: 3.1084  Scores: [3.3933602645858594, 2.8844592338448427, 3.36746351538917, 3.1699578768660333, 3.3943357559126746, 2.441054768947488]
[2022-10-28 07:02:01] - Epoch 1 - Save Best Score: 3.1084 Model
Epoch: [2][0/195] Elapsed 0m 0s (remain 2m 10s) Loss: 2.5453(2.5453) Grad: nan  LR: 0.00009931
Epoch: [2][100/195] Elapsed 2m 1s (remain 1m 52s) Loss: 2.7529(2.6154) Grad: nan  LR: 0.00009931
Epoch: [2][194/195] Elapsed 3m 58s (remain 0m 0s) Loss: 2.3986(2.6041) Grad: nan  LR: 0.00009243
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 2.3929(2.3929)
[2022-10-28 07:06:37] - Epoch 2 - avg_train_loss: 2.6041  avg_val_loss: 2.5394  time: 274s
[2022-10-28 07:06:37] - Epoch 2 - Score: 3.1084  Scores: [3.3933602645858594, 2.8844592338448427, 3.36746351538917, 3.1699578768660333, 3.3943357559126746, 2.441054768947488]
EVAL: [24/25] Elapsed 0m 35s (remain 0m 0s) Loss: 2.4821(2.5394)
Epoch: [3][0/195] Elapsed 0m 0s (remain 3m 5s) Loss: 2.6792(2.6792) Grad: nan  LR: 0.00009402
Epoch: [3][100/195] Elapsed 2m 0s (remain 1m 52s) Loss: 2.6112(2.6021) Grad: nan  LR: 0.00009402
Epoch: [3][194/195] Elapsed 3m 52s (remain 0m 0s) Loss: 2.5550(2.6036) Grad: nan  LR: 0.00008209
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 2.3929(2.3929)
[2022-10-28 07:11:05] - Epoch 3 - avg_train_loss: 2.6036  avg_val_loss: 2.5394  time: 269s
[2022-10-28 07:11:05] - Epoch 3 - Score: 3.1084  Scores: [3.3933602645858594, 2.8844592338448427, 3.36746351538917, 3.1699578768660333, 3.3943357559126746, 2.441054768947488]
EVAL: [24/25] Elapsed 0m 35s (remain 0m 0s) Loss: 2.4821(2.5394)
Epoch: [4][0/195] Elapsed 0m 1s (remain 3m 34s) Loss: 2.6693(2.6693) Grad: nan  LR: 0.00008445
Epoch: [4][100/195] Elapsed 2m 0s (remain 1m 52s) Loss: 2.3214(2.6061) Grad: nan  LR: 0.00008445
Epoch: [4][194/195] Elapsed 3m 54s (remain 0m 0s) Loss: 2.6856(2.6028) Grad: nan  LR: 0.00006864
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 2.3929(2.3929)
EVAL: [24/25] Elapsed 0m 35s (remain 0m 0s) Loss: 2.4821(2.5394)
[2022-10-28 07:15:36] - Epoch 4 - avg_train_loss: 2.6028  avg_val_loss: 2.5394  time: 271s
[2022-10-28 07:15:36] - Epoch 4 - Score: 3.1084  Scores: [3.3933602645858594, 2.8844592338448427, 3.36746351538917, 3.1699578768660333, 3.3943357559126746, 2.441054768947488]
Epoch: [5][0/195] Elapsed 0m 1s (remain 4m 15s) Loss: 2.7627(2.7627) Grad: nan  LR: 0.00007152
Epoch: [5][100/195] Elapsed 1m 58s (remain 1m 50s) Loss: 2.5526(2.5967) Grad: nan  LR: 0.00007152
Epoch: [5][194/195] Elapsed 3m 52s (remain 0m 0s) Loss: 2.9195(2.6029) Grad: nan  LR: 0.00005340
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 2.3929(2.3929)
[2022-10-28 07:20:05] - Epoch 5 - avg_train_loss: 2.6029  avg_val_loss: 2.5394  time: 269s
[2022-10-28 07:20:05] - Epoch 5 - Score: 3.1084  Scores: [3.3933602645858594, 2.8844592338448427, 3.36746351538917, 3.1699578768660333, 3.3943357559126746, 2.441054768947488]
EVAL: [24/25] Elapsed 0m 35s (remain 0m 0s) Loss: 2.4821(2.5394)
Epoch: [6][0/195] Elapsed 0m 2s (remain 7m 2s) Loss: 2.4178(2.4178) Grad: nan  LR: 0.00005652
Epoch: [6][100/195] Elapsed 2m 2s (remain 1m 54s) Loss: 2.4709(2.5889) Grad: nan  LR: 0.00005652
Epoch: [6][194/195] Elapsed 3m 54s (remain 0m 0s) Loss: 2.3659(2.6032) Grad: nan  LR: 0.00003784
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 2.3929(2.3929)
[2022-10-28 07:24:35] - Epoch 6 - avg_train_loss: 2.6032  avg_val_loss: 2.5394  time: 270s
[2022-10-28 07:24:35] - Epoch 6 - Score: 3.1084  Scores: [3.3933602645858594, 2.8844592338448427, 3.36746351538917, 3.1699578768660333, 3.3943357559126746, 2.441054768947488]
EVAL: [24/25] Elapsed 0m 35s (remain 0m 0s) Loss: 2.4821(2.5394)
Epoch: [7][0/195] Elapsed 0m 1s (remain 4m 53s) Loss: 2.5342(2.5342) Grad: nan  LR: 0.00004090
Epoch: [7][100/195] Elapsed 1m 54s (remain 1m 46s) Loss: 2.3866(2.6086) Grad: nan  LR: 0.00004090
Epoch: [7][194/195] Elapsed 3m 55s (remain 0m 0s) Loss: 2.6467(2.6031) Grad: nan  LR: 0.00002351
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 2.3929(2.3929)
[2022-10-28 07:29:07] - Epoch 7 - avg_train_loss: 2.6031  avg_val_loss: 2.5394  time: 272s
[2022-10-28 07:29:07] - Epoch 7 - Score: 3.1084  Scores: [3.3933602645858594, 2.8844592338448427, 3.36746351538917, 3.1699578768660333, 3.3943357559126746, 2.441054768947488]
EVAL: [24/25] Elapsed 0m 35s (remain 0m 0s) Loss: 2.4821(2.5394)
Epoch: [8][0/195] Elapsed 0m 1s (remain 5m 43s) Loss: 2.6661(2.6661) Grad: nan  LR: 0.00002621
Epoch: [8][100/195] Elapsed 2m 2s (remain 1m 53s) Loss: 2.9062(2.6045) Grad: nan  LR: 0.00002621
Epoch: [8][194/195] Elapsed 3m 54s (remain 0m 0s) Loss: 2.8821(2.6033) Grad: nan  LR: 0.00001180
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 2.3929(2.3929)
[2022-10-28 07:33:37] - Epoch 8 - avg_train_loss: 2.6033  avg_val_loss: 2.5394  time: 271s
[2022-10-28 07:33:37] - Epoch 8 - Score: 3.1084  Scores: [3.3933602645858594, 2.8844592338448427, 3.36746351538917, 3.1699578768660333, 3.3943357559126746, 2.441054768947488]
EVAL: [24/25] Elapsed 0m 35s (remain 0m 0s) Loss: 2.4821(2.5394)
Epoch: [9][0/195] Elapsed 0m 2s (remain 6m 54s) Loss: 2.6034(2.6034) Grad: nan  LR: 0.00001387
Epoch: [9][100/195] Elapsed 1m 55s (remain 1m 47s) Loss: 2.7127(2.6245) Grad: nan  LR: 0.00001387
Epoch: [9][194/195] Elapsed 3m 53s (remain 0m 0s) Loss: 2.2315(2.6032) Grad: nan  LR: 0.00000391
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 2.3929(2.3929)
[2022-10-28 07:38:07] - Epoch 9 - avg_train_loss: 2.6032  avg_val_loss: 2.5394  time: 270s
[2022-10-28 07:38:07] - Epoch 9 - Score: 3.1084  Scores: [3.3933602645858594, 2.8844592338448427, 3.36746351538917, 3.1699578768660333, 3.3943357559126746, 2.441054768947488]
EVAL: [24/25] Elapsed 0m 36s (remain 0m 0s) Loss: 2.4821(2.5394)
Epoch: [10][0/195] Elapsed 0m 1s (remain 3m 34s) Loss: 2.5118(2.5118) Grad: nan  LR: 0.00000513
Epoch: [10][100/195] Elapsed 2m 5s (remain 1m 57s) Loss: 2.5711(2.6125) Grad: nan  LR: 0.00000513
Epoch: [10][194/195] Elapsed 3m 59s (remain 0m 0s) Loss: 2.7252(2.6039) Grad: nan  LR: 0.00000010
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 2.3929(2.3929)
[2022-10-28 07:42:43] - Epoch 10 - avg_train_loss: 2.6039  avg_val_loss: 2.5394  time: 276s
[2022-10-28 07:42:43] - Epoch 10 - Score: 3.1084  Scores: [3.3933602645858594, 2.8844592338448427, 3.36746351538917, 3.1699578768660333, 3.3943357559126746, 2.441054768947488]
EVAL: [24/25] Elapsed 0m 35s (remain 0m 0s) Loss: 2.4821(2.5394)
Epoch: [11][0/195] Elapsed 0m 1s (remain 6m 11s) Loss: 2.5872(2.5872) Grad: nan  LR: 0.00000030
Epoch: [11][100/195] Elapsed 1m 56s (remain 1m 48s) Loss: 2.7154(2.5950) Grad: nan  LR: 0.00000030
Epoch: [11][194/195] Elapsed 3m 52s (remain 0m 0s) Loss: 2.7303(2.6037) Grad: nan  LR: 0.00000180
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 2.3929(2.3929)
[2022-10-28 07:47:12] - Epoch 11 - avg_train_loss: 2.6037  avg_val_loss: 2.5394  time: 268s
[2022-10-28 07:47:12] - Epoch 11 - Score: 3.1084  Scores: [3.3933602645858594, 2.8844592338448427, 3.36746351538917, 3.1699578768660333, 3.3943357559126746, 2.441054768947488]
EVAL: [24/25] Elapsed 0m 35s (remain 0m 0s) Loss: 2.4821(2.5394)
Epoch: [12][0/195] Elapsed 0m 1s (remain 4m 37s) Loss: 2.6069(2.6069) Grad: nan  LR: 0.00000111
Epoch: [12][100/195] Elapsed 2m 0s (remain 1m 52s) Loss: 2.4672(2.6161) Grad: nan  LR: 0.00000111
Epoch: [12][194/195] Elapsed 4m 0s (remain 0m 0s) Loss: 2.6029(2.6037) Grad: nan  LR: 0.00000804
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 2.3929(2.3929)
[2022-10-28 07:51:48] - Epoch 12 - avg_train_loss: 2.6037  avg_val_loss: 2.5394  time: 276s
[2022-10-28 07:51:48] - Epoch 12 - Score: 3.1084  Scores: [3.3933602645858594, 2.8844592338448427, 3.36746351538917, 3.1699578768660333, 3.3943357559126746, 2.441054768947488]
EVAL: [24/25] Elapsed 0m 36s (remain 0m 0s) Loss: 2.4821(2.5394)
Epoch: [13][0/195] Elapsed 0m 0s (remain 2m 52s) Loss: 2.8845(2.8845) Grad: nan  LR: 0.00000644
Epoch: [13][100/195] Elapsed 1m 53s (remain 1m 45s) Loss: 2.7815(2.6003) Grad: nan  LR: 0.00000644
Epoch: [13][194/195] Elapsed 3m 53s (remain 0m 0s) Loss: 2.4184(2.6038) Grad: nan  LR: 0.00001839
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 2.3929(2.3929)
[2022-10-28 07:56:18] - Epoch 13 - avg_train_loss: 2.6038  avg_val_loss: 2.5394  time: 270s
[2022-10-28 07:56:18] - Epoch 13 - Score: 3.1084  Scores: [3.3933602645858594, 2.8844592338448427, 3.36746351538917, 3.1699578768660333, 3.3943357559126746, 2.441054768947488]
EVAL: [24/25] Elapsed 0m 35s (remain 0m 0s) Loss: 2.4821(2.5394)
Epoch: [14][0/195] Elapsed 0m 1s (remain 3m 15s) Loss: 2.8066(2.8066) Grad: nan  LR: 0.00001603
Traceback (most recent call last):
  File "/notebooks/code/exp032.py", line 799, in <module>
    main()
  File "/notebooks/code/exp032.py", line 738, in main
    _oof_df = train_loop(train, fold)
  File "/notebooks/code/exp032.py", line 650, in train_loop
    avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)
  File "/notebooks/code/exp032.py", line 467, in train_fn
    scaler.scale(loss).backward()
  File "/usr/local/lib/python3.9/dist-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt