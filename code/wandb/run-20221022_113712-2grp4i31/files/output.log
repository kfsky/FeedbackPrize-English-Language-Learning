Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|                                                                                                                                                                                                                                                | 0/3911 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors

100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3911/3911 [00:04<00:00, 971.40it/s]
[2022-10-22 11:37:19] - max_len: 2048
[2022-10-22 11:37:19] - ========== fold: 0 training ==========
[2022-10-22 11:37:19] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v2-xlarge",
  "attention_dropout": 0.0,
  "attention_head_size": 64,
  "attention_probs_dropout_prob": 0.0,
  "conv_act": "gelu",
  "conv_kernel_size": 3,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 24,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1536,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/782] Elapsed 0m 2s (remain 38m 8s) Loss: 3.0954(3.0954) Grad: inf  LR: 0.00000100
Epoch: [1][100/782] Elapsed 0m 59s (remain 6m 42s) Loss: 0.3046(1.8374) Grad: 30572.8066  LR: 0.00000100
Epoch: [1][200/782] Elapsed 1m 57s (remain 5m 40s) Loss: 0.1277(1.0080) Grad: 33623.7578  LR: 0.00000100
Epoch: [1][300/782] Elapsed 2m 53s (remain 4m 37s) Loss: 0.1277(0.7153) Grad: 18635.4121  LR: 0.00000100
Epoch: [1][400/782] Elapsed 3m 49s (remain 3m 37s) Loss: 0.1462(0.5704) Grad: 5182.3359  LR: 0.00000100
Epoch: [1][500/782] Elapsed 4m 45s (remain 2m 40s) Loss: 0.0859(0.4801) Grad: 8125.0420  LR: 0.00000100
Epoch: [1][600/782] Elapsed 5m 47s (remain 1m 44s) Loss: 0.0553(0.4189) Grad: 4073.8171  LR: 0.00000100
Epoch: [1][700/782] Elapsed 6m 44s (remain 0m 46s) Loss: 0.0827(0.3763) Grad: 7523.1426  LR: 0.00000100
Epoch: [1][781/782] Elapsed 7m 30s (remain 0m 0s) Loss: 0.2137(0.3492) Grad: 12844.3330  LR: 0.00000075
EVAL: [0/98] Elapsed 0m 0s (remain 1m 35s) Loss: 0.0930(0.0930)
EVAL: [97/98] Elapsed 1m 29s (remain 0m 0s) Loss: 0.0852(0.1097)
[2022-10-22 11:46:28] - Epoch 1 - avg_train_loss: 0.3492  avg_val_loss: 0.1097  time: 541s
[2022-10-22 11:46:28] - Epoch 1 - Score: 0.4695  Scores: [0.49598672756457496, 0.47790243154565654, 0.42126972274259833, 0.45832703102391004, 0.4837941319956009, 0.4798924302184235]
[2022-10-22 11:46:28] - Epoch 1 - Save Best Score: 0.4695 Model
Epoch: [2][0/782] Elapsed 0m 1s (remain 14m 16s) Loss: 0.0984(0.0984) Grad: 320059.3438  LR: 0.00000077
Epoch: [2][100/782] Elapsed 0m 57s (remain 6m 29s) Loss: 0.0964(0.1251) Grad: 52659.6445  LR: 0.00000077
Epoch: [2][200/782] Elapsed 1m 55s (remain 5m 32s) Loss: 0.1053(0.1126) Grad: 31863.6523  LR: 0.00000077
Epoch: [2][300/782] Elapsed 2m 56s (remain 4m 41s) Loss: 0.1015(0.1111) Grad: 50907.5508  LR: 0.00000077
Epoch: [2][400/782] Elapsed 3m 50s (remain 3m 39s) Loss: 0.1353(0.1087) Grad: 30374.6035  LR: 0.00000077
Epoch: [2][500/782] Elapsed 4m 44s (remain 2m 39s) Loss: 0.2250(0.1077) Grad: 46478.4727  LR: 0.00000077
Epoch: [2][600/782] Elapsed 5m 41s (remain 1m 42s) Loss: 0.0805(0.1076) Grad: 45881.9453  LR: 0.00000077
Epoch: [2][700/782] Elapsed 6m 39s (remain 0m 46s) Loss: 0.0747(0.1072) Grad: 40276.6562  LR: 0.00000077
Epoch: [2][781/782] Elapsed 7m 25s (remain 0m 0s) Loss: 0.0773(0.1070) Grad: 23823.5762  LR: 0.00000022
EVAL: [0/98] Elapsed 0m 0s (remain 1m 34s) Loss: 0.1003(0.1003)
EVAL: [97/98] Elapsed 1m 29s (remain 0m 0s) Loss: 0.0818(0.1059)
[2022-10-22 11:55:30] - Epoch 2 - avg_train_loss: 0.1070  avg_val_loss: 0.1059  time: 535s
[2022-10-22 11:55:30] - Epoch 2 - Score: 0.4610  Scores: [0.4925574936191099, 0.4701029543583486, 0.41369666633763275, 0.45508776953595936, 0.47947734385108853, 0.4553308646013862]
[2022-10-22 11:55:30] - Epoch 2 - Save Best Score: 0.4610 Model
Epoch: [3][0/782] Elapsed 0m 0s (remain 9m 24s) Loss: 0.0932(0.0932) Grad: inf  LR: 0.00000024
Epoch: [3][100/782] Elapsed 0m 59s (remain 6m 42s) Loss: 0.1515(0.1019) Grad: 59938.1289  LR: 0.00000024
Epoch: [3][200/782] Elapsed 1m 56s (remain 5m 35s) Loss: 0.0385(0.1010) Grad: 29195.0195  LR: 0.00000024
Epoch: [3][300/782] Elapsed 2m 55s (remain 4m 40s) Loss: 0.1479(0.1015) Grad: 25675.4746  LR: 0.00000024
Epoch: [3][400/782] Elapsed 3m 55s (remain 3m 43s) Loss: 0.1662(0.1023) Grad: 186226.7656  LR: 0.00000024
Epoch: [3][500/782] Elapsed 4m 52s (remain 2m 44s) Loss: 0.1720(0.1027) Grad: 25244.4004  LR: 0.00000024
Epoch: [3][600/782] Elapsed 5m 50s (remain 1m 45s) Loss: 0.0818(0.1028) Grad: 22222.7559  LR: 0.00000024
Epoch: [3][700/782] Elapsed 6m 44s (remain 0m 46s) Loss: 0.0914(0.1026) Grad: 16329.0293  LR: 0.00000024
Epoch: [3][781/782] Elapsed 7m 30s (remain 0m 0s) Loss: 0.0772(0.1026) Grad: 9272.1592  LR: 0.00000002
EVAL: [0/98] Elapsed 0m 0s (remain 1m 35s) Loss: 0.0865(0.0865)
EVAL: [97/98] Elapsed 1m 29s (remain 0m 0s) Loss: 0.0967(0.1081)
Epoch: [4][0/782] Elapsed 0m 0s (remain 7m 2s) Loss: 0.0636(0.0636) Grad: inf  LR: 0.00000001
[2022-10-22 12:04:37] - Epoch 3 - avg_train_loss: 0.1026  avg_val_loss: 0.1081  time: 540s
[2022-10-22 12:04:37] - Epoch 3 - Score: 0.4658  Scores: [0.49141991590859163, 0.47098810142515063, 0.42595404754092736, 0.45210040525371814, 0.4936076802768808, 0.46090012128070607]
Epoch: [4][100/782] Elapsed 1m 0s (remain 6m 49s) Loss: 0.1215(0.0992) Grad: 51654.2852  LR: 0.00000001
Epoch: [4][200/782] Elapsed 1m 54s (remain 5m 30s) Loss: 0.0752(0.0987) Grad: 45530.4297  LR: 0.00000001
Epoch: [4][300/782] Elapsed 2m 51s (remain 4m 34s) Loss: 0.1027(0.0972) Grad: 92215.0859  LR: 0.00000001
Epoch: [4][400/782] Elapsed 3m 46s (remain 3m 35s) Loss: 0.0999(0.0952) Grad: 66456.7266  LR: 0.00000001
Epoch: [4][500/782] Elapsed 4m 40s (remain 2m 37s) Loss: 0.0769(0.0967) Grad: 17405.4043  LR: 0.00000001
Epoch: [4][600/782] Elapsed 5m 38s (remain 1m 41s) Loss: 0.0946(0.0974) Grad: 36995.7578  LR: 0.00000001
Epoch: [4][700/782] Elapsed 6m 39s (remain 0m 46s) Loss: 0.1848(0.0984) Grad: 65511.2812  LR: 0.00000001
Epoch: [4][781/782] Elapsed 7m 24s (remain 0m 0s) Loss: 0.0787(0.0990) Grad: 28839.2207  LR: 0.00000038
EVAL: [0/98] Elapsed 0m 0s (remain 1m 35s) Loss: 0.0839(0.0839)
EVAL: [97/98] Elapsed 1m 29s (remain 0m 0s) Loss: 0.1044(0.1104)
Epoch: [5][0/782] Elapsed 0m 0s (remain 6m 54s) Loss: 0.0666(0.0666) Grad: inf  LR: 0.00000035
[2022-10-22 12:13:32] - Epoch 4 - avg_train_loss: 0.0990  avg_val_loss: 0.1104  time: 535s
[2022-10-22 12:13:32] - Epoch 4 - Score: 0.4707  Scores: [0.50504155273123, 0.48166158051423624, 0.41870327941899355, 0.4598834963006184, 0.48970938013579995, 0.46926121970999873]
Epoch: [5][100/782] Elapsed 0m 58s (remain 6m 35s) Loss: 0.1025(0.0942) Grad: 88734.6875  LR: 0.00000035
Epoch: [5][200/782] Elapsed 1m 55s (remain 5m 34s) Loss: 0.0788(0.0926) Grad: 103688.5938  LR: 0.00000035
Epoch: [5][300/782] Elapsed 2m 56s (remain 4m 42s) Loss: 0.0546(0.0926) Grad: 30202.6230  LR: 0.00000035
Epoch: [5][400/782] Elapsed 3m 52s (remain 3m 41s) Loss: 0.1568(0.0937) Grad: 58018.8047  LR: 0.00000035
Epoch: [5][500/782] Elapsed 4m 49s (remain 2m 42s) Loss: 0.1339(0.0927) Grad: 77862.0781  LR: 0.00000035
Epoch: [5][600/782] Elapsed 5m 47s (remain 1m 44s) Loss: 0.1133(0.0945) Grad: 64685.5156  LR: 0.00000035
Epoch: [5][700/782] Elapsed 6m 40s (remain 0m 46s) Loss: 0.0878(0.0960) Grad: 25678.6797  LR: 0.00000035
Epoch: [5][781/782] Elapsed 7m 26s (remain 0m 0s) Loss: 0.0624(0.0961) Grad: 18093.5488  LR: 0.00000089
EVAL: [0/98] Elapsed 0m 0s (remain 1m 34s) Loss: 0.0920(0.0920)
EVAL: [97/98] Elapsed 1m 29s (remain 0m 0s) Loss: 0.0963(0.1066)
[2022-10-22 12:22:28] - Epoch 5 - avg_train_loss: 0.0961  avg_val_loss: 0.1066  time: 537s
[2022-10-22 12:22:28] - Epoch 5 - Score: 0.4623  Scores: [0.5021479279716998, 0.4644389694372882, 0.41729443871439686, 0.45374408955562473, 0.4754705529432798, 0.46043042213214735]
[2022-10-22 12:22:30] - ========== fold: 0 result ==========
[2022-10-22 12:22:30] - Score: 0.4610  Scores: [0.4925574936191099, 0.4701029543583486, 0.41369666633763275, 0.45508776953595936, 0.47947734385108853, 0.4553308646013862]
[2022-10-22 12:22:30] - ========== fold: 1 training ==========
[2022-10-22 12:22:30] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v2-xlarge",
  "attention_dropout": 0.0,
  "attention_head_size": 64,
  "attention_probs_dropout_prob": 0.0,
  "conv_act": "gelu",
  "conv_kernel_size": 3,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 24,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1536,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/782] Elapsed 0m 0s (remain 10m 43s) Loss: 2.5481(2.5481) Grad: inf  LR: 0.00000100
Epoch: [1][100/782] Elapsed 0m 59s (remain 6m 38s) Loss: 0.2605(1.8693) Grad: 162015.2188  LR: 0.00000100
Epoch: [1][200/782] Elapsed 1m 55s (remain 5m 32s) Loss: 0.0724(1.0263) Grad: 58856.8594  LR: 0.00000100
Epoch: [1][300/782] Elapsed 2m 51s (remain 4m 33s) Loss: 0.0882(0.7272) Grad: 53808.5742  LR: 0.00000100
Epoch: [1][400/782] Elapsed 3m 49s (remain 3m 38s) Loss: 0.0984(0.5763) Grad: 73041.3359  LR: 0.00000100
Epoch: [1][500/782] Elapsed 4m 46s (remain 2m 40s) Loss: 0.1119(0.4849) Grad: 45827.5781  LR: 0.00000100
Epoch: [1][600/782] Elapsed 5m 48s (remain 1m 45s) Loss: 0.1806(0.4256) Grad: 79316.8594  LR: 0.00000100
Epoch: [1][700/782] Elapsed 6m 42s (remain 0m 46s) Loss: 0.1051(0.3819) Grad: 13280.1182  LR: 0.00000100
Epoch: [1][781/782] Elapsed 7m 28s (remain 0m 0s) Loss: 0.0804(0.3546) Grad: 7196.4844  LR: 0.00000075
EVAL: [0/98] Elapsed 0m 1s (remain 1m 41s) Loss: 0.1054(0.1054)
EVAL: [97/98] Elapsed 1m 29s (remain 0m 0s) Loss: 0.1087(0.1116)
[2022-10-22 12:31:35] - Epoch 1 - avg_train_loss: 0.3546  avg_val_loss: 0.1116  time: 538s
[2022-10-22 12:31:35] - Epoch 1 - Score: 0.4733  Scores: [0.5320134227162427, 0.45147384244776334, 0.43137761593007495, 0.48740436949076016, 0.47700985157430337, 0.4606478957925406]
[2022-10-22 12:31:35] - Epoch 1 - Save Best Score: 0.4733 Model
Epoch: [2][0/782] Elapsed 0m 0s (remain 12m 7s) Loss: 0.2473(0.2473) Grad: inf  LR: 0.00000077
Epoch: [2][100/782] Elapsed 0m 56s (remain 6m 19s) Loss: 0.0710(0.1270) Grad: 139786.0938  LR: 0.00000077
Epoch: [2][200/782] Elapsed 1m 52s (remain 5m 25s) Loss: 0.1083(0.1159) Grad: 146188.0156  LR: 0.00000077
Epoch: [2][300/782] Elapsed 2m 52s (remain 4m 36s) Loss: 0.0904(0.1131) Grad: 50495.6445  LR: 0.00000077
Epoch: [2][400/782] Elapsed 3m 48s (remain 3m 37s) Loss: 0.1266(0.1119) Grad: 55882.0781  LR: 0.00000077
Epoch: [2][500/782] Elapsed 4m 46s (remain 2m 40s) Loss: 0.0762(0.1107) Grad: 43177.8672  LR: 0.00000077
Epoch: [2][600/782] Elapsed 5m 44s (remain 1m 43s) Loss: 0.0576(0.1104) Grad: 35849.6484  LR: 0.00000077
Epoch: [2][700/782] Elapsed 6m 40s (remain 0m 46s) Loss: 0.1773(0.1100) Grad: 75630.3672  LR: 0.00000077
Epoch: [2][781/782] Elapsed 7m 28s (remain 0m 0s) Loss: 0.1001(0.1088) Grad: 61239.7500  LR: 0.00000022
EVAL: [0/98] Elapsed 0m 1s (remain 1m 40s) Loss: 0.0826(0.0826)
EVAL: [97/98] Elapsed 1m 28s (remain 0m 0s) Loss: 0.1087(0.1061)
[2022-10-22 12:40:37] - Epoch 2 - avg_train_loss: 0.1088  avg_val_loss: 0.1061  time: 538s
[2022-10-22 12:40:37] - Epoch 2 - Score: 0.4618  Scores: [0.49980015290511254, 0.4479588695935865, 0.4208443422224204, 0.48016563918462646, 0.47596930893681033, 0.445772584491798]
[2022-10-22 12:40:37] - Epoch 2 - Save Best Score: 0.4618 Model
Epoch: [3][0/782] Elapsed 0m 1s (remain 19m 32s) Loss: 0.1449(0.1449) Grad: 586525.2500  LR: 0.00000024
Epoch: [3][100/782] Elapsed 1m 2s (remain 6m 58s) Loss: 0.1626(0.1045) Grad: 95363.6484  LR: 0.00000024
Epoch: [3][200/782] Elapsed 2m 4s (remain 5m 59s) Loss: 0.0597(0.1057) Grad: 76600.1094  LR: 0.00000024
Epoch: [3][300/782] Elapsed 2m 59s (remain 4m 46s) Loss: 0.1288(0.1065) Grad: 59829.6641  LR: 0.00000024
Epoch: [3][400/782] Elapsed 3m 52s (remain 3m 40s) Loss: 0.1373(0.1065) Grad: 17227.4023  LR: 0.00000024
Epoch: [3][500/782] Elapsed 4m 49s (remain 2m 42s) Loss: 0.0590(0.1087) Grad: 12722.8359  LR: 0.00000024
Epoch: [3][600/782] Elapsed 5m 50s (remain 1m 45s) Loss: 0.0925(0.1096) Grad: 11044.3965  LR: 0.00000024
Epoch: [3][700/782] Elapsed 6m 43s (remain 0m 46s) Loss: 0.1056(0.1082) Grad: 10700.6045  LR: 0.00000024
Epoch: [3][781/782] Elapsed 7m 28s (remain 0m 0s) Loss: 0.1350(0.1075) Grad: 10699.9775  LR: 0.00000002
EVAL: [0/98] Elapsed 0m 1s (remain 1m 43s) Loss: 0.0880(0.0880)
[2022-10-22 12:49:39] - Epoch 3 - avg_train_loss: 0.1075  avg_val_loss: 0.1059  time: 538s
[2022-10-22 12:49:39] - Epoch 3 - Score: 0.4612  Scores: [0.49844838360056604, 0.4504761858304219, 0.42303131172838804, 0.48041523103009914, 0.4682545382042501, 0.44676000802088606]
[2022-10-22 12:49:39] - Epoch 3 - Save Best Score: 0.4612 Model
EVAL: [97/98] Elapsed 1m 29s (remain 0m 0s) Loss: 0.1222(0.1059)
Epoch: [4][0/782] Elapsed 0m 0s (remain 8m 49s) Loss: 0.0726(0.0726) Grad: inf  LR: 0.00000001
Epoch: [4][100/782] Elapsed 0m 58s (remain 6m 36s) Loss: 0.1134(0.1073) Grad: 313014.3125  LR: 0.00000001
Epoch: [4][200/782] Elapsed 1m 56s (remain 5m 35s) Loss: 0.0510(0.1016) Grad: 23778.2715  LR: 0.00000001
Epoch: [4][300/782] Elapsed 2m 51s (remain 4m 33s) Loss: 0.1151(0.0996) Grad: 78820.4375  LR: 0.00000001
Epoch: [4][400/782] Elapsed 3m 52s (remain 3m 40s) Loss: 0.0766(0.0982) Grad: 26300.8691  LR: 0.00000001
Epoch: [4][500/782] Elapsed 4m 46s (remain 2m 40s) Loss: 0.0756(0.0977) Grad: 30235.6172  LR: 0.00000001
Epoch: [4][600/782] Elapsed 5m 40s (remain 1m 42s) Loss: 0.0905(0.0986) Grad: 13345.4990  LR: 0.00000001
Epoch: [4][700/782] Elapsed 6m 40s (remain 0m 46s) Loss: 0.0917(0.0990) Grad: 10980.3555  LR: 0.00000001
Epoch: [4][781/782] Elapsed 7m 27s (remain 0m 0s) Loss: 0.0943(0.0994) Grad: 11716.3799  LR: 0.00000038
EVAL: [0/98] Elapsed 0m 1s (remain 1m 42s) Loss: 0.0856(0.0856)
EVAL: [97/98] Elapsed 1m 29s (remain 0m 0s) Loss: 0.1116(0.1063)
[2022-10-22 12:58:41] - Epoch 4 - avg_train_loss: 0.0994  avg_val_loss: 0.1063  time: 537s
[2022-10-22 12:58:41] - Epoch 4 - Score: 0.4619  Scores: [0.49697191173684674, 0.4506055017724593, 0.4167739684446757, 0.4737439704165718, 0.4763597535783268, 0.4570339109223703]
Epoch: [5][0/782] Elapsed 0m 1s (remain 13m 55s) Loss: 0.0398(0.0398) Grad: 239148.4375  LR: 0.00000035
Epoch: [5][100/782] Elapsed 0m 57s (remain 6m 28s) Loss: 0.1320(0.0965) Grad: inf  LR: 0.00000035
Epoch: [5][200/782] Elapsed 2m 4s (remain 5m 59s) Loss: 0.0493(0.0982) Grad: 105452.9062  LR: 0.00000035
Epoch: [5][300/782] Elapsed 2m 58s (remain 4m 44s) Loss: 0.1339(0.0988) Grad: 146370.7500  LR: 0.00000035
Epoch: [5][400/782] Elapsed 3m 53s (remain 3m 42s) Loss: 0.0844(0.0968) Grad: 122413.6953  LR: 0.00000035
Epoch: [5][500/782] Elapsed 4m 51s (remain 2m 43s) Loss: 0.0567(0.0952) Grad: 33216.2578  LR: 0.00000035
Epoch: [5][600/782] Elapsed 5m 46s (remain 1m 44s) Loss: 0.0976(0.0958) Grad: 92620.0234  LR: 0.00000035
Epoch: [5][700/782] Elapsed 6m 44s (remain 0m 46s) Loss: 0.0937(0.0958) Grad: 26624.7656  LR: 0.00000035
Epoch: [5][781/782] Elapsed 7m 33s (remain 0m 0s) Loss: 0.3491(0.0960) Grad: 40760.1953  LR: 0.00000089
EVAL: [0/98] Elapsed 0m 1s (remain 1m 40s) Loss: 0.0796(0.0796)
[2022-10-22 13:07:44] - Epoch 5 - avg_train_loss: 0.0960  avg_val_loss: 0.1064  time: 544s
[2022-10-22 13:07:44] - Epoch 5 - Score: 0.4626  Scores: [0.49564680906745134, 0.45087383453872176, 0.41855256237288563, 0.4787587478174927, 0.4670784863098167, 0.4645271377468902]
EVAL: [97/98] Elapsed 1m 29s (remain 0m 0s) Loss: 0.1197(0.1064)
[2022-10-22 13:07:46] - ========== fold: 1 result ==========
[2022-10-22 13:07:46] - Score: 0.4612  Scores: [0.49844838360056604, 0.4504761858304219, 0.42303131172838804, 0.48041523103009914, 0.4682545382042501, 0.44676000802088606]
[2022-10-22 13:07:46] - ========== fold: 2 training ==========
[2022-10-22 13:07:46] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v2-xlarge",
  "attention_dropout": 0.0,
  "attention_head_size": 64,
  "attention_probs_dropout_prob": 0.0,
  "conv_act": "gelu",
  "conv_kernel_size": 3,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 24,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1536,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/782] Elapsed 0m 0s (remain 6m 42s) Loss: 2.8975(2.8975) Grad: inf  LR: 0.00000100
Epoch: [1][100/782] Elapsed 0m 53s (remain 6m 4s) Loss: 0.1492(2.0856) Grad: 45431.3242  LR: 0.00000100
Epoch: [1][200/782] Elapsed 1m 51s (remain 5m 21s) Loss: 0.0684(1.1219) Grad: 26802.5957  LR: 0.00000100
Epoch: [1][300/782] Elapsed 2m 50s (remain 4m 32s) Loss: 0.1158(0.7911) Grad: 25797.5156  LR: 0.00000100
Epoch: [1][400/782] Elapsed 3m 44s (remain 3m 32s) Loss: 0.0975(0.6239) Grad: 35309.3398  LR: 0.00000100
Epoch: [1][500/782] Elapsed 4m 40s (remain 2m 37s) Loss: 0.1919(0.5218) Grad: 44157.3086  LR: 0.00000100
Epoch: [1][600/782] Elapsed 5m 39s (remain 1m 42s) Loss: 0.0821(0.4551) Grad: 20005.2266  LR: 0.00000100
Epoch: [1][700/782] Elapsed 6m 39s (remain 0m 46s) Loss: 0.0954(0.4075) Grad: 13902.7568  LR: 0.00000100
Epoch: [1][781/782] Elapsed 7m 27s (remain 0m 0s) Loss: 0.1254(0.3770) Grad: 26878.7031  LR: 0.00000075
EVAL: [0/98] Elapsed 0m 1s (remain 1m 53s) Loss: 0.1007(0.1007)
[2022-10-22 13:16:55] - Epoch 1 - avg_train_loss: 0.3770  avg_val_loss: 0.1181  time: 542s
[2022-10-22 13:16:55] - Epoch 1 - Score: 0.4878  Scores: [0.5455166286746979, 0.47137699753262197, 0.43379271733821356, 0.5171091426056036, 0.48458440136108216, 0.4747140759045169]
EVAL: [97/98] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0790(0.1181)
[2022-10-22 13:16:55] - Epoch 1 - Save Best Score: 0.4878 Model
Epoch: [2][0/782] Elapsed 0m 0s (remain 7m 34s) Loss: 0.0760(0.0760) Grad: inf  LR: 0.00000077
Epoch: [2][100/782] Elapsed 1m 4s (remain 7m 11s) Loss: 0.1028(0.1105) Grad: 237949.6562  LR: 0.00000077
Epoch: [2][200/782] Elapsed 2m 1s (remain 5m 49s) Loss: 0.2004(0.1111) Grad: 247595.8438  LR: 0.00000077
Epoch: [2][300/782] Elapsed 2m 58s (remain 4m 45s) Loss: 0.1637(0.1100) Grad: 127664.7344  LR: 0.00000077
Epoch: [2][400/782] Elapsed 3m 49s (remain 3m 37s) Loss: 0.1139(0.1065) Grad: 118217.1328  LR: 0.00000077
Epoch: [2][500/782] Elapsed 4m 46s (remain 2m 40s) Loss: 0.0560(0.1061) Grad: 118944.1719  LR: 0.00000077
Epoch: [2][600/782] Elapsed 5m 41s (remain 1m 42s) Loss: 0.0569(0.1057) Grad: 130998.1641  LR: 0.00000077
Epoch: [2][700/782] Elapsed 6m 39s (remain 0m 46s) Loss: 0.1268(0.1053) Grad: 310818.9375  LR: 0.00000077
Epoch: [2][781/782] Elapsed 7m 27s (remain 0m 0s) Loss: 0.0789(0.1047) Grad: 85307.7969  LR: 0.00000022
EVAL: [0/98] Elapsed 0m 1s (remain 1m 54s) Loss: 0.0771(0.0771)
EVAL: [97/98] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0631(0.1081)
[2022-10-22 13:26:01] - Epoch 2 - avg_train_loss: 0.1047  avg_val_loss: 0.1081  time: 542s
[2022-10-22 13:26:01] - Epoch 2 - Score: 0.4663  Scores: [0.501646194384217, 0.46462080158570646, 0.42924381058326216, 0.45119286142472115, 0.4825338160874723, 0.46830271700242737]
[2022-10-22 13:26:01] - Epoch 2 - Save Best Score: 0.4663 Model
Epoch: [3][0/782] Elapsed 0m 0s (remain 12m 29s) Loss: 0.1013(0.1013) Grad: 741071.4375  LR: 0.00000024
Epoch: [3][100/782] Elapsed 0m 56s (remain 6m 23s) Loss: 0.1158(0.1020) Grad: 472044.4688  LR: 0.00000024
Epoch: [3][200/782] Elapsed 1m 55s (remain 5m 34s) Loss: 0.1482(0.1041) Grad: 96345.6094  LR: 0.00000024
Epoch: [3][300/782] Elapsed 2m 56s (remain 4m 42s) Loss: 0.0794(0.1029) Grad: 179726.4531  LR: 0.00000024
Epoch: [3][400/782] Elapsed 3m 52s (remain 3m 40s) Loss: 0.1087(0.1027) Grad: 76721.8672  LR: 0.00000024
Epoch: [3][500/782] Elapsed 4m 52s (remain 2m 43s) Loss: 0.0956(0.1034) Grad: 46905.1953  LR: 0.00000024
Epoch: [3][600/782] Elapsed 5m 49s (remain 1m 45s) Loss: 0.0953(0.1034) Grad: 75642.8594  LR: 0.00000024
Epoch: [3][700/782] Elapsed 6m 41s (remain 0m 46s) Loss: 0.0779(0.1038) Grad: 42316.7383  LR: 0.00000024
Epoch: [3][781/782] Elapsed 7m 27s (remain 0m 0s) Loss: 0.1453(0.1035) Grad: 72357.6172  LR: 0.00000002
EVAL: [0/98] Elapsed 0m 1s (remain 1m 53s) Loss: 0.0860(0.0860)
[2022-10-22 13:35:08] - Epoch 3 - avg_train_loss: 0.1035  avg_val_loss: 0.1094  time: 542s
[2022-10-22 13:35:08] - Epoch 3 - Score: 0.4688  Scores: [0.5008400754572496, 0.4736344741996774, 0.4251095563708559, 0.44558092568494767, 0.49940691817817545, 0.4683730311251761]
EVAL: [97/98] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0697(0.1094)
Epoch: [4][0/782] Elapsed 0m 0s (remain 8m 16s) Loss: 0.0475(0.0475) Grad: 361876.3750  LR: 0.00000001
Epoch: [4][100/782] Elapsed 0m 59s (remain 6m 41s) Loss: 0.0627(0.1000) Grad: 40233.6016  LR: 0.00000001
Epoch: [4][200/782] Elapsed 1m 56s (remain 5m 36s) Loss: 0.0973(0.1022) Grad: 74677.5000  LR: 0.00000001
Epoch: [4][300/782] Elapsed 2m 51s (remain 4m 34s) Loss: 0.0711(0.1032) Grad: 73893.7812  LR: 0.00000001
Epoch: [4][400/782] Elapsed 3m 51s (remain 3m 39s) Loss: 0.0686(0.1041) Grad: 77662.6562  LR: 0.00000001
Epoch: [4][500/782] Elapsed 4m 50s (remain 2m 43s) Loss: 0.1221(0.1035) Grad: 78017.9141  LR: 0.00000001
Epoch: [4][600/782] Elapsed 5m 48s (remain 1m 44s) Loss: 0.1044(0.1027) Grad: 62134.5000  LR: 0.00000001
Epoch: [4][700/782] Elapsed 6m 42s (remain 0m 46s) Loss: 0.0976(0.1027) Grad: 71432.9766  LR: 0.00000001
Epoch: [4][781/782] Elapsed 7m 28s (remain 0m 0s) Loss: 0.0779(0.1023) Grad: 36453.1328  LR: 0.00000038
EVAL: [0/98] Elapsed 0m 1s (remain 1m 54s) Loss: 0.0866(0.0866)
[2022-10-22 13:44:11] - Epoch 4 - avg_train_loss: 0.1023  avg_val_loss: 0.1081  time: 543s
[2022-10-22 13:44:11] - Epoch 4 - Score: 0.4656  Scores: [0.5127814684810859, 0.4645390319562922, 0.42005957943573274, 0.45406962128173306, 0.48198455655699046, 0.46016000318695766]
EVAL: [97/98] Elapsed 1m 34s (remain 0m 0s) Loss: 0.0683(0.1081)
[2022-10-22 13:44:11] - Epoch 4 - Save Best Score: 0.4656 Model
Epoch: [5][0/782] Elapsed 0m 0s (remain 10m 25s) Loss: 0.0508(0.0508) Grad: 434766.6875  LR: 0.00000035
Epoch: [5][100/782] Elapsed 0m 57s (remain 6m 24s) Loss: 0.0815(0.1028) Grad: 144712.6094  LR: 0.00000035
Epoch: [5][200/782] Elapsed 1m 56s (remain 5m 36s) Loss: 0.1274(0.1005) Grad: 216558.6250  LR: 0.00000035
Epoch: [5][300/782] Elapsed 2m 58s (remain 4m 45s) Loss: 0.1868(0.0996) Grad: 172746.9062  LR: 0.00000035
Epoch: [5][400/782] Elapsed 3m 54s (remain 3m 43s) Loss: 0.1247(0.0999) Grad: 100025.5391  LR: 0.00000035
Epoch: [5][500/782] Elapsed 4m 48s (remain 2m 41s) Loss: 0.0764(0.0993) Grad: 63960.4297  LR: 0.00000035
Epoch: [5][600/782] Elapsed 5m 44s (remain 1m 43s) Loss: 0.0790(0.1000) Grad: 36623.7773  LR: 0.00000035
Epoch: [5][700/782] Elapsed 6m 43s (remain 0m 46s) Loss: 0.0994(0.0998) Grad: 67817.3828  LR: 0.00000035
Epoch: [5][781/782] Elapsed 7m 27s (remain 0m 0s) Loss: 0.1424(0.0995) Grad: 73109.1328  LR: 0.00000089
EVAL: [0/98] Elapsed 0m 1s (remain 1m 52s) Loss: 0.1314(0.1314)
[2022-10-22 13:53:17] - Epoch 5 - avg_train_loss: 0.0995  avg_val_loss: 0.1375  time: 542s
[2022-10-22 13:53:17] - Epoch 5 - Score: 0.5273  Scores: [0.5113212523896786, 0.5469707400237804, 0.4773926257141258, 0.5340150192844796, 0.5941611059364902, 0.5001865703381634]
EVAL: [97/98] Elapsed 1m 34s (remain 0m 0s) Loss: 0.1061(0.1375)
[2022-10-22 13:53:19] - ========== fold: 2 result ==========
[2022-10-22 13:53:19] - Score: 0.4656  Scores: [0.5127814684810859, 0.4645390319562922, 0.42005957943573274, 0.45406962128173306, 0.48198455655699046, 0.46016000318695766]
[2022-10-22 13:53:19] - ========== fold: 3 training ==========
[2022-10-22 13:53:19] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v2-xlarge",
  "attention_dropout": 0.0,
  "attention_head_size": 64,
  "attention_probs_dropout_prob": 0.0,
  "conv_act": "gelu",
  "conv_kernel_size": 3,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 24,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1536,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/782] Elapsed 0m 0s (remain 12m 37s) Loss: 2.2692(2.2692) Grad: inf  LR: 0.00000100
Epoch: [1][100/782] Elapsed 0m 59s (remain 6m 44s) Loss: 0.2095(1.4516) Grad: 69171.0547  LR: 0.00000100
Epoch: [1][200/782] Elapsed 1m 59s (remain 5m 44s) Loss: 0.3342(0.8076) Grad: 86287.6875  LR: 0.00000100
Epoch: [1][300/782] Elapsed 2m 54s (remain 4m 39s) Loss: 0.0903(0.5830) Grad: 43462.1992  LR: 0.00000100
Epoch: [1][400/782] Elapsed 3m 50s (remain 3m 38s) Loss: 0.0745(0.4690) Grad: 27918.9863  LR: 0.00000100
Epoch: [1][500/782] Elapsed 4m 47s (remain 2m 41s) Loss: 0.0749(0.3994) Grad: 26370.9199  LR: 0.00000100
Epoch: [1][600/782] Elapsed 5m 45s (remain 1m 44s) Loss: 0.0512(0.3520) Grad: 18232.2734  LR: 0.00000100
Epoch: [1][700/782] Elapsed 6m 42s (remain 0m 46s) Loss: 0.0924(0.3189) Grad: 28173.8848  LR: 0.00000100
Epoch: [1][781/782] Elapsed 7m 29s (remain 0m 0s) Loss: 0.0945(0.2969) Grad: 33850.1797  LR: 0.00000075
EVAL: [0/98] Elapsed 0m 1s (remain 2m 4s) Loss: 0.1712(0.1712)
[2022-10-22 14:02:28] - Epoch 1 - avg_train_loss: 0.2969  avg_val_loss: 0.1133  time: 542s
[2022-10-22 14:02:28] - Epoch 1 - Score: 0.4769  Scores: [0.5268564058890591, 0.46104924819182996, 0.4411406240399818, 0.48492774718371023, 0.5018884649074716, 0.4454734436327166]
[2022-10-22 14:02:28] - Epoch 1 - Save Best Score: 0.4769 Model
EVAL: [97/98] Elapsed 1m 32s (remain 0m 0s) Loss: 0.0794(0.1133)
Epoch: [2][0/782] Elapsed 0m 0s (remain 10m 41s) Loss: 0.0457(0.0457) Grad: 206837.5312  LR: 0.00000077
Epoch: [2][100/782] Elapsed 1m 0s (remain 6m 51s) Loss: 0.1051(0.1002) Grad: 235130.4375  LR: 0.00000077
Epoch: [2][200/782] Elapsed 1m 56s (remain 5m 36s) Loss: 0.1490(0.1018) Grad: 215389.0000  LR: 0.00000077
Epoch: [2][300/782] Elapsed 2m 56s (remain 4m 41s) Loss: 0.0548(0.1023) Grad: 54877.6953  LR: 0.00000077
Epoch: [2][400/782] Elapsed 3m 51s (remain 3m 39s) Loss: 0.1071(0.1026) Grad: 288496.5312  LR: 0.00000077
Epoch: [2][500/782] Elapsed 4m 48s (remain 2m 41s) Loss: 0.0851(0.1039) Grad: 104519.4531  LR: 0.00000077
Epoch: [2][600/782] Elapsed 5m 42s (remain 1m 43s) Loss: 0.0844(0.1044) Grad: 93514.9375  LR: 0.00000077
Epoch: [2][700/782] Elapsed 6m 39s (remain 0m 46s) Loss: 0.0647(0.1045) Grad: 26126.8340  LR: 0.00000077
Epoch: [2][781/782] Elapsed 7m 27s (remain 0m 0s) Loss: 0.0827(0.1045) Grad: 65717.2500  LR: 0.00000022
EVAL: [0/98] Elapsed 0m 1s (remain 2m 1s) Loss: 0.1684(0.1684)
EVAL: [97/98] Elapsed 1m 32s (remain 0m 0s) Loss: 0.0651(0.1091)
[2022-10-22 14:11:32] - Epoch 2 - avg_train_loss: 0.1045  avg_val_loss: 0.1091  time: 539s
[2022-10-22 14:11:32] - Epoch 2 - Score: 0.4678  Scores: [0.49876137410485544, 0.4583973822727025, 0.4330704632750698, 0.47626620059027075, 0.49777891573545247, 0.44250201441274123]
[2022-10-22 14:11:32] - Epoch 2 - Save Best Score: 0.4678 Model
Epoch: [3][0/782] Elapsed 0m 0s (remain 7m 51s) Loss: 0.1145(0.1145) Grad: 449689.7500  LR: 0.00000024
Epoch: [3][100/782] Elapsed 0m 59s (remain 6m 38s) Loss: 0.0613(0.0963) Grad: 203171.2031  LR: 0.00000024
Epoch: [3][200/782] Elapsed 1m 57s (remain 5m 38s) Loss: 0.0990(0.0969) Grad: 74258.4453  LR: 0.00000024
Epoch: [3][300/782] Elapsed 2m 53s (remain 4m 36s) Loss: 0.0823(0.0985) Grad: 64417.0820  LR: 0.00000024
Epoch: [3][400/782] Elapsed 3m 50s (remain 3m 39s) Loss: 0.1455(0.1004) Grad: 79632.4453  LR: 0.00000024
Epoch: [3][500/782] Elapsed 4m 51s (remain 2m 43s) Loss: 0.0787(0.1028) Grad: 45371.8320  LR: 0.00000024
Epoch: [3][600/782] Elapsed 5m 47s (remain 1m 44s) Loss: 0.0497(0.1030) Grad: 45447.6992  LR: 0.00000024
Epoch: [3][700/782] Elapsed 6m 43s (remain 0m 46s) Loss: 0.1155(0.1028) Grad: 96624.1016  LR: 0.00000024
Epoch: [3][781/782] Elapsed 7m 27s (remain 0m 0s) Loss: 0.0834(0.1037) Grad: 64337.6094  LR: 0.00000002
EVAL: [0/98] Elapsed 0m 1s (remain 2m 0s) Loss: 0.1646(0.1646)
EVAL: [97/98] Elapsed 1m 32s (remain 0m 0s) Loss: 0.0674(0.1092)
[2022-10-22 14:20:36] - Epoch 3 - avg_train_loss: 0.1037  avg_val_loss: 0.1092  time: 540s
[2022-10-22 14:20:36] - Epoch 3 - Score: 0.4679  Scores: [0.500894804492608, 0.4559222854973567, 0.43785078615378814, 0.4664752322014939, 0.504867143208088, 0.44139640516429246]
Epoch: [4][0/782] Elapsed 0m 0s (remain 7m 46s) Loss: 0.0701(0.0701) Grad: 431403.1875  LR: 0.00000001
Epoch: [4][100/782] Elapsed 0m 58s (remain 6m 34s) Loss: 0.0797(0.0973) Grad: 626283.0625  LR: 0.00000001
Epoch: [4][200/782] Elapsed 1m 57s (remain 5m 38s) Loss: 0.0734(0.0985) Grad: 51543.6875  LR: 0.00000001
Epoch: [4][300/782] Elapsed 2m 54s (remain 4m 38s) Loss: 0.1001(0.0978) Grad: 53276.1641  LR: 0.00000001
Epoch: [4][400/782] Elapsed 3m 54s (remain 3m 42s) Loss: 0.1165(0.0991) Grad: 61078.7461  LR: 0.00000001
Epoch: [4][500/782] Elapsed 4m 52s (remain 2m 44s) Loss: 0.0619(0.0982) Grad: 60811.0352  LR: 0.00000001
Epoch: [4][600/782] Elapsed 5m 52s (remain 1m 46s) Loss: 0.1793(0.0996) Grad: 102758.0625  LR: 0.00000001
Epoch: [4][700/782] Elapsed 6m 46s (remain 0m 46s) Loss: 0.1155(0.0995) Grad: 57337.5000  LR: 0.00000001
Epoch: [4][781/782] Elapsed 7m 34s (remain 0m 0s) Loss: 0.0704(0.1005) Grad: 53465.6016  LR: 0.00000038
EVAL: [0/98] Elapsed 0m 1s (remain 2m 3s) Loss: 0.2043(0.2043)
EVAL: [97/98] Elapsed 1m 32s (remain 0m 0s) Loss: 0.0759(0.1097)
[2022-10-22 14:29:43] - Epoch 4 - avg_train_loss: 0.1005  avg_val_loss: 0.1097  time: 546s
[2022-10-22 14:29:43] - Epoch 4 - Score: 0.4690  Scores: [0.4963204449606279, 0.45633760344168134, 0.43268939136215295, 0.4719778589924388, 0.5135495846447579, 0.4430126087874284]
Epoch: [5][0/782] Elapsed 0m 0s (remain 8m 42s) Loss: 0.1153(0.1153) Grad: inf  LR: 0.00000035
Epoch: [5][100/782] Elapsed 0m 55s (remain 6m 16s) Loss: 0.1141(0.0918) Grad: 77255.2734  LR: 0.00000035
Epoch: [5][200/782] Elapsed 1m 56s (remain 5m 36s) Loss: 0.1323(0.0979) Grad: 23205.8477  LR: 0.00000035
Epoch: [5][300/782] Elapsed 2m 52s (remain 4m 34s) Loss: 0.0893(0.0993) Grad: 28836.7949  LR: 0.00000035
Epoch: [5][400/782] Elapsed 3m 49s (remain 3m 37s) Loss: 0.0586(0.0980) Grad: 17159.5547  LR: 0.00000035
Epoch: [5][500/782] Elapsed 4m 45s (remain 2m 40s) Loss: 0.1394(0.0977) Grad: 31368.3086  LR: 0.00000035
Epoch: [5][600/782] Elapsed 5m 44s (remain 1m 43s) Loss: 0.0501(0.0985) Grad: 334437.2812  LR: 0.00000035
Epoch: [5][700/782] Elapsed 6m 44s (remain 0m 46s) Loss: 0.1499(0.0987) Grad: 21868.8613  LR: 0.00000035
Epoch: [5][781/782] Elapsed 7m 27s (remain 0m 0s) Loss: 0.0585(0.0981) Grad: 25073.8906  LR: 0.00000089
EVAL: [0/98] Elapsed 0m 1s (remain 2m 0s) Loss: 0.1764(0.1764)
EVAL: [97/98] Elapsed 1m 32s (remain 0m 0s) Loss: 0.0644(0.1112)
[2022-10-22 14:38:43] - Epoch 5 - avg_train_loss: 0.0981  avg_val_loss: 0.1112  time: 540s
[2022-10-22 14:38:43] - Epoch 5 - Score: 0.4725  Scores: [0.48778477620009864, 0.47263118085785166, 0.441355464011653, 0.46925458583993795, 0.5276988478633394, 0.4365599690862123]
[2022-10-22 14:38:45] - ========== fold: 3 result ==========
[2022-10-22 14:38:45] - Score: 0.4678  Scores: [0.49876137410485544, 0.4583973822727025, 0.4330704632750698, 0.47626620059027075, 0.49777891573545247, 0.44250201441274123]
[2022-10-22 14:38:45] - ========== fold: 4 training ==========
[2022-10-22 14:38:45] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v2-xlarge",
  "attention_dropout": 0.0,
  "attention_head_size": 64,
  "attention_probs_dropout_prob": 0.0,
  "conv_act": "gelu",
  "conv_kernel_size": 3,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 24,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1536,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/782] Elapsed 0m 0s (remain 12m 14s) Loss: 3.1315(3.1315) Grad: 1261139.0000  LR: 0.00000100
Epoch: [1][100/782] Elapsed 0m 54s (remain 6m 6s) Loss: 0.1521(1.1608) Grad: 46182.5234  LR: 0.00000100
Epoch: [1][200/782] Elapsed 1m 53s (remain 5m 29s) Loss: 0.1435(0.6609) Grad: 20561.4043  LR: 0.00000100
Epoch: [1][300/782] Elapsed 2m 47s (remain 4m 28s) Loss: 0.1012(0.4823) Grad: 45468.9023  LR: 0.00000100
Epoch: [1][400/782] Elapsed 3m 48s (remain 3m 37s) Loss: 0.1582(0.3928) Grad: 46161.4570  LR: 0.00000100
Epoch: [1][500/782] Elapsed 4m 46s (remain 2m 40s) Loss: 0.0987(0.3396) Grad: 35056.6367  LR: 0.00000100
Epoch: [1][600/782] Elapsed 5m 38s (remain 1m 41s) Loss: 0.1661(0.3033) Grad: 33370.7930  LR: 0.00000100
Epoch: [1][700/782] Elapsed 6m 41s (remain 0m 46s) Loss: 0.1127(0.2770) Grad: 46078.8477  LR: 0.00000100
Epoch: [1][781/782] Elapsed 7m 28s (remain 0m 0s) Loss: 0.2584(0.2610) Grad: 55546.7891  LR: 0.00000075
EVAL: [0/98] Elapsed 0m 2s (remain 3m 23s) Loss: 0.0599(0.0599)
EVAL: [97/98] Elapsed 1m 30s (remain 0m 0s) Loss: 0.0817(0.1120)
[2022-10-22 14:47:50] - Epoch 1 - avg_train_loss: 0.2610  avg_val_loss: 0.1120  time: 539s
[2022-10-22 14:47:50] - Epoch 1 - Score: 0.4747  Scores: [0.504143476166617, 0.44860333956109844, 0.4332715862923193, 0.49739019130023776, 0.4913580651852641, 0.4735828327682819]
[2022-10-22 14:47:50] - Epoch 1 - Save Best Score: 0.4747 Model
Epoch: [2][0/782] Elapsed 0m 0s (remain 11m 10s) Loss: 0.1116(0.1116) Grad: 580363.8125  LR: 0.00000077
Epoch: [2][100/782] Elapsed 0m 58s (remain 6m 37s) Loss: 0.0452(0.1005) Grad: 124577.1641  LR: 0.00000077
Epoch: [2][200/782] Elapsed 1m 57s (remain 5m 38s) Loss: 0.1250(0.1038) Grad: 180486.0938  LR: 0.00000077
Epoch: [2][300/782] Elapsed 2m 53s (remain 4m 36s) Loss: 0.1347(0.1050) Grad: 188528.7344  LR: 0.00000077
Epoch: [2][400/782] Elapsed 3m 53s (remain 3m 42s) Loss: 0.1241(0.1057) Grad: 59956.4453  LR: 0.00000077
Epoch: [2][500/782] Elapsed 4m 49s (remain 2m 42s) Loss: 0.1595(0.1060) Grad: 46403.5156  LR: 0.00000077
Epoch: [2][600/782] Elapsed 5m 45s (remain 1m 44s) Loss: 0.0580(0.1064) Grad: 47565.8867  LR: 0.00000077
Epoch: [2][700/782] Elapsed 6m 42s (remain 0m 46s) Loss: 0.1560(0.1065) Grad: 52488.6133  LR: 0.00000077
Epoch: [2][781/782] Elapsed 7m 30s (remain 0m 0s) Loss: 0.0856(0.1073) Grad: 37088.3867  LR: 0.00000022
EVAL: [0/98] Elapsed 0m 2s (remain 3m 25s) Loss: 0.0641(0.0641)
EVAL: [97/98] Elapsed 1m 30s (remain 0m 0s) Loss: 0.0779(0.1127)
[2022-10-22 14:56:55] - Epoch 2 - avg_train_loss: 0.1073  avg_val_loss: 0.1127  time: 541s
[2022-10-22 14:56:55] - Epoch 2 - Score: 0.4766  Scores: [0.5062951797855822, 0.46334704831064494, 0.4321821965031108, 0.4710199571354476, 0.5123497370574311, 0.4745076813026901]
Epoch: [3][0/782] Elapsed 0m 0s (remain 8m 42s) Loss: 0.0777(0.0777) Grad: 344391.0312  LR: 0.00000024
Epoch: [3][100/782] Elapsed 1m 1s (remain 6m 53s) Loss: 0.1110(0.1097) Grad: 141573.7969  LR: 0.00000024
Epoch: [3][200/782] Elapsed 1m 55s (remain 5m 32s) Loss: 0.0820(0.1084) Grad: 72887.5547  LR: 0.00000024
Epoch: [3][300/782] Elapsed 2m 56s (remain 4m 41s) Loss: 0.1004(0.1081) Grad: inf  LR: 0.00000024
Epoch: [3][400/782] Elapsed 3m 52s (remain 3m 41s) Loss: 0.1161(0.1061) Grad: 21186.0840  LR: 0.00000024
Epoch: [3][500/782] Elapsed 4m 50s (remain 2m 42s) Loss: 0.1204(0.1066) Grad: 13562.1328  LR: 0.00000024
Epoch: [3][600/782] Elapsed 5m 48s (remain 1m 45s) Loss: 0.0670(0.1064) Grad: 14639.6396  LR: 0.00000024
Epoch: [3][700/782] Elapsed 6m 47s (remain 0m 47s) Loss: 0.0710(0.1067) Grad: 12217.2422  LR: 0.00000024
Epoch: [3][781/782] Elapsed 7m 33s (remain 0m 0s) Loss: 0.0865(0.1065) Grad: 20007.0371  LR: 0.00000002
EVAL: [0/98] Elapsed 0m 2s (remain 3m 22s) Loss: 0.0573(0.0573)
EVAL: [97/98] Elapsed 1m 30s (remain 0m 0s) Loss: 0.0696(0.1089)
[2022-10-22 15:05:59] - Epoch 3 - avg_train_loss: 0.1065  avg_val_loss: 0.1089  time: 544s
[2022-10-22 15:05:59] - Epoch 3 - Score: 0.4684  Scores: [0.4988298127630346, 0.44652852189681896, 0.429089831799555, 0.47010338747883695, 0.4977041194875703, 0.4678656526868151]
[2022-10-22 15:05:59] - Epoch 3 - Save Best Score: 0.4684 Model
Epoch: [4][0/782] Elapsed 0m 1s (remain 18m 15s) Loss: 0.1339(0.1339) Grad: inf  LR: 0.00000001
Epoch: [4][100/782] Elapsed 0m 59s (remain 6m 40s) Loss: 0.1288(0.1023) Grad: 480868.9375  LR: 0.00000001
Epoch: [4][200/782] Elapsed 1m 53s (remain 5m 28s) Loss: 0.0918(0.0968) Grad: 162109.1562  LR: 0.00000001
Epoch: [4][300/782] Elapsed 2m 51s (remain 4m 33s) Loss: 0.1322(0.0991) Grad: 94267.5156  LR: 0.00000001
Epoch: [4][400/782] Elapsed 3m 47s (remain 3m 36s) Loss: 0.0579(0.0986) Grad: 180691.5000  LR: 0.00000001
Epoch: [4][500/782] Elapsed 4m 48s (remain 2m 41s) Loss: 0.1503(0.0988) Grad: 186456.1250  LR: 0.00000001
Epoch: [4][600/782] Elapsed 5m 45s (remain 1m 43s) Loss: 0.0569(0.0981) Grad: 72959.8594  LR: 0.00000001
Epoch: [4][700/782] Elapsed 6m 44s (remain 0m 46s) Loss: 0.0899(0.0977) Grad: 158889.1094  LR: 0.00000001
Epoch: [4][781/782] Elapsed 7m 30s (remain 0m 0s) Loss: 0.0825(0.0972) Grad: 48211.2461  LR: 0.00000038
EVAL: [0/98] Elapsed 0m 2s (remain 3m 21s) Loss: 0.0555(0.0555)
[2022-10-22 15:15:05] - Epoch 4 - avg_train_loss: 0.0972  avg_val_loss: 0.1054  time: 541s
[2022-10-22 15:15:05] - Epoch 4 - Score: 0.4607  Scores: [0.4913368412214786, 0.44448092689129115, 0.4214806212269019, 0.463363667439652, 0.4784667483048276, 0.4647788528257289]
EVAL: [97/98] Elapsed 1m 30s (remain 0m 0s) Loss: 0.0790(0.1054)
[2022-10-22 15:15:05] - Epoch 4 - Save Best Score: 0.4607 Model
Epoch: [5][0/782] Elapsed 0m 0s (remain 12m 36s) Loss: 0.0869(0.0869) Grad: 347715.6250  LR: 0.00000035
Epoch: [5][100/782] Elapsed 0m 54s (remain 6m 8s) Loss: 0.1039(0.0953) Grad: 97595.0938  LR: 0.00000035
Epoch: [5][200/782] Elapsed 1m 53s (remain 5m 29s) Loss: 0.0782(0.0939) Grad: 190765.9219  LR: 0.00000035
Epoch: [5][300/782] Elapsed 2m 47s (remain 4m 27s) Loss: 0.0712(0.0954) Grad: 67456.8359  LR: 0.00000035
Epoch: [5][400/782] Elapsed 3m 48s (remain 3m 37s) Loss: 0.0787(0.0956) Grad: 84890.2891  LR: 0.00000035
Epoch: [5][500/782] Elapsed 4m 49s (remain 2m 42s) Loss: 0.0864(0.0962) Grad: 45537.3984  LR: 0.00000035
Epoch: [5][600/782] Elapsed 5m 47s (remain 1m 44s) Loss: 0.0988(0.0962) Grad: 61984.9961  LR: 0.00000035
Epoch: [5][700/782] Elapsed 6m 41s (remain 0m 46s) Loss: 0.0576(0.0961) Grad: 34171.6523  LR: 0.00000035
Epoch: [5][781/782] Elapsed 7m 29s (remain 0m 0s) Loss: 0.0878(0.0958) Grad: 40336.3906  LR: 0.00000089
EVAL: [0/98] Elapsed 0m 2s (remain 3m 22s) Loss: 0.0758(0.0758)
EVAL: [97/98] Elapsed 1m 30s (remain 0m 0s) Loss: 0.0871(0.1081)
[2022-10-22 15:24:10] - Epoch 5 - avg_train_loss: 0.0958  avg_val_loss: 0.1081  time: 540s
[2022-10-22 15:24:10] - Epoch 5 - Score: 0.4668  Scores: [0.49293508788878554, 0.4478904446428846, 0.43463996845130054, 0.4743684196103036, 0.4922508378858482, 0.4584487911375996]
[2022-10-22 15:24:11] - ========== fold: 4 result ==========
[2022-10-22 15:24:11] - Score: 0.4607  Scores: [0.4913368412214786, 0.44448092689129115, 0.4214806212269019, 0.463363667439652, 0.4784667483048276, 0.4647788528257289]
[2022-10-22 15:24:11] - ========== CV ==========
[2022-10-22 15:24:11] - Score: 0.4633  Scores: [0.49883520657179037, 0.45769098080011594, 0.4223143824101458, 0.46596904579693954, 0.4812833690445369, 0.4539795212609703]