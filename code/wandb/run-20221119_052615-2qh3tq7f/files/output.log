(15594, 9)
(15142, 9)
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3911/3911 [00:04<00:00, 963.99it/s]
[2022-11-19 05:26:22] - comment: deberta-v3-base, add 2021data kunisho data
[2022-11-19 05:26:22] - max_len: 2048
[2022-11-19 05:26:22] - ========== fold: 0 training ==========
[2022-11-19 05:26:22] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Reinitializing Last 1 Layers ...
Done.!
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/952] Elapsed 0m 1s (remain 27m 58s) Loss: 3.3869(3.3869) Grad: inf  LR: 0.00002994
Epoch: [1][100/952] Elapsed 1m 58s (remain 16m 34s) Loss: 0.0389(0.4060) Grad: 57065.9727  LR: 0.00002994
Epoch: [1][200/952] Elapsed 4m 5s (remain 15m 16s) Loss: 0.0147(0.2229) Grad: 41875.6250  LR: 0.00002994
Epoch: [1][300/952] Elapsed 6m 11s (remain 13m 24s) Loss: 0.0576(0.1596) Grad: 103906.9922  LR: 0.00002994
Epoch: [1][400/952] Elapsed 8m 15s (remain 11m 20s) Loss: 0.0315(0.1267) Grad: 103059.1094  LR: 0.00002994
Epoch: [1][500/952] Elapsed 10m 19s (remain 9m 17s) Loss: 0.0076(0.1070) Grad: 29180.9902  LR: 0.00002994
Epoch: [1][600/952] Elapsed 12m 32s (remain 7m 19s) Loss: 0.0155(0.0935) Grad: 51186.1523  LR: 0.00002994
Epoch: [1][700/952] Elapsed 14m 28s (remain 5m 10s) Loss: 0.0621(0.0840) Grad: 104184.5234  LR: 0.00002994
Epoch: [1][800/952] Elapsed 16m 26s (remain 3m 5s) Loss: 0.0221(0.0763) Grad: 40823.6758  LR: 0.00002994
Epoch: [1][900/952] Elapsed 18m 31s (remain 1m 2s) Loss: 0.0337(0.0705) Grad: 54681.0898  LR: 0.00002994
Epoch: [1][951/952] Elapsed 19m 28s (remain 0m 0s) Loss: 0.0332(0.0681) Grad: 48066.2383  LR: 0.00000057
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 0.1048(0.1048)
[2022-11-19 05:46:31] - Epoch 1 - avg_train_loss: 0.0681  avg_val_loss: 0.1005  time: 1206s
[2022-11-19 05:46:31] - Epoch 1 - Score: 0.4486  Scores: [0.4822736062310806, 0.45690174362755975, 0.4003335024526629, 0.44852883690466766, 0.46444313964254247, 0.4391560099714694]
[2022-11-19 05:46:31] - Epoch 1 - Save Best Score: 0.4486 Model
EVAL: [24/25] Elapsed 0m 36s (remain 0m 0s) Loss: 0.1039(0.1005)
Epoch: [2][0/952] Elapsed 0m 2s (remain 32m 58s) Loss: 0.0205(0.0205) Grad: 49556.0430  LR: 0.00000069
Epoch: [2][100/952] Elapsed 2m 8s (remain 18m 6s) Loss: 0.0276(0.0225) Grad: 56235.1758  LR: 0.00000069
Epoch: [2][200/952] Elapsed 4m 17s (remain 16m 3s) Loss: 0.0036(0.0235) Grad: 45883.9570  LR: 0.00000069
Epoch: [2][300/952] Elapsed 6m 22s (remain 13m 47s) Loss: 0.0308(0.0238) Grad: 69768.4531  LR: 0.00000069
Epoch: [2][400/952] Elapsed 8m 16s (remain 11m 21s) Loss: 0.0025(0.0239) Grad: 14531.3184  LR: 0.00000069
Epoch: [2][500/952] Elapsed 10m 20s (remain 9m 18s) Loss: 0.0152(0.0238) Grad: 43257.6367  LR: 0.00000069
Epoch: [2][600/952] Elapsed 12m 19s (remain 7m 12s) Loss: 0.0277(0.0235) Grad: 103927.3828  LR: 0.00000069
Epoch: [2][700/952] Elapsed 14m 19s (remain 5m 7s) Loss: 0.0058(0.0233) Grad: 27183.2285  LR: 0.00000069
Epoch: [2][800/952] Elapsed 16m 19s (remain 3m 4s) Loss: 0.0434(0.0232) Grad: 100827.2656  LR: 0.00000069
Epoch: [2][900/952] Elapsed 18m 28s (remain 1m 2s) Loss: 0.0149(0.0232) Grad: 42034.9531  LR: 0.00000069
Epoch: [2][951/952] Elapsed 19m 26s (remain 0m 0s) Loss: 0.0159(0.0233) Grad: 61961.2891  LR: 0.00002933
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 0.1047(0.1047)
[2022-11-19 06:06:38] - Epoch 2 - avg_train_loss: 0.0233  avg_val_loss: 0.1016  time: 1203s
[2022-11-19 06:06:38] - Epoch 2 - Score: 0.4510  Scores: [0.4947066503218633, 0.45713325801419835, 0.4065670104384549, 0.44318741722707955, 0.4560965814349918, 0.44824929822869847]
EVAL: [24/25] Elapsed 0m 36s (remain 0m 0s) Loss: 0.1057(0.1016)
Epoch: [3][0/952] Elapsed 0m 1s (remain 16m 28s) Loss: 0.0125(0.0125) Grad: 83625.6953  LR: 0.00002901
Epoch: [3][100/952] Elapsed 2m 8s (remain 18m 4s) Loss: 0.0339(0.0225) Grad: 70806.3203  LR: 0.00002901
Epoch: [3][200/952] Elapsed 4m 16s (remain 15m 56s) Loss: 0.0433(0.0240) Grad: 51542.5977  LR: 0.00002901
Epoch: [3][300/952] Elapsed 6m 9s (remain 13m 19s) Loss: 0.0302(0.0232) Grad: 82295.1562  LR: 0.00002901
Traceback (most recent call last):
  File "/notebooks/code/exp059.py", line 828, in <module>
    main()
  File "/notebooks/code/exp059.py", line 766, in main
    _oof_df = train_loop(train2, train, fold) # ★★★
  File "/notebooks/code/exp059.py", line 657, in train_loop
    avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)
  File "/notebooks/code/exp059.py", line 486, in train_fn
    scaler.scale(loss).backward()
  File "/usr/local/lib/python3.9/dist-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt