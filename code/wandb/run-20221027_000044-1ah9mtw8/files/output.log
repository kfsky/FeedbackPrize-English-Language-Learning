Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3911/3911 [00:03<00:00, 987.13it/s]
[2022-10-27 00:00:52] - max_len: 2048
[2022-10-27 00:00:52] - ========== fold: 0 training ==========
[2022-10-27 00:00:52] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Reinitializing Last 2 Layers ...
Done.!
Epoch: [1][0/195] Elapsed 0m 2s (remain 7m 54s) Loss: 2.6779(2.6779) Grad: inf  LR: 0.00000100
Epoch: [1][100/195] Elapsed 2m 10s (remain 2m 1s) Loss: 1.6836(2.1693) Grad: 394542.2500  LR: 0.00000100
Epoch: [1][194/195] Elapsed 4m 9s (remain 0m 0s) Loss: 0.1888(1.4286) Grad: 56756.9336  LR: 0.00000099
EVAL: [0/25] Elapsed 0m 1s (remain 0m 30s) Loss: 0.2325(0.2325)
EVAL: [24/25] Elapsed 0m 36s (remain 0m 0s) Loss: 0.2303(0.2077)
[2022-10-27 00:05:42] - Epoch 1 - avg_train_loss: 1.4286  avg_val_loss: 0.2077  time: 287s
[2022-10-27 00:05:42] - Epoch 1 - Score: 0.6533  Scores: [0.6430173713959959, 0.6321503297894128, 0.5465652293361575, 0.6773168559738945, 0.6965138632198008, 0.7239949357973754]
[2022-10-27 00:05:42] - Epoch 1 - Save Best Score: 0.6533 Model
Epoch: [2][0/195] Elapsed 0m 1s (remain 3m 24s) Loss: 0.1570(0.1570) Grad: 117629.0703  LR: 0.00000099
Epoch: [2][100/195] Elapsed 2m 5s (remain 1m 57s) Loss: 0.2220(0.1936) Grad: 78924.2891  LR: 0.00000099
Epoch: [2][194/195] Elapsed 4m 8s (remain 0m 0s) Loss: 0.1436(0.1761) Grad: 93257.9141  LR: 0.00000093
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 0.1571(0.1571)
EVAL: [24/25] Elapsed 0m 36s (remain 0m 0s) Loss: 0.1524(0.1471)
[2022-10-27 00:10:28] - Epoch 2 - avg_train_loss: 0.1761  avg_val_loss: 0.1471  time: 285s
[2022-10-27 00:10:28] - Epoch 2 - Score: 0.5445  Scores: [0.5671874616184637, 0.529536695361741, 0.4624467354998808, 0.5387734360841311, 0.564697735483419, 0.6045590889623923]
[2022-10-27 00:10:28] - Epoch 2 - Save Best Score: 0.5445 Model
Epoch: [3][0/195] Elapsed 0m 1s (remain 3m 23s) Loss: 0.1192(0.1192) Grad: 253749.7031  LR: 0.00000095
Epoch: [3][100/195] Elapsed 2m 5s (remain 1m 56s) Loss: 0.1067(0.1360) Grad: 190643.1875  LR: 0.00000095
Epoch: [3][194/195] Elapsed 4m 6s (remain 0m 0s) Loss: 0.1723(0.1350) Grad: 360692.1562  LR: 0.00000084
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 0.1263(0.1263)
[2022-10-27 00:15:12] - Epoch 3 - avg_train_loss: 0.1350  avg_val_loss: 0.1245  time: 282s
[2022-10-27 00:15:12] - Epoch 3 - Score: 0.5011  Scores: [0.5286300399646428, 0.4869579288514147, 0.4535512401112192, 0.4998982342454932, 0.5183678590151011, 0.5190175087901415]
[2022-10-27 00:15:12] - Epoch 3 - Save Best Score: 0.5011 Model
EVAL: [24/25] Elapsed 0m 36s (remain 0m 0s) Loss: 0.1066(0.1245)
Epoch: [4][0/195] Elapsed 0m 1s (remain 5m 0s) Loss: 0.1229(0.1229) Grad: 146864.6094  LR: 0.00000086
Epoch: [4][100/195] Elapsed 2m 10s (remain 2m 1s) Loss: 0.1153(0.1242) Grad: 261262.3906  LR: 0.00000086
Epoch: [4][194/195] Elapsed 4m 6s (remain 0m 0s) Loss: 0.1312(0.1232) Grad: 207479.0312  LR: 0.00000072
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 0.1239(0.1239)
EVAL: [24/25] Elapsed 0m 36s (remain 0m 0s) Loss: 0.1012(0.1198)
[2022-10-27 00:19:56] - Epoch 4 - avg_train_loss: 0.1232  avg_val_loss: 0.1198  time: 283s
[2022-10-27 00:19:56] - Epoch 4 - Score: 0.4907  Scores: [0.5183120813976791, 0.4738401714409247, 0.43829625620875035, 0.49445180535251787, 0.5164830186619682, 0.5025381912818112]
[2022-10-27 00:19:56] - Epoch 4 - Save Best Score: 0.4907 Model
Epoch: [5][0/195] Elapsed 0m 1s (remain 4m 41s) Loss: 0.1071(0.1071) Grad: 294483.3750  LR: 0.00000074
Epoch: [5][100/195] Elapsed 2m 7s (remain 1m 58s) Loss: 0.1145(0.1198) Grad: 155797.8750  LR: 0.00000074
Epoch: [5][194/195] Elapsed 4m 5s (remain 0m 0s) Loss: 0.1223(0.1177) Grad: 131539.6875  LR: 0.00000058
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 0.1183(0.1183)
EVAL: [24/25] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0990(0.1139)
[2022-10-27 00:24:39] - Epoch 5 - avg_train_loss: 0.1177  avg_val_loss: 0.1139  time: 282s
[2022-10-27 00:24:39] - Epoch 5 - Score: 0.4785  Scores: [0.5078131657054068, 0.46886480167569683, 0.4347273724029814, 0.47449327680800363, 0.49718124360716903, 0.48783738943780414]
[2022-10-27 00:24:39] - Epoch 5 - Save Best Score: 0.4785 Model
[2022-10-27 00:24:41] - ========== fold: 0 result ==========
[2022-10-27 00:24:41] - Score: 0.4785  Scores: [0.5078131657054068, 0.46886480167569683, 0.4347273724029814, 0.47449327680800363, 0.49718124360716903, 0.48783738943780414]
[2022-10-27 00:24:41] - ========== fold: 1 training ==========
[2022-10-27 00:24:41] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Reinitializing Last 2 Layers ...
Done.!
Epoch: [1][0/195] Elapsed 0m 1s (remain 4m 46s) Loss: 2.3061(2.3061) Grad: inf  LR: 0.00000100
Epoch: [1][100/195] Elapsed 2m 5s (remain 1m 56s) Loss: 1.1931(2.0137) Grad: 365810.2812  LR: 0.00000100
Epoch: [1][194/195] Elapsed 4m 4s (remain 0m 0s) Loss: 0.1895(1.3228) Grad: 75245.8750  LR: 0.00000099
EVAL: [0/25] Elapsed 0m 2s (remain 0m 52s) Loss: 0.2841(0.2841)
EVAL: [24/25] Elapsed 0m 37s (remain 0m 0s) Loss: 0.1926(0.2113)
[2022-10-27 00:29:26] - Epoch 1 - avg_train_loss: 1.3228  avg_val_loss: 0.2113  time: 282s
[2022-10-27 00:29:26] - Epoch 1 - Score: 0.6620  Scores: [0.6359962105297692, 0.676417335940682, 0.5909659940282745, 0.6570022031240951, 0.7186703467374103, 0.6928377161883952]
[2022-10-27 00:29:26] - Epoch 1 - Save Best Score: 0.6620 Model
Epoch: [2][0/195] Elapsed 0m 1s (remain 4m 29s) Loss: 0.2363(0.2363) Grad: 166845.1719  LR: 0.00000099
Epoch: [2][100/195] Elapsed 2m 8s (remain 1m 59s) Loss: 0.2850(0.1799) Grad: 993438.6250  LR: 0.00000099
Epoch: [2][194/195] Elapsed 4m 3s (remain 0m 0s) Loss: 0.1730(0.1678) Grad: 198584.4688  LR: 0.00000093
EVAL: [0/25] Elapsed 0m 2s (remain 0m 53s) Loss: 0.1929(0.1929)
EVAL: [24/25] Elapsed 0m 37s (remain 0m 0s) Loss: 0.1496(0.1418)
[2022-10-27 00:34:09] - Epoch 2 - avg_train_loss: 0.1678  avg_val_loss: 0.1418  time: 282s
[2022-10-27 00:34:09] - Epoch 2 - Score: 0.5370  Scores: [0.5531477101812519, 0.5212609664095292, 0.4931545504668967, 0.5418485816804703, 0.5545224436553314, 0.5580573173954012]
[2022-10-27 00:34:09] - Epoch 2 - Save Best Score: 0.5370 Model
Epoch: [3][0/195] Elapsed 0m 2s (remain 8m 3s) Loss: 0.0919(0.0919) Grad: 100122.2422  LR: 0.00000095
Epoch: [3][100/195] Elapsed 2m 4s (remain 1m 56s) Loss: 0.1497(0.1407) Grad: 131221.5625  LR: 0.00000095
Epoch: [3][194/195] Elapsed 4m 4s (remain 0m 0s) Loss: 0.1118(0.1307) Grad: 120467.9844  LR: 0.00000084
EVAL: [0/25] Elapsed 0m 2s (remain 0m 53s) Loss: 0.1557(0.1557)
EVAL: [24/25] Elapsed 0m 37s (remain 0m 0s) Loss: 0.1179(0.1244)
[2022-10-27 00:38:53] - Epoch 3 - avg_train_loss: 0.1307  avg_val_loss: 0.1244  time: 283s
[2022-10-27 00:38:53] - Epoch 3 - Score: 0.5011  Scores: [0.5283842125779867, 0.48493731645156324, 0.45921737057593137, 0.5069625211119818, 0.5217494327851209, 0.5052117387695588]
[2022-10-27 00:38:53] - Epoch 3 - Save Best Score: 0.5011 Model
Epoch: [4][0/195] Elapsed 0m 1s (remain 4m 30s) Loss: 0.1287(0.1287) Grad: 106743.0625  LR: 0.00000086
Epoch: [4][100/195] Elapsed 2m 5s (remain 1m 56s) Loss: 0.0972(0.1188) Grad: 174691.6562  LR: 0.00000086
Epoch: [4][194/195] Elapsed 4m 0s (remain 0m 0s) Loss: 0.1014(0.1180) Grad: 172975.1562  LR: 0.00000072
EVAL: [0/25] Elapsed 0m 2s (remain 0m 54s) Loss: 0.1467(0.1467)
[2022-10-27 00:43:32] - Epoch 4 - avg_train_loss: 0.1180  avg_val_loss: 0.1192  time: 279s
[2022-10-27 00:43:32] - Epoch 4 - Score: 0.4901  Scores: [0.5205014372461279, 0.47351715417535833, 0.44687333799858175, 0.4970421445795351, 0.5174665136109324, 0.48545495737685107]
EVAL: [24/25] Elapsed 0m 37s (remain 0m 0s) Loss: 0.1097(0.1192)
[2022-10-27 00:43:32] - Epoch 4 - Save Best Score: 0.4901 Model
Epoch: [5][0/195] Elapsed 0m 2s (remain 7m 12s) Loss: 0.1059(0.1059) Grad: 159755.7656  LR: 0.00000074
Epoch: [5][100/195] Elapsed 1m 59s (remain 1m 51s) Loss: 0.1208(0.1122) Grad: 165915.2188  LR: 0.00000074
Epoch: [5][194/195] Elapsed 4m 0s (remain 0m 0s) Loss: 0.0940(0.1128) Grad: 239827.8281  LR: 0.00000058
EVAL: [0/25] Elapsed 0m 2s (remain 0m 53s) Loss: 0.1334(0.1334)
EVAL: [24/25] Elapsed 0m 37s (remain 0m 0s) Loss: 0.1113(0.1159)
[2022-10-27 00:48:12] - Epoch 5 - avg_train_loss: 0.1128  avg_val_loss: 0.1159  time: 278s
[2022-10-27 00:48:12] - Epoch 5 - Score: 0.4830  Scores: [0.5145925798260549, 0.4699312882046595, 0.43960446415264365, 0.49162385121126695, 0.5058179116888496, 0.4762632709338611]
[2022-10-27 00:48:12] - Epoch 5 - Save Best Score: 0.4830 Model
[2022-10-27 00:48:14] - ========== fold: 1 result ==========
[2022-10-27 00:48:14] - Score: 0.4830  Scores: [0.5145925798260549, 0.4699312882046595, 0.43960446415264365, 0.49162385121126695, 0.5058179116888496, 0.4762632709338611]
[2022-10-27 00:48:14] - ========== fold: 2 training ==========
[2022-10-27 00:48:14] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Reinitializing Last 2 Layers ...
Done.!
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/195] Elapsed 0m 1s (remain 4m 28s) Loss: 2.6166(2.6166) Grad: inf  LR: 0.00000100
Epoch: [1][100/195] Elapsed 2m 5s (remain 1m 57s) Loss: 1.5595(2.1078) Grad: 343603.9688  LR: 0.00000100
Epoch: [1][194/195] Elapsed 4m 1s (remain 0m 0s) Loss: 0.2534(1.5053) Grad: 126177.4062  LR: 0.00000099
EVAL: [0/25] Elapsed 0m 2s (remain 0m 54s) Loss: 0.2508(0.2508)
EVAL: [24/25] Elapsed 0m 40s (remain 0m 0s) Loss: 0.1984(0.3091)
[2022-10-27 00:52:57] - Epoch 1 - avg_train_loss: 1.5053  avg_val_loss: 0.3091  time: 282s
[2022-10-27 00:52:57] - Epoch 1 - Score: 0.8232  Scores: [0.8136615040079882, 0.855218750575915, 0.8997839426256581, 0.7474365472388064, 0.8141984969822601, 0.8087906966970037]
[2022-10-27 00:52:57] - Epoch 1 - Save Best Score: 0.8232 Model
Epoch: [2][0/195] Elapsed 0m 1s (remain 3m 37s) Loss: 0.2372(0.2372) Grad: 166651.9375  LR: 0.00000099
Epoch: [2][100/195] Elapsed 2m 11s (remain 2m 2s) Loss: 0.1386(0.1911) Grad: 177298.3125  LR: 0.00000099
Epoch: [2][194/195] Elapsed 4m 2s (remain 0m 0s) Loss: 0.1113(0.1694) Grad: 77618.2969  LR: 0.00000093
EVAL: [0/25] Elapsed 0m 1s (remain 0m 47s) Loss: 0.1074(0.1074)
EVAL: [24/25] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0817(0.1385)
[2022-10-27 00:57:42] - Epoch 2 - avg_train_loss: 0.1694  avg_val_loss: 0.1385  time: 283s
[2022-10-27 00:57:42] - Epoch 2 - Score: 0.5303  Scores: [0.5802486434147786, 0.5257562368713511, 0.4779926049665261, 0.5021900536408082, 0.5408507723982273, 0.5545543341208502]
[2022-10-27 00:57:42] - Epoch 2 - Save Best Score: 0.5303 Model
Epoch: [3][0/195] Elapsed 0m 1s (remain 6m 24s) Loss: 0.1254(0.1254) Grad: 171190.9844  LR: 0.00000095
Epoch: [3][100/195] Elapsed 2m 5s (remain 1m 56s) Loss: 0.1290(0.1267) Grad: 142890.9062  LR: 0.00000095
Epoch: [3][194/195] Elapsed 3m 59s (remain 0m 0s) Loss: 0.0902(0.1258) Grad: 204399.5938  LR: 0.00000084
EVAL: [0/25] Elapsed 0m 1s (remain 0m 44s) Loss: 0.1029(0.1029)
EVAL: [24/25] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0772(0.1271)
[2022-10-27 01:02:22] - Epoch 3 - avg_train_loss: 0.1258  avg_val_loss: 0.1271  time: 279s
[2022-10-27 01:02:22] - Epoch 3 - Score: 0.5066  Scores: [0.5539250852979768, 0.498929600566931, 0.4643582587563226, 0.47352851875607443, 0.5220822005854151, 0.5267957762096308]
[2022-10-27 01:02:22] - Epoch 3 - Save Best Score: 0.5066 Model
Epoch: [4][0/195] Elapsed 0m 1s (remain 4m 43s) Loss: 0.1520(0.1520) Grad: 307718.2812  LR: 0.00000086
Epoch: [4][100/195] Elapsed 2m 8s (remain 1m 59s) Loss: 0.1013(0.1213) Grad: 115330.2812  LR: 0.00000086
Epoch: [4][194/195] Elapsed 3m 57s (remain 0m 0s) Loss: 0.1364(0.1188) Grad: 114413.4453  LR: 0.00000072
EVAL: [0/25] Elapsed 0m 1s (remain 0m 46s) Loss: 0.1041(0.1041)
EVAL: [24/25] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0782(0.1225)
[2022-10-27 01:07:01] - Epoch 4 - avg_train_loss: 0.1188  avg_val_loss: 0.1225  time: 278s
[2022-10-27 01:07:01] - Epoch 4 - Score: 0.4968  Scores: [0.545658620138712, 0.49024964922286374, 0.44820257192546326, 0.4682977287675154, 0.5112631285138076, 0.5172593040210907]
[2022-10-27 01:07:01] - Epoch 4 - Save Best Score: 0.4968 Model
Epoch: [5][0/195] Elapsed 0m 1s (remain 4m 43s) Loss: 0.1345(0.1345) Grad: 172593.6875  LR: 0.00000074
Epoch: [5][100/195] Elapsed 2m 7s (remain 1m 58s) Loss: 0.1195(0.1174) Grad: 159172.2188  LR: 0.00000074
Epoch: [5][194/195] Elapsed 3m 58s (remain 0m 0s) Loss: 0.1000(0.1145) Grad: 258741.8281  LR: 0.00000058
EVAL: [0/25] Elapsed 0m 1s (remain 0m 45s) Loss: 0.1005(0.1005)
EVAL: [24/25] Elapsed 0m 39s (remain 0m 0s) Loss: 0.0791(0.1187)
[2022-10-27 01:11:40] - Epoch 5 - avg_train_loss: 0.1145  avg_val_loss: 0.1187  time: 278s
[2022-10-27 01:11:40] - Epoch 5 - Score: 0.4889  Scores: [0.5370180584951528, 0.4847947029279949, 0.44042861602046185, 0.4606595292610295, 0.5050681607695005, 0.5055713043294952]
[2022-10-27 01:11:40] - Epoch 5 - Save Best Score: 0.4889 Model
[2022-10-27 01:11:42] - ========== fold: 2 result ==========
[2022-10-27 01:11:42] - Score: 0.4889  Scores: [0.5370180584951528, 0.4847947029279949, 0.44042861602046185, 0.4606595292610295, 0.5050681607695005, 0.5055713043294952]
[2022-10-27 01:11:42] - ========== fold: 3 training ==========
[2022-10-27 01:11:42] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Reinitializing Last 2 Layers ...
Done.!
Epoch: [1][0/195] Elapsed 0m 1s (remain 4m 42s) Loss: 2.8269(2.8269) Grad: inf  LR: 0.00000100
Epoch: [1][100/195] Elapsed 2m 6s (remain 1m 57s) Loss: 1.4692(2.2274) Grad: 417343.4375  LR: 0.00000100
Epoch: [1][194/195] Elapsed 4m 3s (remain 0m 0s) Loss: 0.2260(1.5516) Grad: 79974.3359  LR: 0.00000099
EVAL: [0/25] Elapsed 0m 2s (remain 0m 52s) Loss: 0.2354(0.2354)
EVAL: [24/25] Elapsed 0m 39s (remain 0m 0s) Loss: 0.2573(0.2989)
[2022-10-27 01:16:27] - Epoch 1 - avg_train_loss: 1.5516  avg_val_loss: 0.2989  time: 283s
[2022-10-27 01:16:27] - Epoch 1 - Score: 0.8120  Scores: [0.7989312182834094, 0.7564246863167379, 0.7696668426354978, 0.8165254332019825, 0.9605391077227289, 0.770014260445822]
[2022-10-27 01:16:27] - Epoch 1 - Save Best Score: 0.8120 Model
Epoch: [2][0/195] Elapsed 0m 1s (remain 3m 32s) Loss: 0.2357(0.2357) Grad: 378380.4375  LR: 0.00000099
Epoch: [2][100/195] Elapsed 2m 2s (remain 1m 54s) Loss: 0.2037(0.1970) Grad: 129833.9141  LR: 0.00000099
Epoch: [2][194/195] Elapsed 4m 3s (remain 0m 0s) Loss: 0.1129(0.1729) Grad: 170111.5469  LR: 0.00000093
EVAL: [0/25] Elapsed 0m 2s (remain 0m 52s) Loss: 0.1331(0.1331)
EVAL: [24/25] Elapsed 0m 38s (remain 0m 0s) Loss: 0.1230(0.1418)
[2022-10-27 01:21:11] - Epoch 2 - avg_train_loss: 0.1729  avg_val_loss: 0.1418  time: 282s
[2022-10-27 01:21:11] - Epoch 2 - Score: 0.5364  Scores: [0.5791961492010852, 0.5284528978934755, 0.5026812105315523, 0.5269004400715267, 0.5658778638616256, 0.5154684090932046]
[2022-10-27 01:21:11] - Epoch 2 - Save Best Score: 0.5364 Model
Epoch: [3][0/195] Elapsed 0m 1s (remain 5m 46s) Loss: 0.1283(0.1283) Grad: 169324.3594  LR: 0.00000095
Epoch: [3][100/195] Elapsed 2m 6s (remain 1m 57s) Loss: 0.2154(0.1327) Grad: 275530.5625  LR: 0.00000095
Epoch: [3][194/195] Elapsed 4m 3s (remain 0m 0s) Loss: 0.0938(0.1288) Grad: 143242.2656  LR: 0.00000084
EVAL: [0/25] Elapsed 0m 2s (remain 0m 51s) Loss: 0.1097(0.1097)
[2022-10-27 01:25:54] - Epoch 3 - avg_train_loss: 0.1288  avg_val_loss: 0.1228  time: 282s
[2022-10-27 01:25:54] - Epoch 3 - Score: 0.4976  Scores: [0.5284941823884668, 0.48967593666185416, 0.4627250479260892, 0.4966264441713296, 0.5362851607350365, 0.4720443888570379]
[2022-10-27 01:25:54] - Epoch 3 - Save Best Score: 0.4976 Model
EVAL: [24/25] Elapsed 0m 38s (remain 0m 0s) Loss: 0.1103(0.1228)
Epoch: [4][0/195] Elapsed 0m 2s (remain 6m 30s) Loss: 0.1253(0.1253) Grad: 229638.0938  LR: 0.00000086
Epoch: [4][100/195] Elapsed 2m 2s (remain 1m 54s) Loss: 0.1159(0.1192) Grad: 251631.7344  LR: 0.00000086
Epoch: [4][194/195] Elapsed 4m 4s (remain 0m 0s) Loss: 0.1218(0.1193) Grad: 119266.5859  LR: 0.00000072
EVAL: [0/25] Elapsed 0m 2s (remain 0m 51s) Loss: 0.1039(0.1039)
[2022-10-27 01:30:39] - Epoch 4 - avg_train_loss: 0.1193  avg_val_loss: 0.1159  time: 283s
[2022-10-27 01:30:39] - Epoch 4 - Score: 0.4829  Scores: [0.5130796246401849, 0.4713266375989454, 0.45386743349311354, 0.478896797272909, 0.5189951399842171, 0.4613472652015037]
EVAL: [24/25] Elapsed 0m 38s (remain 0m 0s) Loss: 0.1052(0.1159)
[2022-10-27 01:30:39] - Epoch 4 - Save Best Score: 0.4829 Model
Epoch: [5][0/195] Elapsed 0m 1s (remain 3m 31s) Loss: 0.0743(0.0743) Grad: 103546.7188  LR: 0.00000074
Epoch: [5][100/195] Elapsed 2m 3s (remain 1m 55s) Loss: 0.1527(0.1175) Grad: 363667.9688  LR: 0.00000074
Epoch: [5][194/195] Elapsed 3m 59s (remain 0m 0s) Loss: 0.1322(0.1153) Grad: 253294.1719  LR: 0.00000058
EVAL: [0/25] Elapsed 0m 2s (remain 0m 50s) Loss: 0.1037(0.1037)
EVAL: [24/25] Elapsed 0m 38s (remain 0m 0s) Loss: 0.1007(0.1143)
[2022-10-27 01:35:19] - Epoch 5 - avg_train_loss: 0.1153  avg_val_loss: 0.1143  time: 278s
[2022-10-27 01:35:19] - Epoch 5 - Score: 0.4794  Scores: [0.5091657792691819, 0.4677955395683259, 0.4510427625487437, 0.47645340948404763, 0.516995221789662, 0.45502741686766285]
[2022-10-27 01:35:19] - Epoch 5 - Save Best Score: 0.4794 Model
[2022-10-27 01:35:21] - ========== fold: 3 result ==========
[2022-10-27 01:35:21] - Score: 0.4794  Scores: [0.5091657792691819, 0.4677955395683259, 0.4510427625487437, 0.47645340948404763, 0.516995221789662, 0.45502741686766285]
[2022-10-27 01:35:21] - ========== fold: 4 training ==========
[2022-10-27 01:35:21] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Reinitializing Last 2 Layers ...
Done.!
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/195] Elapsed 0m 2s (remain 8m 18s) Loss: 2.6417(2.6417) Grad: inf  LR: 0.00000100
Epoch: [1][100/195] Elapsed 2m 6s (remain 1m 58s) Loss: 1.5592(2.1798) Grad: 367581.7188  LR: 0.00000100
Epoch: [1][194/195] Elapsed 4m 6s (remain 0m 0s) Loss: 0.4457(1.5650) Grad: 195229.8750  LR: 0.00000099
EVAL: [0/25] Elapsed 0m 2s (remain 1m 0s) Loss: 0.2459(0.2459)
EVAL: [24/25] Elapsed 0m 37s (remain 0m 0s) Loss: 0.3195(0.3366)
[2022-10-27 01:40:06] - Epoch 1 - avg_train_loss: 1.5650  avg_val_loss: 0.3366  time: 284s
[2022-10-27 01:40:06] - Epoch 1 - Score: 0.8652  Scores: [0.909099151137239, 0.7696881336814958, 0.7879468674121681, 0.8494664045717292, 0.9624938469791277, 0.9122405439156513]
[2022-10-27 01:40:06] - Epoch 1 - Save Best Score: 0.8652 Model
Epoch: [2][0/195] Elapsed 0m 1s (remain 3m 52s) Loss: 0.2030(0.2030) Grad: 219370.2344  LR: 0.00000099
Epoch: [2][100/195] Elapsed 2m 6s (remain 1m 57s) Loss: 0.1564(0.1865) Grad: 85457.4922  LR: 0.00000099
Epoch: [2][194/195] Elapsed 4m 2s (remain 0m 0s) Loss: 0.0812(0.1671) Grad: 32573.7383  LR: 0.00000093
EVAL: [0/25] Elapsed 0m 2s (remain 1m 0s) Loss: 0.1223(0.1223)
EVAL: [24/25] Elapsed 0m 37s (remain 0m 0s) Loss: 0.1061(0.1334)
[2022-10-27 01:44:48] - Epoch 2 - avg_train_loss: 0.1671  avg_val_loss: 0.1334  time: 281s
[2022-10-27 01:44:48] - Epoch 2 - Score: 0.5201  Scores: [0.549210253435992, 0.495304073785088, 0.4798714690118927, 0.5135677568444794, 0.5555663357136656, 0.5270965469754456]
[2022-10-27 01:44:48] - Epoch 2 - Save Best Score: 0.5201 Model
Epoch: [3][0/195] Elapsed 0m 1s (remain 4m 53s) Loss: 0.1290(0.1290) Grad: 194127.7031  LR: 0.00000095
Epoch: [3][100/195] Elapsed 1m 59s (remain 1m 50s) Loss: 0.1004(0.1309) Grad: 159639.7656  LR: 0.00000095
Epoch: [3][194/195] Elapsed 4m 2s (remain 0m 0s) Loss: 0.1010(0.1253) Grad: 106266.4766  LR: 0.00000084
EVAL: [0/25] Elapsed 0m 2s (remain 1m 2s) Loss: 0.1158(0.1158)
[2022-10-27 01:49:30] - Epoch 3 - avg_train_loss: 0.1253  avg_val_loss: 0.1219  time: 280s
[2022-10-27 01:49:30] - Epoch 3 - Score: 0.4960  Scores: [0.5206056735906702, 0.4755853690528257, 0.4604423411066417, 0.48962529083382833, 0.5312564933505293, 0.4986316251763916]
[2022-10-27 01:49:30] - Epoch 3 - Save Best Score: 0.4960 Model
EVAL: [24/25] Elapsed 0m 37s (remain 0m 0s) Loss: 0.0951(0.1219)
Epoch: [4][0/195] Elapsed 0m 1s (remain 4m 1s) Loss: 0.1133(0.1133) Grad: 156813.9375  LR: 0.00000086
Epoch: [4][100/195] Elapsed 2m 3s (remain 1m 54s) Loss: 0.1075(0.1169) Grad: 178246.2812  LR: 0.00000086
Epoch: [4][194/195] Elapsed 4m 2s (remain 0m 0s) Loss: 0.1451(0.1180) Grad: 189949.0000  LR: 0.00000072
EVAL: [0/25] Elapsed 0m 2s (remain 1m 0s) Loss: 0.1141(0.1141)
EVAL: [24/25] Elapsed 0m 37s (remain 0m 0s) Loss: 0.0936(0.1181)
[2022-10-27 01:54:12] - Epoch 4 - avg_train_loss: 0.1180  avg_val_loss: 0.1181  time: 280s
[2022-10-27 01:54:12] - Epoch 4 - Score: 0.4876  Scores: [0.5173438791989546, 0.4623406168306506, 0.4490005218704466, 0.48325985501744206, 0.526036910624551, 0.4875527214244666]
[2022-10-27 01:54:12] - Epoch 4 - Save Best Score: 0.4876 Model
Epoch: [5][0/195] Elapsed 0m 1s (remain 5m 0s) Loss: 0.1436(0.1436) Grad: 138624.0000  LR: 0.00000074
Epoch: [5][100/195] Elapsed 1m 59s (remain 1m 51s) Loss: 0.1070(0.1142) Grad: 142506.7656  LR: 0.00000074
Epoch: [5][194/195] Elapsed 4m 1s (remain 0m 0s) Loss: 0.0980(0.1138) Grad: 185498.5781  LR: 0.00000058
EVAL: [0/25] Elapsed 0m 2s (remain 0m 59s) Loss: 0.1126(0.1126)
[2022-10-27 01:58:53] - Epoch 5 - avg_train_loss: 0.1138  avg_val_loss: 0.1153  time: 279s
[2022-10-27 01:58:53] - Epoch 5 - Score: 0.4815  Scores: [0.5108920492278352, 0.4562889821372352, 0.44212285049392774, 0.4804770713514362, 0.5180516634489352, 0.4814496190877043]
[2022-10-27 01:58:53] - Epoch 5 - Save Best Score: 0.4815 Model
EVAL: [24/25] Elapsed 0m 37s (remain 0m 0s) Loss: 0.0915(0.1153)
[2022-10-27 01:58:55] - ========== fold: 4 result ==========
[2022-10-27 01:58:55] - Score: 0.4815  Scores: [0.5108920492278352, 0.4562889821372352, 0.44212285049392774, 0.4804770713514362, 0.5180516634489352, 0.4814496190877043]
[2022-10-27 01:58:55] - ========== CV ==========
[2022-10-27 01:58:55] - Score: 0.4824  Scores: [0.5160090579796415, 0.46962286879781845, 0.44161686106232023, 0.4768499653033425, 0.5086831359613998, 0.48150838565639986]