Downloading tokenizer_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 52.0/52.0 [00:00<00:00, 68.1kB/s]
Downloading config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 579/579 [00:00<00:00, 730kB/s]
Downloading spm.model: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.35M/2.35M [00:00<00:00, 8.32MB/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3911/3911 [00:03<00:00, 988.47it/s]
[2022-11-17 00:31:26] - comment: abhishek/deberta-v3-base-autotrain, LLRD 0.7, reinit=1 Attention Pooling, exp050=LLRD=0.8
[2022-11-17 00:31:26] - max_len: 2048
[2022-11-17 00:31:26] - ========== fold: 0 training ==========
[2022-11-17 00:31:26] - DebertaV2Config {
  "_name_or_path": "abhishek/deberta-v3-base-autotrain",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}



Downloading pytorch_model.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 354M/354M [00:06<00:00, 56.4MB/s]
Some weights of the model checkpoint at abhishek/deberta-v3-base-autotrain were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Reinitializing Last 1 Layers ...
Done.!
Epoch: [1][0/391] Elapsed 0m 1s (remain 10m 0s) Loss: 2.6500(2.6500) Grad: inf  LR: 0.00002994
Epoch: [1][100/391] Elapsed 0m 53s (remain 2m 32s) Loss: 0.1672(0.3829) Grad: 439828.2812  LR: 0.00002994
Epoch: [1][200/391] Elapsed 1m 45s (remain 1m 39s) Loss: 0.0958(0.2599) Grad: 81355.4375  LR: 0.00002994
Epoch: [1][300/391] Elapsed 2m 38s (remain 0m 47s) Loss: 0.1948(0.2115) Grad: 91174.4453  LR: 0.00002994
Epoch: [1][390/391] Elapsed 3m 29s (remain 0m 0s) Loss: 0.0787(0.1890) Grad: 65736.2734  LR: 0.00002821
EVAL: [0/49] Elapsed 0m 0s (remain 0m 36s) Loss: 0.1394(0.1394)
[2022-11-17 00:35:37] - Epoch 1 - avg_train_loss: 0.1890  avg_val_loss: 0.1285  time: 241s
[2022-11-17 00:35:37] - Epoch 1 - Score: 0.5089  Scores: [0.518032695598368, 0.4633829811399425, 0.5169872091991015, 0.5202497821731696, 0.5195638295513135, 0.5151900847859228]
[2022-11-17 00:35:37] - Epoch 1 - Save Best Score: 0.5089 Model
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.1141(0.1285)
Epoch: [2][0/391] Elapsed 0m 0s (remain 4m 36s) Loss: 0.1389(0.1389) Grad: 198897.6094  LR: 0.00002864
Epoch: [2][100/391] Elapsed 0m 53s (remain 2m 33s) Loss: 0.1537(0.1123) Grad: 309090.2812  LR: 0.00002864
Epoch: [2][200/391] Elapsed 1m 47s (remain 1m 41s) Loss: 0.1090(0.1049) Grad: 171632.4219  LR: 0.00002864
Epoch: [2][300/391] Elapsed 2m 42s (remain 0m 48s) Loss: 0.0930(0.1069) Grad: 132794.8438  LR: 0.00002864
Epoch: [2][390/391] Elapsed 3m 28s (remain 0m 0s) Loss: 0.0777(0.1080) Grad: 121033.3906  LR: 0.00002231
EVAL: [0/49] Elapsed 0m 0s (remain 0m 36s) Loss: 0.1452(0.1452)
[2022-11-17 00:39:38] - Epoch 2 - avg_train_loss: 0.1080  avg_val_loss: 0.1182  time: 239s
[2022-11-17 00:39:38] - Epoch 2 - Score: 0.4873  Scores: [0.5374017363283392, 0.486007499690341, 0.4695411825428261, 0.5204754103561366, 0.46361464606280217, 0.4468090580630862]
[2022-11-17 00:39:38] - Epoch 2 - Save Best Score: 0.4873 Model
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.1222(0.1182)
Epoch: [3][0/391] Elapsed 0m 0s (remain 4m 12s) Loss: 0.1032(0.1032) Grad: 233100.3438  LR: 0.00002312
Epoch: [3][100/391] Elapsed 0m 54s (remain 2m 36s) Loss: 0.1161(0.1054) Grad: 136119.6875  LR: 0.00002312
Epoch: [3][200/391] Elapsed 1m 48s (remain 1m 42s) Loss: 0.1488(0.1046) Grad: 163721.9375  LR: 0.00002312
Epoch: [3][300/391] Elapsed 2m 37s (remain 0m 46s) Loss: 0.1434(0.1043) Grad: 315216.9062  LR: 0.00002312
Epoch: [3][390/391] Elapsed 3m 25s (remain 0m 0s) Loss: 0.0660(0.1046) Grad: 98107.9141  LR: 0.00001417
EVAL: [0/49] Elapsed 0m 0s (remain 0m 36s) Loss: 0.1173(0.1173)
[2022-11-17 00:43:35] - Epoch 3 - avg_train_loss: 0.1046  avg_val_loss: 0.1022  time: 236s
[2022-11-17 00:43:35] - Epoch 3 - Score: 0.4525  Scores: [0.48917708633410073, 0.4717496324334053, 0.40555792392657153, 0.451422918040014, 0.45312670390498916, 0.44398722425296133]
[2022-11-17 00:43:35] - Epoch 3 - Save Best Score: 0.4525 Model
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.0998(0.1022)
Epoch: [4][0/391] Elapsed 0m 0s (remain 3m 30s) Loss: 0.1082(0.1082) Grad: 117891.7422  LR: 0.00001511
Epoch: [4][100/391] Elapsed 0m 54s (remain 2m 37s) Loss: 0.0804(0.1027) Grad: 94832.6250  LR: 0.00001511
Epoch: [4][200/391] Elapsed 1m 46s (remain 1m 41s) Loss: 0.1133(0.1008) Grad: 136756.5312  LR: 0.00001511
Epoch: [4][300/391] Elapsed 2m 38s (remain 0m 47s) Loss: 0.1044(0.1011) Grad: 212752.2188  LR: 0.00001511
Epoch: [4][390/391] Elapsed 3m 29s (remain 0m 0s) Loss: 0.1438(0.1020) Grad: 270060.9688  LR: 0.00000633
EVAL: [0/49] Elapsed 0m 0s (remain 0m 37s) Loss: 0.1187(0.1187)
[2022-11-17 00:47:37] - Epoch 4 - avg_train_loss: 0.1020  avg_val_loss: 0.1003  time: 241s
[2022-11-17 00:47:37] - Epoch 4 - Score: 0.4482  Scores: [0.4846323567034738, 0.4601304150063441, 0.4031567386976082, 0.4477793914011456, 0.4516881705755762, 0.44184504928264784]
[2022-11-17 00:47:37] - Epoch 4 - Save Best Score: 0.4482 Model
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.0988(0.1003)
Epoch: [5][0/391] Elapsed 0m 1s (remain 6m 55s) Loss: 0.1807(0.1807) Grad: 721008.4375  LR: 0.00000711
Epoch: [5][100/391] Elapsed 0m 55s (remain 2m 40s) Loss: 0.0998(0.1010) Grad: 265518.0312  LR: 0.00000711
Epoch: [5][200/391] Elapsed 1m 47s (remain 1m 41s) Loss: 0.0837(0.1031) Grad: 128186.7656  LR: 0.00000711
Epoch: [5][300/391] Elapsed 2m 39s (remain 0m 47s) Loss: 0.0797(0.1014) Grad: 172674.3594  LR: 0.00000711
Epoch: [5][390/391] Elapsed 3m 29s (remain 0m 0s) Loss: 0.1014(0.1012) Grad: 142979.4219  LR: 0.00000124
EVAL: [0/49] Elapsed 0m 0s (remain 0m 36s) Loss: 0.1159(0.1159)
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.1019(0.1014)
[2022-11-17 00:51:39] - Epoch 5 - avg_train_loss: 0.1012  avg_val_loss: 0.1014  time: 241s
[2022-11-17 00:51:39] - Epoch 5 - Score: 0.4509  Scores: [0.4816751559591468, 0.46235279046063105, 0.40999352119725563, 0.4509149701831391, 0.45479113896662293, 0.4455371799308784]
[2022-11-17 00:51:40] - ========== fold: 0 result ==========
[2022-11-17 00:51:40] - Score: 0.4482  Scores: [0.4846323567034738, 0.4601304150063441, 0.4031567386976082, 0.4477793914011456, 0.4516881705755762, 0.44184504928264784]
[2022-11-17 00:51:40] - ========== fold: 1 training ==========
[2022-11-17 00:51:40] - DebertaV2Config {
  "_name_or_path": "abhishek/deberta-v3-base-autotrain",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Reinitializing Last 1 Layers ...
Done.!
Some weights of the model checkpoint at abhishek/deberta-v3-base-autotrain were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/391] Elapsed 0m 0s (remain 4m 28s) Loss: 2.2790(2.2790) Grad: inf  LR: 0.00002994
Epoch: [1][100/391] Elapsed 0m 56s (remain 2m 43s) Loss: 0.1061(0.3696) Grad: 170312.1719  LR: 0.00002994
Epoch: [1][200/391] Elapsed 1m 51s (remain 1m 45s) Loss: 0.1405(0.2486) Grad: 91199.0391  LR: 0.00002994
Epoch: [1][300/391] Elapsed 2m 44s (remain 0m 49s) Loss: 0.1328(0.2040) Grad: 136763.5312  LR: 0.00002994
Epoch: [1][390/391] Elapsed 3m 30s (remain 0m 0s) Loss: 0.0783(0.1825) Grad: 58724.8711  LR: 0.00002821
EVAL: [0/49] Elapsed 0m 1s (remain 0m 59s) Loss: 0.1026(0.1026)
EVAL: [48/49] Elapsed 0m 31s (remain 0m 0s) Loss: 0.1219(0.1141)
[2022-11-17 00:55:43] - Epoch 1 - avg_train_loss: 0.1825  avg_val_loss: 0.1141  time: 242s
[2022-11-17 00:55:43] - Epoch 1 - Score: 0.4788  Scores: [0.5105733324087554, 0.470955540447272, 0.43127059111657995, 0.511194660069215, 0.4758097444084535, 0.4732227985216338]
[2022-11-17 00:55:43] - Epoch 1 - Save Best Score: 0.4788 Model
Epoch: [2][0/391] Elapsed 0m 0s (remain 6m 27s) Loss: 0.0885(0.0885) Grad: 104295.0859  LR: 0.00002864
Epoch: [2][100/391] Elapsed 0m 55s (remain 2m 38s) Loss: 0.1081(0.1048) Grad: 125597.3594  LR: 0.00002864
Epoch: [2][200/391] Elapsed 1m 49s (remain 1m 43s) Loss: 0.0969(0.1049) Grad: 275643.2500  LR: 0.00002864
Epoch: [2][300/391] Elapsed 2m 42s (remain 0m 48s) Loss: 0.0783(0.1063) Grad: 124798.1094  LR: 0.00002864
Epoch: [2][390/391] Elapsed 3m 29s (remain 0m 0s) Loss: 0.0910(0.1066) Grad: 124045.0469  LR: 0.00002231
EVAL: [0/49] Elapsed 0m 1s (remain 1m 0s) Loss: 0.1093(0.1093)
[2022-11-17 00:59:46] - Epoch 2 - avg_train_loss: 0.1066  avg_val_loss: 0.1178  time: 241s
[2022-11-17 00:59:46] - Epoch 2 - Score: 0.4873  Scores: [0.5068240057093083, 0.4975045873494994, 0.47532196026461837, 0.49752480370665186, 0.48439482107615384, 0.46233102986251]
EVAL: [48/49] Elapsed 0m 31s (remain 0m 0s) Loss: 0.1262(0.1178)
Epoch: [3][0/391] Elapsed 0m 0s (remain 3m 18s) Loss: 0.1169(0.1169) Grad: 192216.5156  LR: 0.00002312
Epoch: [3][100/391] Elapsed 0m 54s (remain 2m 35s) Loss: 0.1258(0.1075) Grad: 137068.8594  LR: 0.00002312
Epoch: [3][200/391] Elapsed 1m 48s (remain 1m 42s) Loss: 0.1022(0.1043) Grad: 312409.2500  LR: 0.00002312
Epoch: [3][300/391] Elapsed 2m 39s (remain 0m 47s) Loss: 0.1418(0.1043) Grad: 225131.1094  LR: 0.00002312
Epoch: [3][390/391] Elapsed 3m 28s (remain 0m 0s) Loss: 0.1038(0.1034) Grad: 251619.2656  LR: 0.00001417
EVAL: [0/49] Elapsed 0m 1s (remain 0m 58s) Loss: 0.0934(0.0934)
EVAL: [48/49] Elapsed 0m 31s (remain 0m 0s) Loss: 0.1124(0.1053)
[2022-11-17 01:03:46] - Epoch 3 - avg_train_loss: 0.1034  avg_val_loss: 0.1053  time: 240s
[2022-11-17 01:03:46] - Epoch 3 - Score: 0.4597  Scores: [0.493689678163917, 0.44658850935921146, 0.42018605431534894, 0.48475654789500633, 0.4701048021394879, 0.44281117083963367]
[2022-11-17 01:03:46] - Epoch 3 - Save Best Score: 0.4597 Model
Epoch: [4][0/391] Elapsed 0m 0s (remain 5m 56s) Loss: 0.1316(0.1316) Grad: 356488.0312  LR: 0.00001511
Epoch: [4][100/391] Elapsed 0m 55s (remain 2m 40s) Loss: 0.0690(0.0940) Grad: 208651.3906  LR: 0.00001511
Epoch: [4][200/391] Elapsed 1m 48s (remain 1m 42s) Loss: 0.0758(0.0967) Grad: 104060.2734  LR: 0.00001511
Epoch: [4][300/391] Elapsed 2m 41s (remain 0m 48s) Loss: 0.0910(0.0986) Grad: 156125.2031  LR: 0.00001511
Epoch: [4][390/391] Elapsed 3m 29s (remain 0m 0s) Loss: 0.0839(0.0997) Grad: 165502.4219  LR: 0.00000633
EVAL: [0/49] Elapsed 0m 1s (remain 1m 1s) Loss: 0.0866(0.0866)
EVAL: [48/49] Elapsed 0m 31s (remain 0m 0s) Loss: 0.1106(0.1050)
[2022-11-17 01:07:48] - Epoch 4 - avg_train_loss: 0.0997  avg_val_loss: 0.1050  time: 241s
[2022-11-17 01:07:48] - Epoch 4 - Score: 0.4591  Scores: [0.49493394642586624, 0.4462721587511295, 0.4249125279893658, 0.47820309004183864, 0.4676473200313675, 0.44256472154705195]
[2022-11-17 01:07:48] - Epoch 4 - Save Best Score: 0.4591 Model
Epoch: [5][0/391] Elapsed 0m 0s (remain 5m 21s) Loss: 0.1636(0.1636) Grad: 476763.1250  LR: 0.00000711
Epoch: [5][100/391] Elapsed 0m 57s (remain 2m 45s) Loss: 0.0936(0.0927) Grad: 195823.3750  LR: 0.00000711
Epoch: [5][200/391] Elapsed 1m 48s (remain 1m 42s) Loss: 0.0985(0.0945) Grad: 147260.9062  LR: 0.00000711
Epoch: [5][300/391] Elapsed 2m 44s (remain 0m 49s) Loss: 0.0834(0.0961) Grad: 137072.4844  LR: 0.00000711
Epoch: [5][390/391] Elapsed 3m 31s (remain 0m 0s) Loss: 0.0735(0.0953) Grad: 72240.4609  LR: 0.00000124
EVAL: [0/49] Elapsed 0m 1s (remain 0m 59s) Loss: 0.0917(0.0917)
EVAL: [48/49] Elapsed 0m 31s (remain 0m 0s) Loss: 0.1134(0.1062)
[2022-11-17 01:11:52] - Epoch 5 - avg_train_loss: 0.0953  avg_val_loss: 0.1062  time: 243s
[2022-11-17 01:11:52] - Epoch 5 - Score: 0.4618  Scores: [0.4916265208508353, 0.452301585335751, 0.4268372428245648, 0.4798562236124235, 0.4715785643515342, 0.4487607243157081]
[2022-11-17 01:11:53] - ========== fold: 1 result ==========
[2022-11-17 01:11:53] - Score: 0.4591  Scores: [0.49493394642586624, 0.4462721587511295, 0.4249125279893658, 0.47820309004183864, 0.4676473200313675, 0.44256472154705195]
[2022-11-17 01:11:53] - ========== fold: 2 training ==========
[2022-11-17 01:11:53] - DebertaV2Config {
  "_name_or_path": "abhishek/deberta-v3-base-autotrain",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Reinitializing Last 1 Layers ...
Done.!
Some weights of the model checkpoint at abhishek/deberta-v3-base-autotrain were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/391] Elapsed 0m 0s (remain 3m 43s) Loss: 2.1833(2.1833) Grad: inf  LR: 0.00002994
Epoch: [1][100/391] Elapsed 0m 51s (remain 2m 26s) Loss: 0.1039(0.3306) Grad: 149402.1875  LR: 0.00002994
Epoch: [1][200/391] Elapsed 1m 43s (remain 1m 37s) Loss: 0.0772(0.2261) Grad: 70036.4375  LR: 0.00002994
Epoch: [1][300/391] Elapsed 2m 40s (remain 0m 47s) Loss: 0.0865(0.1908) Grad: 125459.8828  LR: 0.00002994
Epoch: [1][390/391] Elapsed 3m 25s (remain 0m 0s) Loss: 0.1517(0.1747) Grad: 277236.2500  LR: 0.00002821
EVAL: [0/49] Elapsed 0m 0s (remain 0m 34s) Loss: 0.0820(0.0820)
[2022-11-17 01:15:54] - Epoch 1 - avg_train_loss: 0.1747  avg_val_loss: 0.1106  time: 239s
[2022-11-17 01:15:54] - Epoch 1 - Score: 0.4716  Scores: [0.5197835365507473, 0.47118823249325376, 0.42958224998579214, 0.4529298744801282, 0.4849315602496602, 0.47137898711164583]
[2022-11-17 01:15:54] - Epoch 1 - Save Best Score: 0.4716 Model
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0722(0.1106)
Epoch: [2][0/391] Elapsed 0m 0s (remain 5m 19s) Loss: 0.1223(0.1223) Grad: 199394.9375  LR: 0.00002864
Epoch: [2][100/391] Elapsed 0m 55s (remain 2m 38s) Loss: 0.1254(0.1103) Grad: 160889.8438  LR: 0.00002864
Epoch: [2][200/391] Elapsed 1m 47s (remain 1m 41s) Loss: 0.0918(0.1098) Grad: 269115.3438  LR: 0.00002864
Epoch: [2][300/391] Elapsed 2m 39s (remain 0m 47s) Loss: 0.1216(0.1082) Grad: 214829.6250  LR: 0.00002864
Epoch: [2][390/391] Elapsed 3m 28s (remain 0m 0s) Loss: 0.1265(0.1074) Grad: 150286.0156  LR: 0.00002231
EVAL: [0/49] Elapsed 0m 0s (remain 0m 34s) Loss: 0.1089(0.1089)
[2022-11-17 01:19:57] - Epoch 2 - avg_train_loss: 0.1074  avg_val_loss: 0.1254  time: 241s
[2022-11-17 01:19:57] - Epoch 2 - Score: 0.5031  Scores: [0.5794466021843739, 0.4612398211333381, 0.5128504411766417, 0.48625560467171586, 0.48745533673907326, 0.4913502005680191]
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0938(0.1254)
Epoch: [3][0/391] Elapsed 0m 0s (remain 3m 46s) Loss: 0.0999(0.0999) Grad: 205305.7188  LR: 0.00002312
Epoch: [3][100/391] Elapsed 0m 53s (remain 2m 34s) Loss: 0.1049(0.1082) Grad: 269796.2500  LR: 0.00002312
Epoch: [3][200/391] Elapsed 1m 46s (remain 1m 40s) Loss: 0.0779(0.1053) Grad: 110326.9219  LR: 0.00002312
Epoch: [3][300/391] Elapsed 2m 42s (remain 0m 48s) Loss: 0.1269(0.1057) Grad: 217168.2344  LR: 0.00002312
Epoch: [3][390/391] Elapsed 3m 29s (remain 0m 0s) Loss: 0.1470(0.1053) Grad: 252179.5469  LR: 0.00001417
EVAL: [0/49] Elapsed 0m 0s (remain 0m 34s) Loss: 0.0759(0.0759)
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0733(0.1076)
[2022-11-17 01:23:59] - Epoch 3 - avg_train_loss: 0.1053  avg_val_loss: 0.1076  time: 243s
[2022-11-17 01:23:59] - Epoch 3 - Score: 0.4652  Scores: [0.500988813884869, 0.45931479557042115, 0.42123502699753607, 0.4563578546882221, 0.4839085963465934, 0.4693857853921696]
[2022-11-17 01:23:59] - Epoch 3 - Save Best Score: 0.4652 Model
Epoch: [4][0/391] Elapsed 0m 0s (remain 5m 42s) Loss: 0.1076(0.1076) Grad: 166138.8906  LR: 0.00001511
Epoch: [4][100/391] Elapsed 0m 55s (remain 2m 39s) Loss: 0.0987(0.0997) Grad: 188875.3281  LR: 0.00001511
Epoch: [4][200/391] Elapsed 1m 45s (remain 1m 40s) Loss: 0.1196(0.1014) Grad: 220508.3906  LR: 0.00001511
Epoch: [4][300/391] Elapsed 2m 37s (remain 0m 47s) Loss: 0.1087(0.1006) Grad: 155065.4375  LR: 0.00001511
Epoch: [4][390/391] Elapsed 3m 26s (remain 0m 0s) Loss: 0.1122(0.1001) Grad: 284520.8438  LR: 0.00000633
EVAL: [0/49] Elapsed 0m 0s (remain 0m 33s) Loss: 0.0756(0.0756)
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0680(0.1044)
[2022-11-17 01:28:00] - Epoch 4 - avg_train_loss: 0.1001  avg_val_loss: 0.1044  time: 239s
[2022-11-17 01:28:00] - Epoch 4 - Score: 0.4582  Scores: [0.4926598085062719, 0.4600365103500327, 0.4141694681270038, 0.443227798002754, 0.48197677621168894, 0.4572106438897099]
[2022-11-17 01:28:00] - Epoch 4 - Save Best Score: 0.4582 Model
Epoch: [5][0/391] Elapsed 0m 0s (remain 3m 7s) Loss: 0.0681(0.0681) Grad: 102961.7500  LR: 0.00000711
Epoch: [5][100/391] Elapsed 0m 53s (remain 2m 32s) Loss: 0.0938(0.0975) Grad: 128466.7344  LR: 0.00000711
Epoch: [5][200/391] Elapsed 1m 48s (remain 1m 42s) Loss: 0.0946(0.0986) Grad: 172877.7969  LR: 0.00000711
Epoch: [5][300/391] Elapsed 2m 42s (remain 0m 48s) Loss: 0.1643(0.0981) Grad: 157384.4062  LR: 0.00000711
Epoch: [5][390/391] Elapsed 3m 29s (remain 0m 0s) Loss: 0.0742(0.0982) Grad: 127992.6484  LR: 0.00000124
EVAL: [0/49] Elapsed 0m 0s (remain 0m 34s) Loss: 0.0738(0.0738)
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0682(0.1080)
[2022-11-17 01:32:04] - Epoch 5 - avg_train_loss: 0.0982  avg_val_loss: 0.1080  time: 243s
[2022-11-17 01:32:04] - Epoch 5 - Score: 0.4658  Scores: [0.49387342882206986, 0.494028730251333, 0.4184328510984112, 0.4483369119158362, 0.48113481221369747, 0.45897415516997253]
[2022-11-17 01:32:04] - ========== fold: 2 result ==========
[2022-11-17 01:32:04] - Score: 0.4582  Scores: [0.4926598085062719, 0.4600365103500327, 0.4141694681270038, 0.443227798002754, 0.48197677621168894, 0.4572106438897099]
[2022-11-17 01:32:04] - ========== fold: 3 training ==========
[2022-11-17 01:32:04] - DebertaV2Config {
  "_name_or_path": "abhishek/deberta-v3-base-autotrain",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Reinitializing Last 1 Layers ...
Done.!
Some weights of the model checkpoint at abhishek/deberta-v3-base-autotrain were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/391] Elapsed 0m 0s (remain 3m 54s) Loss: 2.6954(2.6954) Grad: inf  LR: 0.00002994
Epoch: [1][100/391] Elapsed 0m 54s (remain 2m 37s) Loss: 0.0914(0.3599) Grad: 72146.6250  LR: 0.00002994
Epoch: [1][200/391] Elapsed 1m 48s (remain 1m 42s) Loss: 0.1056(0.2452) Grad: 290370.0625  LR: 0.00002994
Epoch: [1][300/391] Elapsed 2m 42s (remain 0m 48s) Loss: 0.1807(0.2021) Grad: 191384.0469  LR: 0.00002994
Epoch: [1][390/391] Elapsed 3m 27s (remain 0m 0s) Loss: 0.0994(0.1847) Grad: 78815.1172  LR: 0.00002821
EVAL: [0/49] Elapsed 0m 1s (remain 0m 56s) Loss: 0.1326(0.1326)
EVAL: [48/49] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0819(0.1141)
[2022-11-17 01:36:08] - Epoch 1 - avg_train_loss: 0.1847  avg_val_loss: 0.1141  time: 241s
[2022-11-17 01:36:08] - Epoch 1 - Score: 0.4781  Scores: [0.5240555346107262, 0.45412306389359797, 0.4337007236411828, 0.4830554252517227, 0.5253333740584131, 0.4483519927246218]
[2022-11-17 01:36:08] - Epoch 1 - Save Best Score: 0.4781 Model
Epoch: [2][0/391] Elapsed 0m 0s (remain 4m 26s) Loss: 0.0843(0.0843) Grad: 162576.4219  LR: 0.00002864
Epoch: [2][100/391] Elapsed 0m 50s (remain 2m 26s) Loss: 0.2180(0.1079) Grad: 350278.4375  LR: 0.00002864
Epoch: [2][200/391] Elapsed 1m 40s (remain 1m 35s) Loss: 0.1891(0.1067) Grad: 481474.2188  LR: 0.00002864
Epoch: [2][300/391] Elapsed 2m 36s (remain 0m 46s) Loss: 0.1041(0.1046) Grad: 244712.4531  LR: 0.00002864
Epoch: [2][390/391] Elapsed 3m 25s (remain 0m 0s) Loss: 0.0851(0.1067) Grad: 274837.5312  LR: 0.00002231
EVAL: [0/49] Elapsed 0m 1s (remain 0m 56s) Loss: 0.1227(0.1227)
[2022-11-17 01:40:08] - Epoch 2 - avg_train_loss: 0.1067  avg_val_loss: 0.1091  time: 239s
[2022-11-17 01:40:08] - Epoch 2 - Score: 0.4679  Scores: [0.4821759844082187, 0.4541735865968806, 0.4352916828903092, 0.49913103590560814, 0.48907776263769875, 0.44760418188632806]
[2022-11-17 01:40:08] - Epoch 2 - Save Best Score: 0.4679 Model
EVAL: [48/49] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0754(0.1091)
Epoch: [3][0/391] Elapsed 0m 1s (remain 8m 12s) Loss: 0.0844(0.0844) Grad: 148915.5781  LR: 0.00002312
Epoch: [3][100/391] Elapsed 0m 53s (remain 2m 34s) Loss: 0.1709(0.1031) Grad: 126588.3438  LR: 0.00002312
Epoch: [3][200/391] Elapsed 1m 48s (remain 1m 42s) Loss: 0.1387(0.1023) Grad: 138397.1719  LR: 0.00002312
Epoch: [3][300/391] Elapsed 2m 39s (remain 0m 47s) Loss: 0.0767(0.1012) Grad: 214020.9688  LR: 0.00002312
Epoch: [3][390/391] Elapsed 3m 27s (remain 0m 0s) Loss: 0.0899(0.1018) Grad: 142160.4844  LR: 0.00001417
EVAL: [0/49] Elapsed 0m 1s (remain 0m 57s) Loss: 0.1281(0.1281)
[2022-11-17 01:44:11] - Epoch 3 - avg_train_loss: 0.1018  avg_val_loss: 0.1051  time: 241s
[2022-11-17 01:44:11] - Epoch 3 - Score: 0.4591  Scores: [0.47641468523535485, 0.43905243497950536, 0.43398723508615217, 0.46665767341720754, 0.48981868274755846, 0.44889749586854655]
[2022-11-17 01:44:11] - Epoch 3 - Save Best Score: 0.4591 Model
EVAL: [48/49] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0687(0.1051)
Epoch: [4][0/391] Elapsed 0m 0s (remain 4m 51s) Loss: 0.1507(0.1507) Grad: 303290.5312  LR: 0.00001511
Epoch: [4][100/391] Elapsed 0m 51s (remain 2m 27s) Loss: 0.1545(0.1004) Grad: 284738.6250  LR: 0.00001511
Epoch: [4][200/391] Elapsed 1m 41s (remain 1m 35s) Loss: 0.0677(0.1000) Grad: 136772.5156  LR: 0.00001511
Epoch: [4][300/391] Elapsed 2m 37s (remain 0m 46s) Loss: 0.0602(0.0987) Grad: 109689.7422  LR: 0.00001511
Epoch: [4][390/391] Elapsed 3m 29s (remain 0m 0s) Loss: 0.0832(0.0993) Grad: 160225.7812  LR: 0.00000633
EVAL: [0/49] Elapsed 0m 1s (remain 0m 58s) Loss: 0.1235(0.1235)
[2022-11-17 01:48:15] - Epoch 4 - avg_train_loss: 0.0993  avg_val_loss: 0.1036  time: 243s
[2022-11-17 01:48:15] - Epoch 4 - Score: 0.4557  Scores: [0.4785928521104384, 0.4410676808684158, 0.425189132227814, 0.45917025239686926, 0.48871848507844456, 0.4413461751784799]
[2022-11-17 01:48:15] - Epoch 4 - Save Best Score: 0.4557 Model
EVAL: [48/49] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0703(0.1036)
Epoch: [5][0/391] Elapsed 0m 0s (remain 6m 2s) Loss: 0.1361(0.1361) Grad: 257050.9375  LR: 0.00000711
Epoch: [5][100/391] Elapsed 0m 55s (remain 2m 38s) Loss: 0.1211(0.0906) Grad: 245601.2500  LR: 0.00000711
Epoch: [5][200/391] Elapsed 1m 47s (remain 1m 41s) Loss: 0.0792(0.0940) Grad: 141266.1875  LR: 0.00000711
Epoch: [5][300/391] Elapsed 2m 40s (remain 0m 48s) Loss: 0.1102(0.0943) Grad: 122038.1172  LR: 0.00000711
Epoch: [5][390/391] Elapsed 3m 26s (remain 0m 0s) Loss: 0.0696(0.0961) Grad: 190720.7656  LR: 0.00000124
EVAL: [0/49] Elapsed 0m 1s (remain 0m 57s) Loss: 0.1253(0.1253)
EVAL: [48/49] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0748(0.1085)
[2022-11-17 01:52:17] - Epoch 5 - avg_train_loss: 0.0961  avg_val_loss: 0.1085  time: 240s
[2022-11-17 01:52:17] - Epoch 5 - Score: 0.4670  Scores: [0.48062548898456714, 0.45526280942210345, 0.4434677369553786, 0.4798222086307613, 0.4964302341534724, 0.4461332577977878]
[2022-11-17 01:52:18] - ========== fold: 3 result ==========
[2022-11-17 01:52:18] - Score: 0.4557  Scores: [0.4785928521104384, 0.4410676808684158, 0.425189132227814, 0.45917025239686926, 0.48871848507844456, 0.4413461751784799]
[2022-11-17 01:52:18] - ========== fold: 4 training ==========
[2022-11-17 01:52:18] - DebertaV2Config {
  "_name_or_path": "abhishek/deberta-v3-base-autotrain",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Reinitializing Last 1 Layers ...
Some weights of the model checkpoint at abhishek/deberta-v3-base-autotrain were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Done.!
Epoch: [1][0/391] Elapsed 0m 1s (remain 6m 43s) Loss: 2.7355(2.7355) Grad: inf  LR: 0.00002994
Epoch: [1][100/391] Elapsed 0m 54s (remain 2m 36s) Loss: 0.1681(0.3587) Grad: 105885.4609  LR: 0.00002994
Epoch: [1][200/391] Elapsed 1m 48s (remain 1m 42s) Loss: 0.1519(0.2416) Grad: 98765.6250  LR: 0.00002994
Epoch: [1][300/391] Elapsed 2m 40s (remain 0m 47s) Loss: 0.1869(0.2003) Grad: 216685.0469  LR: 0.00002994
Epoch: [1][390/391] Elapsed 3m 30s (remain 0m 0s) Loss: 0.1514(0.1814) Grad: 125627.5469  LR: 0.00002821
EVAL: [0/49] Elapsed 0m 1s (remain 1m 6s) Loss: 0.1204(0.1204)
[2022-11-17 01:56:22] - Epoch 1 - avg_train_loss: 0.1814  avg_val_loss: 0.1389  time: 243s
[2022-11-17 01:56:22] - Epoch 1 - Score: 0.5280  Scores: [0.6067282757277572, 0.44517186758039673, 0.5029458385559424, 0.5708334241609788, 0.5764420689346182, 0.4660061206916894]
[2022-11-17 01:56:22] - Epoch 1 - Save Best Score: 0.5280 Model
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.1004(0.1389)
Epoch: [2][0/391] Elapsed 0m 0s (remain 4m 5s) Loss: 0.1127(0.1127) Grad: 176179.0625  LR: 0.00002864
Epoch: [2][100/391] Elapsed 0m 52s (remain 2m 30s) Loss: 0.0825(0.1120) Grad: 293263.6562  LR: 0.00002864
Epoch: [2][200/391] Elapsed 1m 43s (remain 1m 38s) Loss: 0.0857(0.1061) Grad: 182175.4688  LR: 0.00002864
Epoch: [2][300/391] Elapsed 2m 34s (remain 0m 46s) Loss: 0.1088(0.1062) Grad: 231331.0938  LR: 0.00002864
Epoch: [2][390/391] Elapsed 3m 26s (remain 0m 0s) Loss: 0.1131(0.1063) Grad: 166001.9219  LR: 0.00002231
EVAL: [0/49] Elapsed 0m 1s (remain 1m 5s) Loss: 0.0953(0.0953)
[2022-11-17 02:00:23] - Epoch 2 - avg_train_loss: 0.1063  avg_val_loss: 0.1044  time: 239s
[2022-11-17 02:00:23] - Epoch 2 - Score: 0.4582  Scores: [0.48180298139841804, 0.4342553600297065, 0.43636603203815094, 0.46182063539740753, 0.47931230276885334, 0.45586385951426167]
[2022-11-17 02:00:23] - Epoch 2 - Save Best Score: 0.4582 Model
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0779(0.1044)
Epoch: [3][0/391] Elapsed 0m 0s (remain 6m 4s) Loss: 0.1347(0.1347) Grad: 148631.7031  LR: 0.00002312
Epoch: [3][100/391] Elapsed 0m 56s (remain 2m 40s) Loss: 0.0946(0.1053) Grad: 200528.1406  LR: 0.00002312
Epoch: [3][200/391] Elapsed 1m 49s (remain 1m 43s) Loss: 0.0726(0.1051) Grad: 119736.0312  LR: 0.00002312
Epoch: [3][300/391] Elapsed 2m 46s (remain 0m 49s) Loss: 0.1064(0.1052) Grad: 203754.1562  LR: 0.00002312
Epoch: [3][390/391] Elapsed 3m 30s (remain 0m 0s) Loss: 0.1446(0.1048) Grad: 166925.2969  LR: 0.00001417
EVAL: [0/49] Elapsed 0m 1s (remain 1m 6s) Loss: 0.1120(0.1120)
[2022-11-17 02:04:27] - Epoch 3 - avg_train_loss: 0.1048  avg_val_loss: 0.1141  time: 243s
[2022-11-17 02:04:27] - Epoch 3 - Score: 0.4786  Scores: [0.5449027698241016, 0.4364811588823926, 0.4294003689612151, 0.4769838661102459, 0.505417501335739, 0.47827624602459046]
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.1090(0.1141)
Epoch: [4][0/391] Elapsed 0m 1s (remain 8m 31s) Loss: 0.1174(0.1174) Grad: 221099.5469  LR: 0.00001511
Epoch: [4][100/391] Elapsed 0m 52s (remain 2m 32s) Loss: 0.0820(0.1055) Grad: 123636.0703  LR: 0.00001511
Epoch: [4][200/391] Elapsed 1m 49s (remain 1m 43s) Loss: 0.1082(0.1020) Grad: 301854.5938  LR: 0.00001511
Epoch: [4][300/391] Elapsed 2m 43s (remain 0m 48s) Loss: 0.0866(0.1036) Grad: 126478.2344  LR: 0.00001511
Epoch: [4][390/391] Elapsed 3m 29s (remain 0m 0s) Loss: 0.0731(0.1028) Grad: 138529.7188  LR: 0.00000633
EVAL: [0/49] Elapsed 0m 1s (remain 1m 6s) Loss: 0.0974(0.0974)
[2022-11-17 02:08:28] - Epoch 4 - avg_train_loss: 0.1028  avg_val_loss: 0.1030  time: 242s
[2022-11-17 02:08:28] - Epoch 4 - Score: 0.4553  Scores: [0.4799653861332966, 0.43608486636229393, 0.4275335784461876, 0.4578232663074588, 0.4758375952277646, 0.4544935683947903]
[2022-11-17 02:08:28] - Epoch 4 - Save Best Score: 0.4553 Model
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0817(0.1030)
Epoch: [5][0/391] Elapsed 0m 0s (remain 4m 2s) Loss: 0.0855(0.0855) Grad: 130283.0547  LR: 0.00000711
Epoch: [5][100/391] Elapsed 0m 53s (remain 2m 33s) Loss: 0.0958(0.0989) Grad: 207288.8906  LR: 0.00000711
Epoch: [5][200/391] Elapsed 1m 47s (remain 1m 41s) Loss: 0.0777(0.0997) Grad: 150778.7656  LR: 0.00000711
Epoch: [5][300/391] Elapsed 2m 42s (remain 0m 48s) Loss: 0.1034(0.0988) Grad: 218952.4219  LR: 0.00000711
Epoch: [5][390/391] Elapsed 3m 28s (remain 0m 0s) Loss: 0.0993(0.0989) Grad: 188529.6875  LR: 0.00000124
EVAL: [0/49] Elapsed 0m 1s (remain 1m 6s) Loss: 0.0924(0.0924)
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0754(0.1048)
[2022-11-17 02:12:30] - Epoch 5 - avg_train_loss: 0.0989  avg_val_loss: 0.1048  time: 241s
[2022-11-17 02:12:30] - Epoch 5 - Score: 0.4589  Scores: [0.47790357501089714, 0.44053438142105955, 0.4276894920753474, 0.47582007381009866, 0.4766999939502122, 0.45495858283798934]
[2022-11-17 02:12:31] - ========== fold: 4 result ==========
[2022-11-17 02:12:31] - Score: 0.4553  Scores: [0.4799653861332966, 0.43608486636229393, 0.4275335784461876, 0.4578232663074588, 0.4758375952277646, 0.4544935683947903]
[2022-11-17 02:12:31] - ========== CV ==========
[2022-11-17 02:12:31] - Score: 0.4554  Scores: [0.4862037973280451, 0.4488251813048896, 0.4190941181542137, 0.4574056856057763, 0.4733451490367747, 0.4475438159200157]