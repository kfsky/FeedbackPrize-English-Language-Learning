Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3911/3911 [00:03<00:00, 987.56it/s]
[2022-11-02 17:24:35] - max_len: 2048
[2022-11-02 17:24:35] - ========== fold: 0 training ==========
[2022-11-02 17:24:35] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Reinitializing Last 2 Layers ...
Done.!
Epoch: [1][0/391] Elapsed 0m 1s (remain 10m 21s) Loss: 2.6680(2.6680) Grad: inf  LR: 0.00002994
Epoch: [1][100/391] Elapsed 0m 54s (remain 2m 37s) Loss: 0.1608(0.3256) Grad: 191624.3750  LR: 0.00002994
Epoch: [1][200/391] Elapsed 1m 49s (remain 1m 43s) Loss: 0.1032(0.2321) Grad: 103125.3125  LR: 0.00002994
Epoch: [1][300/391] Elapsed 2m 46s (remain 0m 49s) Loss: 0.0794(0.1951) Grad: 65891.5234  LR: 0.00002994
Epoch: [1][390/391] Elapsed 3m 29s (remain 0m 0s) Loss: 0.1053(0.1772) Grad: 65590.0625  LR: 0.00002821
EVAL: [0/49] Elapsed 0m 0s (remain 0m 35s) Loss: 0.1166(0.1166)
[2022-11-02 17:28:38] - Epoch 1 - avg_train_loss: 0.1772  avg_val_loss: 0.1073  time: 240s
[2022-11-02 17:28:38] - Epoch 1 - Score: 0.4637  Scores: [0.4996309888033582, 0.4643950548514061, 0.4150214007317844, 0.46072221817430997, 0.47876636417081103, 0.4637044600629805]
[2022-11-02 17:28:38] - Epoch 1 - Save Best Score: 0.4637 Model
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.1029(0.1073)
Epoch: [2][0/391] Elapsed 0m 0s (remain 4m 41s) Loss: 0.1252(0.1252) Grad: 431869.1250  LR: 0.00002864
Epoch: [2][100/391] Elapsed 0m 52s (remain 2m 30s) Loss: 0.0811(0.1145) Grad: 114946.6250  LR: 0.00002864
Epoch: [2][200/391] Elapsed 1m 48s (remain 1m 42s) Loss: 0.0879(0.1101) Grad: 101323.4531  LR: 0.00002864
Epoch: [2][300/391] Elapsed 2m 43s (remain 0m 48s) Loss: 0.1117(0.1097) Grad: 217224.2812  LR: 0.00002864
Epoch: [2][390/391] Elapsed 3m 31s (remain 0m 0s) Loss: 0.1561(0.1085) Grad: 190252.5625  LR: 0.00002231
EVAL: [0/49] Elapsed 0m 0s (remain 0m 34s) Loss: 0.1159(0.1159)
[2022-11-02 17:32:41] - Epoch 2 - avg_train_loss: 0.1085  avg_val_loss: 0.1081  time: 242s
[2022-11-02 17:32:41] - Epoch 2 - Score: 0.4654  Scores: [0.4918657152400823, 0.47361886552163024, 0.42159959823212184, 0.4861631107294719, 0.47077328788523365, 0.4480906885302313]
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.1018(0.1081)
Epoch: [3][0/391] Elapsed 0m 0s (remain 4m 50s) Loss: 0.1098(0.1098) Grad: 418774.1875  LR: 0.00002312
Epoch: [3][100/391] Elapsed 0m 53s (remain 2m 33s) Loss: 0.0914(0.1075) Grad: 160176.4844  LR: 0.00002312
Epoch: [3][200/391] Elapsed 1m 46s (remain 1m 40s) Loss: 0.1113(0.1057) Grad: 213935.8125  LR: 0.00002312
Epoch: [3][300/391] Elapsed 2m 40s (remain 0m 47s) Loss: 0.0917(0.1045) Grad: 178050.6875  LR: 0.00002312
Epoch: [3][390/391] Elapsed 3m 27s (remain 0m 0s) Loss: 0.0996(0.1037) Grad: 117562.2422  LR: 0.00001417
EVAL: [0/49] Elapsed 0m 0s (remain 0m 35s) Loss: 0.1047(0.1047)
[2022-11-02 17:36:40] - Epoch 3 - avg_train_loss: 0.1037  avg_val_loss: 0.1054  time: 239s
[2022-11-02 17:36:40] - Epoch 3 - Score: 0.4596  Scores: [0.4930645759560365, 0.47560717696832266, 0.42335827663454295, 0.4569415923965488, 0.4619383159927605, 0.4467157129923568]
[2022-11-02 17:36:40] - Epoch 3 - Save Best Score: 0.4596 Model
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.0987(0.1054)
Epoch: [4][0/391] Elapsed 0m 0s (remain 4m 35s) Loss: 0.0977(0.0977) Grad: 144361.1562  LR: 0.00001511
Epoch: [4][100/391] Elapsed 0m 58s (remain 2m 46s) Loss: 0.0711(0.1013) Grad: 113641.3594  LR: 0.00001511
Epoch: [4][200/391] Elapsed 1m 52s (remain 1m 46s) Loss: 0.1001(0.1040) Grad: 225391.0000  LR: 0.00001511
Epoch: [4][300/391] Elapsed 2m 43s (remain 0m 48s) Loss: 0.0717(0.1029) Grad: 98255.2969  LR: 0.00001511
Epoch: [4][390/391] Elapsed 3m 33s (remain 0m 0s) Loss: 0.0821(0.1017) Grad: 157864.3438  LR: 0.00000633
EVAL: [0/49] Elapsed 0m 0s (remain 0m 36s) Loss: 0.1122(0.1122)
[2022-11-02 17:40:46] - Epoch 4 - avg_train_loss: 0.1017  avg_val_loss: 0.1017  time: 244s
[2022-11-02 17:40:46] - Epoch 4 - Score: 0.4511  Scores: [0.4879374061823365, 0.45588764858883307, 0.4054967561749478, 0.4528369542441791, 0.4604994180214758, 0.4441954832337676]
[2022-11-02 17:40:46] - Epoch 4 - Save Best Score: 0.4511 Model
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.1035(0.1017)
Epoch: [5][0/391] Elapsed 0m 0s (remain 4m 46s) Loss: 0.0861(0.0861) Grad: 209815.4688  LR: 0.00000711
Epoch: [5][100/391] Elapsed 0m 58s (remain 2m 47s) Loss: 0.0819(0.0962) Grad: 100269.8203  LR: 0.00000711
Epoch: [5][200/391] Elapsed 1m 51s (remain 1m 45s) Loss: 0.1060(0.0971) Grad: 108866.1094  LR: 0.00000711
Epoch: [5][300/391] Elapsed 2m 42s (remain 0m 48s) Loss: 0.0842(0.0982) Grad: 154513.2344  LR: 0.00000711
Epoch: [5][390/391] Elapsed 3m 30s (remain 0m 0s) Loss: 0.0915(0.0986) Grad: 124087.1641  LR: 0.00000124
EVAL: [0/49] Elapsed 0m 0s (remain 0m 35s) Loss: 0.1150(0.1150)
[2022-11-02 17:44:49] - Epoch 5 - avg_train_loss: 0.0986  avg_val_loss: 0.1042  time: 242s
[2022-11-02 17:44:49] - Epoch 5 - Score: 0.4570  Scores: [0.4886190591646704, 0.46521466481199003, 0.4140165606948839, 0.4644474362799093, 0.4632358279060743, 0.44617974780509123]
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.1074(0.1042)
[2022-11-02 17:44:49] - ========== fold: 0 result ==========
[2022-11-02 17:44:49] - Score: 0.4511  Scores: [0.4879374061823365, 0.45588764858883307, 0.4054967561749478, 0.4528369542441791, 0.4604994180214758, 0.4441954832337676]
[2022-11-02 17:44:49] - ========== fold: 1 training ==========
[2022-11-02 17:44:49] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Reinitializing Last 2 Layers ...
Done.!
Epoch: [1][0/391] Elapsed 0m 1s (remain 7m 1s) Loss: 2.0906(2.0906) Grad: inf  LR: 0.00002994
Epoch: [1][100/391] Elapsed 0m 54s (remain 2m 35s) Loss: 0.0989(0.3170) Grad: 76092.9453  LR: 0.00002994
Epoch: [1][200/391] Elapsed 1m 45s (remain 1m 39s) Loss: 0.0892(0.2208) Grad: 67143.5859  LR: 0.00002994
Epoch: [1][300/391] Elapsed 2m 37s (remain 0m 47s) Loss: 0.0825(0.1890) Grad: 91763.8516  LR: 0.00002994
Epoch: [1][390/391] Elapsed 3m 27s (remain 0m 0s) Loss: 0.1126(0.1744) Grad: 93236.0391  LR: 0.00002821
EVAL: [0/49] Elapsed 0m 1s (remain 0m 59s) Loss: 0.1245(0.1245)
[2022-11-02 17:48:50] - Epoch 1 - avg_train_loss: 0.1744  avg_val_loss: 0.1198  time: 239s
[2022-11-02 17:48:50] - Epoch 1 - Score: 0.4911  Scores: [0.4977006429935503, 0.4892855438671127, 0.43061270217043207, 0.5275315239131563, 0.5040989504334138, 0.497666427915793]
[2022-11-02 17:48:50] - Epoch 1 - Save Best Score: 0.4911 Model
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.1072(0.1198)
Epoch: [2][0/391] Elapsed 0m 1s (remain 8m 49s) Loss: 0.1410(0.1410) Grad: 308847.2500  LR: 0.00002864
Epoch: [2][100/391] Elapsed 0m 55s (remain 2m 40s) Loss: 0.1133(0.1090) Grad: 175011.8594  LR: 0.00002864
Epoch: [2][200/391] Elapsed 1m 48s (remain 1m 42s) Loss: 0.0867(0.1123) Grad: 142893.6250  LR: 0.00002864
Epoch: [2][300/391] Elapsed 2m 42s (remain 0m 48s) Loss: 0.1129(0.1113) Grad: 281959.4688  LR: 0.00002864
Epoch: [2][390/391] Elapsed 3m 30s (remain 0m 0s) Loss: 0.1497(0.1103) Grad: 316627.9688  LR: 0.00002231
EVAL: [0/49] Elapsed 0m 1s (remain 0m 59s) Loss: 0.1131(0.1131)
[2022-11-02 17:52:52] - Epoch 2 - avg_train_loss: 0.1103  avg_val_loss: 0.1127  time: 241s
[2022-11-02 17:52:52] - Epoch 2 - Score: 0.4763  Scores: [0.4975710839519152, 0.45236571360604216, 0.440913912440553, 0.4807528916337638, 0.4834251179621675, 0.5027642850870204]
[2022-11-02 17:52:52] - Epoch 2 - Save Best Score: 0.4763 Model
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.1037(0.1127)
Epoch: [3][0/391] Elapsed 0m 0s (remain 4m 9s) Loss: 0.1182(0.1182) Grad: 100242.4922  LR: 0.00002312
Epoch: [3][100/391] Elapsed 0m 53s (remain 2m 34s) Loss: 0.0520(0.1052) Grad: 91900.5391  LR: 0.00002312
Epoch: [3][200/391] Elapsed 1m 42s (remain 1m 36s) Loss: 0.0793(0.1039) Grad: 126077.0859  LR: 0.00002312
Epoch: [3][300/391] Elapsed 2m 35s (remain 0m 46s) Loss: 0.1314(0.1050) Grad: 304949.9375  LR: 0.00002312
Epoch: [3][390/391] Elapsed 3m 24s (remain 0m 0s) Loss: 0.0760(0.1042) Grad: 65984.4297  LR: 0.00001417
EVAL: [0/49] Elapsed 0m 1s (remain 0m 59s) Loss: 0.1020(0.1020)
[2022-11-02 17:56:50] - Epoch 3 - avg_train_loss: 0.1042  avg_val_loss: 0.1093  time: 236s
[2022-11-02 17:56:50] - Epoch 3 - Score: 0.4690  Scores: [0.4954543824361846, 0.451214345620883, 0.42624205724455083, 0.479394287127567, 0.48088559158118205, 0.48109243030278265]
[2022-11-02 17:56:50] - Epoch 3 - Save Best Score: 0.4690 Model
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.1145(0.1093)
Epoch: [4][0/391] Elapsed 0m 1s (remain 6m 30s) Loss: 0.0894(0.0894) Grad: 277381.7500  LR: 0.00001511
Epoch: [4][100/391] Elapsed 0m 52s (remain 2m 29s) Loss: 0.0772(0.0995) Grad: 334347.7500  LR: 0.00001511
Epoch: [4][200/391] Elapsed 1m 48s (remain 1m 42s) Loss: 0.0919(0.0998) Grad: 198506.8125  LR: 0.00001511
Epoch: [4][300/391] Elapsed 2m 44s (remain 0m 49s) Loss: 0.0716(0.1011) Grad: 185125.6406  LR: 0.00001511
Epoch: [4][390/391] Elapsed 3m 31s (remain 0m 0s) Loss: 0.0914(0.1013) Grad: 114766.9141  LR: 0.00000633
EVAL: [0/49] Elapsed 0m 1s (remain 1m 0s) Loss: 0.1043(0.1043)
[2022-11-02 18:00:53] - Epoch 4 - avg_train_loss: 0.1013  avg_val_loss: 0.1069  time: 242s
[2022-11-02 18:00:53] - Epoch 4 - Score: 0.4640  Scores: [0.4933082063429618, 0.4567386807784105, 0.4251108936199594, 0.4760177038223513, 0.47838275948020237, 0.4541525437296434]
[2022-11-02 18:00:53] - Epoch 4 - Save Best Score: 0.4640 Model
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.1213(0.1069)
Epoch: [5][0/391] Elapsed 0m 1s (remain 7m 33s) Loss: 0.1210(0.1210) Grad: 224856.6250  LR: 0.00000711
Epoch: [5][100/391] Elapsed 0m 52s (remain 2m 29s) Loss: 0.0949(0.0967) Grad: 100532.7266  LR: 0.00000711
Epoch: [5][200/391] Elapsed 1m 46s (remain 1m 40s) Loss: 0.0577(0.0953) Grad: 130891.1562  LR: 0.00000711
Epoch: [5][300/391] Elapsed 2m 43s (remain 0m 48s) Loss: 0.0813(0.0966) Grad: 179694.3281  LR: 0.00000711
Epoch: [5][390/391] Elapsed 3m 32s (remain 0m 0s) Loss: 0.0975(0.0974) Grad: 154685.3125  LR: 0.00000124
EVAL: [0/49] Elapsed 0m 1s (remain 0m 59s) Loss: 0.1046(0.1046)
[2022-11-02 18:04:58] - Epoch 5 - avg_train_loss: 0.0974  avg_val_loss: 0.1065  time: 243s
[2022-11-02 18:04:58] - Epoch 5 - Score: 0.4627  Scores: [0.49508818724005554, 0.4533600908026956, 0.4257114890194275, 0.4723891654627158, 0.47594934650896087, 0.4539462773367466]
[2022-11-02 18:04:58] - Epoch 5 - Save Best Score: 0.4627 Model
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.1111(0.1065)
[2022-11-02 18:05:00] - ========== fold: 1 result ==========
[2022-11-02 18:05:00] - Score: 0.4627  Scores: [0.49508818724005554, 0.4533600908026956, 0.4257114890194275, 0.4723891654627158, 0.47594934650896087, 0.4539462773367466]
[2022-11-02 18:05:00] - ========== fold: 2 training ==========
[2022-11-02 18:05:00] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Reinitializing Last 2 Layers ...
Done.!
Epoch: [1][0/391] Elapsed 0m 0s (remain 3m 31s) Loss: 2.8339(2.8339) Grad: inf  LR: 0.00002994
Epoch: [1][100/391] Elapsed 0m 58s (remain 2m 47s) Loss: 0.1466(0.3428) Grad: 101186.7812  LR: 0.00002994
Epoch: [1][200/391] Elapsed 1m 48s (remain 1m 42s) Loss: 0.1105(0.2433) Grad: 95713.6953  LR: 0.00002994
Epoch: [1][300/391] Elapsed 2m 43s (remain 0m 48s) Loss: 0.1458(0.2032) Grad: 113537.7578  LR: 0.00002994
Epoch: [1][390/391] Elapsed 3m 28s (remain 0m 0s) Loss: 0.0748(0.1825) Grad: 81309.3516  LR: 0.00002821
EVAL: [0/49] Elapsed 0m 0s (remain 0m 35s) Loss: 0.0894(0.0894)
[2022-11-02 18:09:03] - Epoch 1 - avg_train_loss: 0.1825  avg_val_loss: 0.1149  time: 241s
[2022-11-02 18:09:03] - Epoch 1 - Score: 0.4807  Scores: [0.508160428077181, 0.4764899238971766, 0.4516725686170476, 0.45865392310247666, 0.4985732854951529, 0.4907483093069242]
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0760(0.1149)
[2022-11-02 18:09:03] - Epoch 1 - Save Best Score: 0.4807 Model
Epoch: [2][0/391] Elapsed 0m 0s (remain 4m 56s) Loss: 0.1230(0.1230) Grad: 145848.7969  LR: 0.00002864
Epoch: [2][100/391] Elapsed 0m 54s (remain 2m 37s) Loss: 0.1311(0.1074) Grad: 231592.4531  LR: 0.00002864
Epoch: [2][200/391] Elapsed 1m 48s (remain 1m 43s) Loss: 0.1433(0.1096) Grad: 425283.3125  LR: 0.00002864
Epoch: [2][300/391] Elapsed 2m 38s (remain 0m 47s) Loss: 0.1286(0.1087) Grad: 400020.5938  LR: 0.00002864
Epoch: [2][390/391] Elapsed 3m 28s (remain 0m 0s) Loss: 0.0725(0.1076) Grad: 152063.4688  LR: 0.00002231
EVAL: [0/49] Elapsed 0m 0s (remain 0m 35s) Loss: 0.0745(0.0745)
[2022-11-02 18:13:05] - Epoch 2 - avg_train_loss: 0.1076  avg_val_loss: 0.1146  time: 241s
[2022-11-02 18:13:05] - Epoch 2 - Score: 0.4806  Scores: [0.5028974345907243, 0.4926963776269653, 0.44697721875135776, 0.46963452060988997, 0.4976246387448446, 0.4739297727322583]
[2022-11-02 18:13:05] - Epoch 2 - Save Best Score: 0.4806 Model
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0720(0.1146)
Epoch: [3][0/391] Elapsed 0m 0s (remain 4m 28s) Loss: 0.0816(0.0816) Grad: 91372.9766  LR: 0.00002312
Epoch: [3][100/391] Elapsed 0m 55s (remain 2m 38s) Loss: 0.1070(0.1061) Grad: 173749.6094  LR: 0.00002312
Epoch: [3][200/391] Elapsed 1m 46s (remain 1m 41s) Loss: 0.0773(0.1044) Grad: 177503.3594  LR: 0.00002312
Epoch: [3][300/391] Elapsed 2m 41s (remain 0m 48s) Loss: 0.1067(0.1033) Grad: 242786.7812  LR: 0.00002312
Epoch: [3][390/391] Elapsed 3m 27s (remain 0m 0s) Loss: 0.0951(0.1026) Grad: 285466.1875  LR: 0.00001417
EVAL: [0/49] Elapsed 0m 0s (remain 0m 35s) Loss: 0.0791(0.0791)
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0691(0.1115)
[2022-11-02 18:17:08] - Epoch 3 - avg_train_loss: 0.1026  avg_val_loss: 0.1115  time: 241s
[2022-11-02 18:17:08] - Epoch 3 - Score: 0.4729  Scores: [0.5075549336445428, 0.4778230303575506, 0.4272232778836204, 0.44479166814891385, 0.5146019726338611, 0.4654503767996877]
[2022-11-02 18:17:08] - Epoch 3 - Save Best Score: 0.4729 Model
Epoch: [4][0/391] Elapsed 0m 0s (remain 4m 44s) Loss: 0.0909(0.0909) Grad: 113674.1641  LR: 0.00001511
Epoch: [4][100/391] Elapsed 0m 50s (remain 2m 25s) Loss: 0.0841(0.0972) Grad: 282830.0625  LR: 0.00001511
Epoch: [4][200/391] Elapsed 1m 44s (remain 1m 38s) Loss: 0.1033(0.0981) Grad: 116212.5078  LR: 0.00001511
Epoch: [4][300/391] Elapsed 2m 39s (remain 0m 47s) Loss: 0.0929(0.0993) Grad: 74954.5625  LR: 0.00001511
Epoch: [4][390/391] Elapsed 3m 26s (remain 0m 0s) Loss: 0.0822(0.0996) Grad: 178652.1406  LR: 0.00000633
EVAL: [0/49] Elapsed 0m 0s (remain 0m 36s) Loss: 0.0744(0.0744)
[2022-11-02 18:21:08] - Epoch 4 - avg_train_loss: 0.0996  avg_val_loss: 0.1062  time: 239s
[2022-11-02 18:21:08] - Epoch 4 - Score: 0.4619  Scores: [0.49715247134917673, 0.4613182295049185, 0.4269278869684913, 0.44440321701048485, 0.4788853360271195, 0.4625559837676445]
[2022-11-02 18:21:08] - Epoch 4 - Save Best Score: 0.4619 Model
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0766(0.1062)
Epoch: [5][0/391] Elapsed 0m 0s (remain 5m 59s) Loss: 0.1008(0.1008) Grad: 103520.8203  LR: 0.00000711
Epoch: [5][100/391] Elapsed 0m 50s (remain 2m 25s) Loss: 0.0864(0.0982) Grad: 92505.1719  LR: 0.00000711
Epoch: [5][200/391] Elapsed 1m 41s (remain 1m 36s) Loss: 0.1667(0.0966) Grad: 440260.3125  LR: 0.00000711
Epoch: [5][300/391] Elapsed 2m 34s (remain 0m 46s) Loss: 0.0779(0.0965) Grad: 216947.0469  LR: 0.00000711
Epoch: [5][390/391] Elapsed 3m 25s (remain 0m 0s) Loss: 0.0591(0.0960) Grad: 59896.3281  LR: 0.00000124
EVAL: [0/49] Elapsed 0m 0s (remain 0m 36s) Loss: 0.0735(0.0735)
[2022-11-02 18:25:07] - Epoch 5 - avg_train_loss: 0.0960  avg_val_loss: 0.1104  time: 238s
[2022-11-02 18:25:07] - Epoch 5 - Score: 0.4716  Scores: [0.500662037277494, 0.4767901562097145, 0.42853893269772664, 0.45892544615825587, 0.4845490624521388, 0.47996056658231595]
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0740(0.1104)
[2022-11-02 18:25:08] - ========== fold: 2 result ==========
[2022-11-02 18:25:08] - Score: 0.4619  Scores: [0.49715247134917673, 0.4613182295049185, 0.4269278869684913, 0.44440321701048485, 0.4788853360271195, 0.4625559837676445]
[2022-11-02 18:25:08] - ========== fold: 3 training ==========
[2022-11-02 18:25:08] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Reinitializing Last 2 Layers ...
Done.!
Epoch: [1][0/391] Elapsed 0m 1s (remain 7m 7s) Loss: 2.4497(2.4497) Grad: inf  LR: 0.00002994
Epoch: [1][100/391] Elapsed 0m 53s (remain 2m 33s) Loss: 0.1644(0.3167) Grad: 107365.5000  LR: 0.00002994
Epoch: [1][200/391] Elapsed 1m 44s (remain 1m 39s) Loss: 0.1179(0.2246) Grad: 143351.8125  LR: 0.00002994
Epoch: [1][300/391] Elapsed 2m 40s (remain 0m 48s) Loss: 0.0787(0.1911) Grad: 70483.3281  LR: 0.00002994
Epoch: [1][390/391] Elapsed 3m 28s (remain 0m 0s) Loss: 0.0636(0.1737) Grad: 81792.2266  LR: 0.00002821
EVAL: [0/49] Elapsed 0m 1s (remain 0m 59s) Loss: 0.1391(0.1391)
EVAL: [48/49] Elapsed 0m 33s (remain 0m 0s) Loss: 0.1019(0.1385)
[2022-11-02 18:29:12] - Epoch 1 - avg_train_loss: 0.1737  avg_val_loss: 0.1385  time: 242s
[2022-11-02 18:29:12] - Epoch 1 - Score: 0.5277  Scores: [0.5625325386559347, 0.511190858916626, 0.4583116698342865, 0.6138933119181812, 0.5023411513522094, 0.5178822461782957]
[2022-11-02 18:29:12] - Epoch 1 - Save Best Score: 0.5277 Model
Epoch: [2][0/391] Elapsed 0m 0s (remain 3m 38s) Loss: 0.1394(0.1394) Grad: 337937.6562  LR: 0.00002864
Epoch: [2][100/391] Elapsed 0m 51s (remain 2m 27s) Loss: 0.1026(0.1153) Grad: 146838.6875  LR: 0.00002864
Epoch: [2][200/391] Elapsed 1m 44s (remain 1m 38s) Loss: 0.0745(0.1109) Grad: 202047.2344  LR: 0.00002864
Epoch: [2][300/391] Elapsed 2m 38s (remain 0m 47s) Loss: 0.1164(0.1090) Grad: 170180.6562  LR: 0.00002864
Epoch: [2][390/391] Elapsed 3m 24s (remain 0m 0s) Loss: 0.0921(0.1083) Grad: 135117.8125  LR: 0.00002231
EVAL: [0/49] Elapsed 0m 1s (remain 1m 0s) Loss: 0.1233(0.1233)
[2022-11-02 18:33:12] - Epoch 2 - avg_train_loss: 0.1083  avg_val_loss: 0.1065  time: 238s
[2022-11-02 18:33:12] - Epoch 2 - Score: 0.4621  Scores: [0.49436792235792165, 0.4527214183143741, 0.4223928053423174, 0.46237779228936393, 0.49707345272260894, 0.4437810521886022]
[2022-11-02 18:33:12] - Epoch 2 - Save Best Score: 0.4621 Model
EVAL: [48/49] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0859(0.1065)
Epoch: [3][0/391] Elapsed 0m 0s (remain 3m 40s) Loss: 0.1160(0.1160) Grad: 213079.8438  LR: 0.00002312
Epoch: [3][100/391] Elapsed 0m 53s (remain 2m 34s) Loss: 0.1349(0.1038) Grad: 300107.5938  LR: 0.00002312
Epoch: [3][200/391] Elapsed 1m 48s (remain 1m 42s) Loss: 0.0715(0.1015) Grad: 99418.8750  LR: 0.00002312
Epoch: [3][300/391] Elapsed 2m 40s (remain 0m 47s) Loss: 0.1011(0.1041) Grad: 142757.6250  LR: 0.00002312
Epoch: [3][390/391] Elapsed 3m 28s (remain 0m 0s) Loss: 0.0947(0.1051) Grad: 105184.2969  LR: 0.00001417
EVAL: [0/49] Elapsed 0m 1s (remain 0m 58s) Loss: 0.1244(0.1244)
EVAL: [48/49] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0780(0.1090)
[2022-11-02 18:37:14] - Epoch 3 - avg_train_loss: 0.1051  avg_val_loss: 0.1090  time: 241s
[2022-11-02 18:37:14] - Epoch 3 - Score: 0.4677  Scores: [0.48544226285805187, 0.4546707590771791, 0.43308297648779476, 0.46517176635066576, 0.5268582893241512, 0.4409709264130238]
Epoch: [4][0/391] Elapsed 0m 0s (remain 4m 57s) Loss: 0.0811(0.0811) Grad: 186931.7812  LR: 0.00001511
Epoch: [4][100/391] Elapsed 0m 50s (remain 2m 25s) Loss: 0.1140(0.0977) Grad: 93466.5859  LR: 0.00001511
Epoch: [4][200/391] Elapsed 1m 46s (remain 1m 40s) Loss: 0.1628(0.1028) Grad: 427332.4375  LR: 0.00001511
Epoch: [4][300/391] Elapsed 2m 36s (remain 0m 46s) Loss: 0.1043(0.1014) Grad: 257174.2656  LR: 0.00001511
Epoch: [4][390/391] Elapsed 3m 25s (remain 0m 0s) Loss: 0.0826(0.1017) Grad: 74810.9531  LR: 0.00000633
EVAL: [0/49] Elapsed 0m 1s (remain 0m 57s) Loss: 0.1261(0.1261)
[2022-11-02 18:41:13] - Epoch 4 - avg_train_loss: 0.1017  avg_val_loss: 0.1039  time: 239s
[2022-11-02 18:41:13] - Epoch 4 - Score: 0.4567  Scores: [0.4787734935062982, 0.4432317112008603, 0.42160346094650036, 0.4609503572578082, 0.49135574962877376, 0.44409858968379334]
[2022-11-02 18:41:13] - Epoch 4 - Save Best Score: 0.4567 Model
EVAL: [48/49] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0777(0.1039)
Epoch: [5][0/391] Elapsed 0m 0s (remain 3m 54s) Loss: 0.0728(0.0728) Grad: 93566.3594  LR: 0.00000711
Epoch: [5][100/391] Elapsed 0m 55s (remain 2m 39s) Loss: 0.0907(0.0947) Grad: 119737.2344  LR: 0.00000711
Epoch: [5][200/391] Elapsed 1m 48s (remain 1m 42s) Loss: 0.1014(0.0971) Grad: 137127.4219  LR: 0.00000711
Epoch: [5][300/391] Elapsed 2m 38s (remain 0m 47s) Loss: 0.1138(0.0978) Grad: 179501.6094  LR: 0.00000711
Epoch: [5][390/391] Elapsed 3m 28s (remain 0m 0s) Loss: 0.0940(0.0973) Grad: 134104.4219  LR: 0.00000124
EVAL: [0/49] Elapsed 0m 1s (remain 0m 57s) Loss: 0.1244(0.1244)
[2022-11-02 18:45:17] - Epoch 5 - avg_train_loss: 0.0973  avg_val_loss: 0.1058  time: 242s
[2022-11-02 18:45:17] - Epoch 5 - Score: 0.4607  Scores: [0.485173893639139, 0.4480671962404474, 0.42305248113479055, 0.4679894685722385, 0.4964950388200356, 0.44316583851213515]
EVAL: [48/49] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0766(0.1058)
[2022-11-02 18:45:17] - ========== fold: 3 result ==========
[2022-11-02 18:45:17] - Score: 0.4567  Scores: [0.4787734935062982, 0.4432317112008603, 0.42160346094650036, 0.4609503572578082, 0.49135574962877376, 0.44409858968379334]
[2022-11-02 18:45:17] - ========== fold: 4 training ==========
[2022-11-02 18:45:17] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Reinitializing Last 2 Layers ...
Done.!
Epoch: [1][0/391] Elapsed 0m 0s (remain 3m 52s) Loss: 2.3121(2.3121) Grad: inf  LR: 0.00002994
Epoch: [1][100/391] Elapsed 0m 52s (remain 2m 30s) Loss: 0.1057(0.2934) Grad: 64435.4531  LR: 0.00002994
Epoch: [1][200/391] Elapsed 1m 44s (remain 1m 38s) Loss: 0.1625(0.2097) Grad: 232319.3438  LR: 0.00002994
Epoch: [1][300/391] Elapsed 2m 41s (remain 0m 48s) Loss: 0.1476(0.1797) Grad: 83157.6250  LR: 0.00002994
Epoch: [1][390/391] Elapsed 3m 32s (remain 0m 0s) Loss: 0.2094(0.1661) Grad: 276728.6250  LR: 0.00002821
EVAL: [0/49] Elapsed 0m 1s (remain 1m 7s) Loss: 0.1033(0.1033)
EVAL: [48/49] Elapsed 0m 31s (remain 0m 0s) Loss: 0.1169(0.1201)
[2022-11-02 18:49:23] - Epoch 1 - avg_train_loss: 0.1661  avg_val_loss: 0.1201  time: 244s
[2022-11-02 18:49:23] - Epoch 1 - Score: 0.4913  Scores: [0.4913338071189551, 0.4509714926357894, 0.4588490993025876, 0.544157978794552, 0.5045209815442703, 0.4980543046524673]
[2022-11-02 18:49:23] - Epoch 1 - Save Best Score: 0.4913 Model
Epoch: [2][0/391] Elapsed 0m 1s (remain 6m 33s) Loss: 0.0820(0.0820) Grad: 149514.1250  LR: 0.00002864
Epoch: [2][100/391] Elapsed 0m 54s (remain 2m 36s) Loss: 0.0799(0.1125) Grad: 117912.3750  LR: 0.00002864
Epoch: [2][200/391] Elapsed 1m 43s (remain 1m 37s) Loss: 0.1294(0.1089) Grad: 175192.9844  LR: 0.00002864
Epoch: [2][300/391] Elapsed 2m 37s (remain 0m 47s) Loss: 0.1020(0.1088) Grad: 181787.8125  LR: 0.00002864
Epoch: [2][390/391] Elapsed 3m 26s (remain 0m 0s) Loss: 0.0786(0.1088) Grad: 147499.7188  LR: 0.00002231
EVAL: [0/49] Elapsed 0m 1s (remain 1m 7s) Loss: 0.1106(0.1106)
[2022-11-02 18:53:23] - Epoch 2 - avg_train_loss: 0.1088  avg_val_loss: 0.1095  time: 239s
[2022-11-02 18:53:23] - Epoch 2 - Score: 0.4695  Scores: [0.48827433649872776, 0.43684089630994183, 0.4670163917195796, 0.46385558475226996, 0.4968992186740725, 0.46439778225423123]
EVAL: [48/49] Elapsed 0m 31s (remain 0m 0s) Loss: 0.0885(0.1095)
[2022-11-02 18:53:23] - Epoch 2 - Save Best Score: 0.4695 Model
Epoch: [3][0/391] Elapsed 0m 0s (remain 3m 22s) Loss: 0.1199(0.1199) Grad: 150997.7969  LR: 0.00002312
Epoch: [3][100/391] Elapsed 0m 51s (remain 2m 28s) Loss: 0.1142(0.1039) Grad: 169104.5781  LR: 0.00002312
Epoch: [3][200/391] Elapsed 1m 45s (remain 1m 40s) Loss: 0.1308(0.1034) Grad: 208148.9844  LR: 0.00002312
Epoch: [3][300/391] Elapsed 2m 38s (remain 0m 47s) Loss: 0.0886(0.1043) Grad: 137427.5000  LR: 0.00002312
Epoch: [3][390/391] Elapsed 3m 27s (remain 0m 0s) Loss: 0.0714(0.1053) Grad: 100089.4375  LR: 0.00001417
EVAL: [0/49] Elapsed 0m 1s (remain 1m 6s) Loss: 0.0998(0.0998)
[2022-11-02 18:57:25] - Epoch 3 - avg_train_loss: 0.1053  avg_val_loss: 0.1091  time: 240s
[2022-11-02 18:57:25] - Epoch 3 - Score: 0.4686  Scores: [0.5033031295882765, 0.463463919877408, 0.4488039673541285, 0.4619756543716753, 0.4825548193994484, 0.45171597131072216]
[2022-11-02 18:57:25] - Epoch 3 - Save Best Score: 0.4686 Model
EVAL: [48/49] Elapsed 0m 31s (remain 0m 0s) Loss: 0.0768(0.1091)
Epoch: [4][0/391] Elapsed 0m 1s (remain 7m 14s) Loss: 0.0947(0.0947) Grad: 155665.7500  LR: 0.00001511
Epoch: [4][100/391] Elapsed 0m 54s (remain 2m 35s) Loss: 0.1146(0.1075) Grad: 244011.9062  LR: 0.00001511
Epoch: [4][200/391] Elapsed 1m 45s (remain 1m 39s) Loss: 0.0937(0.1010) Grad: 121097.8594  LR: 0.00001511
Epoch: [4][300/391] Elapsed 2m 41s (remain 0m 48s) Loss: 0.0697(0.0997) Grad: 169516.4844  LR: 0.00001511
Epoch: [4][390/391] Elapsed 3m 27s (remain 0m 0s) Loss: 0.0916(0.1016) Grad: 223958.4531  LR: 0.00000633
EVAL: [0/49] Elapsed 0m 1s (remain 1m 7s) Loss: 0.0929(0.0929)
EVAL: [48/49] Elapsed 0m 31s (remain 0m 0s) Loss: 0.0791(0.1043)
[2022-11-02 19:01:26] - Epoch 4 - avg_train_loss: 0.1016  avg_val_loss: 0.1043  time: 240s
[2022-11-02 19:01:26] - Epoch 4 - Score: 0.4579  Scores: [0.48087500578996784, 0.43710866527689984, 0.4276016913582009, 0.4598829813996901, 0.48372628763920256, 0.4579494242525864]
[2022-11-02 19:01:26] - Epoch 4 - Save Best Score: 0.4579 Model
Epoch: [5][0/391] Elapsed 0m 1s (remain 7m 42s) Loss: 0.0614(0.0614) Grad: 162867.6250  LR: 0.00000711
Epoch: [5][100/391] Elapsed 0m 51s (remain 2m 28s) Loss: 0.0949(0.0964) Grad: 163462.8750  LR: 0.00000711
Epoch: [5][200/391] Elapsed 1m 46s (remain 1m 40s) Loss: 0.1223(0.0973) Grad: 214407.7656  LR: 0.00000711
Epoch: [5][300/391] Elapsed 2m 40s (remain 0m 47s) Loss: 0.1317(0.0956) Grad: 433796.8438  LR: 0.00000711
Epoch: [5][390/391] Elapsed 3m 30s (remain 0m 0s) Loss: 0.0526(0.0971) Grad: 174110.2812  LR: 0.00000124
EVAL: [0/49] Elapsed 0m 1s (remain 1m 6s) Loss: 0.1136(0.1136)
[2022-11-02 19:05:29] - Epoch 5 - avg_train_loss: 0.0971  avg_val_loss: 0.1052  time: 242s
[2022-11-02 19:05:29] - Epoch 5 - Score: 0.4598  Scores: [0.4791109266544228, 0.4394606903220537, 0.4247188895101895, 0.4629701758139041, 0.4962914030635889, 0.45601331926217237]
[2022-11-02 19:05:30] - ========== fold: 4 result ==========
[2022-11-02 19:05:30] - Score: 0.4579  Scores: [0.48087500578996784, 0.43710866527689984, 0.4276016913582009, 0.4598829813996901, 0.48372628763920256, 0.4579494242525864]
[2022-11-02 19:05:30] - ========== CV ==========
[2022-11-02 19:05:30] - Score: 0.4581  Scores: [0.4880224415781954, 0.4502677730630097, 0.4215501168819074, 0.45819025201297564, 0.47819191070740424, 0.4526096928168561]
EVAL: [48/49] Elapsed 0m 31s (remain 0m 0s) Loss: 0.0876(0.1052)