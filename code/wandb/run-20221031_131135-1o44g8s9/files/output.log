Downloading tokenizer_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 52.0/52.0 [00:00<00:00, 98.1kB/s]
Downloading config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 633/633 [00:00<00:00, 1.38MB/s]
Downloading spm.model: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.33M/2.33M [00:00<00:00, 24.2MB/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|                                                                                                                                                                                                                                                            | 0/3911 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3911/3911 [00:03<00:00, 1005.45it/s]
[2022-10-31 13:11:41] - max_len: 2048
[2022-10-31 13:11:41] - ========== fold: 0 training ==========
[2022-10-31 13:11:41] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v2-xlarge",
  "attention_dropout": 0.0,
  "attention_head_size": 64,
  "attention_probs_dropout_prob": 0.0,
  "conv_act": "gelu",
  "conv_kernel_size": 3,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 24,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1536,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}












Downloading pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.65G/1.65G [00:23<00:00, 74.6MB/s]
Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/391] Elapsed 0m 3s (remain 21m 36s) Loss: 3.0219(3.0219) Grad: inf  LR: 0.00000200
Epoch: [1][100/391] Elapsed 2m 4s (remain 5m 58s) Loss: 0.1040(0.9734) Grad: 68807.0000  LR: 0.00000200
Epoch: [1][200/391] Elapsed 3m 59s (remain 3m 46s) Loss: 0.1418(0.5537) Grad: 42988.0938  LR: 0.00000200
Epoch: [1][300/391] Elapsed 6m 7s (remain 1m 49s) Loss: 0.0630(0.4071) Grad: 36504.5156  LR: 0.00000200
Epoch: [1][390/391] Elapsed 7m 57s (remain 0m 0s) Loss: 0.1357(0.3397) Grad: 71733.6250  LR: 0.00000189
EVAL: [0/49] Elapsed 0m 2s (remain 1m 36s) Loss: 0.1342(0.1342)
EVAL: [48/49] Elapsed 1m 42s (remain 0m 0s) Loss: 0.1323(0.1122)
[2022-10-31 13:21:54] - Epoch 1 - avg_train_loss: 0.3397  avg_val_loss: 0.1122  time: 580s
[2022-10-31 13:21:54] - Epoch 1 - Score: 0.4751  Scores: [0.502956576120361, 0.48004374497526847, 0.4408192373242195, 0.4629351545724332, 0.4766463653358433, 0.48703579033023636]
[2022-10-31 13:21:54] - Epoch 1 - Save Best Score: 0.4751 Model
Epoch: [2][0/391] Elapsed 0m 1s (remain 11m 19s) Loss: 0.1219(0.1219) Grad: inf  LR: 0.00000191
Epoch: [2][100/391] Elapsed 2m 0s (remain 5m 46s) Loss: 0.0967(0.1042) Grad: 92884.8984  LR: 0.00000191
Epoch: [2][200/391] Elapsed 4m 5s (remain 3m 52s) Loss: 0.1009(0.1043) Grad: 74397.2578  LR: 0.00000191
Epoch: [2][300/391] Elapsed 6m 2s (remain 1m 48s) Loss: 0.0871(0.1055) Grad: 94746.7656  LR: 0.00000191
Epoch: [2][390/391] Elapsed 7m 54s (remain 0m 0s) Loss: 0.0891(0.1053) Grad: 109070.9766  LR: 0.00000151
EVAL: [0/49] Elapsed 0m 2s (remain 1m 37s) Loss: 0.1215(0.1215)
[2022-10-31 13:31:36] - Epoch 2 - avg_train_loss: 0.1053  avg_val_loss: 0.1106  time: 577s
[2022-10-31 13:31:36] - Epoch 2 - Score: 0.4713  Scores: [0.4930663830286579, 0.48007998617469344, 0.426458406496676, 0.4843572687475062, 0.4752831480721103, 0.46831226474117343]
[2022-10-31 13:31:36] - Epoch 2 - Save Best Score: 0.4713 Model
EVAL: [48/49] Elapsed 1m 41s (remain 0m 0s) Loss: 0.1212(0.1106)
Epoch: [3][0/391] Elapsed 0m 1s (remain 7m 46s) Loss: 0.0969(0.0969) Grad: 238185.2344  LR: 0.00000156
Epoch: [3][100/391] Elapsed 2m 6s (remain 6m 1s) Loss: 0.0616(0.0984) Grad: 104649.6094  LR: 0.00000156
Epoch: [3][200/391] Elapsed 4m 15s (remain 4m 1s) Loss: 0.1261(0.0995) Grad: 28040.6543  LR: 0.00000156
Epoch: [3][300/391] Elapsed 6m 18s (remain 1m 53s) Loss: 0.0844(0.1015) Grad: 37208.4453  LR: 0.00000156
Epoch: [3][390/391] Elapsed 8m 3s (remain 0m 0s) Loss: 0.0652(0.1014) Grad: 22371.5098  LR: 0.00000099
EVAL: [0/49] Elapsed 0m 2s (remain 1m 39s) Loss: 0.1200(0.1200)
EVAL: [48/49] Elapsed 1m 41s (remain 0m 0s) Loss: 0.1240(0.1068)
[2022-10-31 13:41:28] - Epoch 3 - avg_train_loss: 0.1014  avg_val_loss: 0.1068  time: 586s
[2022-10-31 13:41:28] - Epoch 3 - Score: 0.4629  Scores: [0.4964305446719307, 0.4708685854554681, 0.41241366378707933, 0.4656260590064897, 0.47955967751227657, 0.45231131186306717]
[2022-10-31 13:41:28] - Epoch 3 - Save Best Score: 0.4629 Model
Epoch: [4][0/391] Elapsed 0m 0s (remain 5m 43s) Loss: 0.0692(0.0692) Grad: inf  LR: 0.00000105
Epoch: [4][100/391] Elapsed 2m 8s (remain 6m 7s) Loss: 0.0855(0.0918) Grad: 90486.3984  LR: 0.00000105
Epoch: [4][200/391] Elapsed 4m 6s (remain 3m 52s) Loss: 0.0848(0.0898) Grad: 33634.5469  LR: 0.00000105
Epoch: [4][300/391] Elapsed 6m 2s (remain 1m 48s) Loss: 0.0777(0.0933) Grad: 37637.5117  LR: 0.00000105
Epoch: [4][390/391] Elapsed 7m 58s (remain 0m 0s) Loss: 0.0858(0.0962) Grad: 18235.4609  LR: 0.00000050
EVAL: [0/49] Elapsed 0m 2s (remain 1m 38s) Loss: 0.1193(0.1193)
EVAL: [48/49] Elapsed 1m 41s (remain 0m 0s) Loss: 0.1233(0.1154)
[2022-10-31 13:51:14] - Epoch 4 - avg_train_loss: 0.0962  avg_val_loss: 0.1154  time: 581s
[2022-10-31 13:51:14] - Epoch 4 - Score: 0.4813  Scores: [0.5047287495756868, 0.4971175549541558, 0.4219167384751223, 0.4685272208760245, 0.5016601347265028, 0.4938217552016668]
Epoch: [5][0/391] Elapsed 0m 1s (remain 8m 9s) Loss: 0.0839(0.0839) Grad: inf  LR: 0.00000055
Epoch: [5][100/391] Elapsed 2m 5s (remain 6m 0s) Loss: 0.1172(0.0881) Grad: 71195.2969  LR: 0.00000055
Epoch: [5][200/391] Elapsed 4m 11s (remain 3m 57s) Loss: 0.1014(0.0877) Grad: 30169.8320  LR: 0.00000055
Epoch: [5][300/391] Elapsed 6m 13s (remain 1m 51s) Loss: 0.1170(0.0886) Grad: 40124.3047  LR: 0.00000055
Epoch: [5][390/391] Elapsed 7m 59s (remain 0m 0s) Loss: 0.0642(0.0894) Grad: 18839.5312  LR: 0.00000017
EVAL: [0/49] Elapsed 0m 2s (remain 1m 37s) Loss: 0.1144(0.1144)
EVAL: [48/49] Elapsed 1m 41s (remain 0m 0s) Loss: 0.1154(0.1038)
[2022-10-31 14:00:56] - Epoch 5 - avg_train_loss: 0.0894  avg_val_loss: 0.1038  time: 582s
[2022-10-31 14:00:56] - Epoch 5 - Score: 0.4562  Scores: [0.4889369377237626, 0.4643365230885086, 0.41474058534220654, 0.4521886711569201, 0.4676093712701163, 0.4496874121542718]
[2022-10-31 14:00:56] - Epoch 5 - Save Best Score: 0.4562 Model
[2022-10-31 14:01:05] - ========== fold: 0 result ==========
[2022-10-31 14:01:05] - Score: 0.4562  Scores: [0.4889369377237626, 0.4643365230885086, 0.41474058534220654, 0.4521886711569201, 0.4676093712701163, 0.4496874121542718]
[2022-10-31 14:01:05] - ========== fold: 1 training ==========
[2022-10-31 14:01:05] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v2-xlarge",
  "attention_dropout": 0.0,
  "attention_head_size": 64,
  "attention_probs_dropout_prob": 0.0,
  "conv_act": "gelu",
  "conv_kernel_size": 3,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 24,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1536,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/391] Elapsed 0m 1s (remain 8m 46s) Loss: 2.5617(2.5617) Grad: inf  LR: 0.00000200
Epoch: [1][100/391] Elapsed 2m 5s (remain 6m 1s) Loss: 0.0726(1.0439) Grad: 15085.9277  LR: 0.00000200
Epoch: [1][200/391] Elapsed 4m 9s (remain 3m 56s) Loss: 0.1303(0.5869) Grad: 29684.2871  LR: 0.00000200
Epoch: [1][300/391] Elapsed 6m 18s (remain 1m 53s) Loss: 0.1251(0.4334) Grad: 14251.6836  LR: 0.00000200
Epoch: [1][390/391] Elapsed 8m 4s (remain 0m 0s) Loss: 0.1329(0.3608) Grad: 23497.1543  LR: 0.00000189
EVAL: [0/49] Elapsed 0m 3s (remain 2m 45s) Loss: 0.1580(0.1580)
EVAL: [48/49] Elapsed 1m 43s (remain 0m 0s) Loss: 0.1614(0.1685)
[2022-10-31 14:11:00] - Epoch 1 - avg_train_loss: 0.3608  avg_val_loss: 0.1685  time: 588s
[2022-10-31 14:11:00] - Epoch 1 - Score: 0.5785  Scores: [0.8005148035909677, 0.5462138706113142, 0.452064136233785, 0.5586550385105026, 0.5525312069983331, 0.5610614820246341]
[2022-10-31 14:11:00] - Epoch 1 - Save Best Score: 0.5785 Model
Epoch: [2][0/391] Elapsed 0m 1s (remain 9m 34s) Loss: 0.2122(0.2122) Grad: inf  LR: 0.00000191
Epoch: [2][100/391] Elapsed 1m 57s (remain 5m 37s) Loss: 0.0853(0.1120) Grad: 90540.0547  LR: 0.00000191
Epoch: [2][200/391] Elapsed 4m 4s (remain 3m 50s) Loss: 0.1329(0.1087) Grad: 105644.7969  LR: 0.00000191
Epoch: [2][300/391] Elapsed 6m 6s (remain 1m 49s) Loss: 0.0643(0.1069) Grad: 25045.3086  LR: 0.00000191
Epoch: [2][390/391] Elapsed 7m 59s (remain 0m 0s) Loss: 0.1069(0.1065) Grad: 19920.3438  LR: 0.00000151
EVAL: [0/49] Elapsed 0m 3s (remain 2m 45s) Loss: 0.0920(0.0920)
EVAL: [48/49] Elapsed 1m 43s (remain 0m 0s) Loss: 0.1145(0.1059)
[2022-10-31 14:20:46] - Epoch 2 - avg_train_loss: 0.1065  avg_val_loss: 0.1059  time: 583s
[2022-10-31 14:20:46] - Epoch 2 - Score: 0.4612  Scores: [0.49808333695616946, 0.4587881109072182, 0.41729960002276134, 0.4695384085268358, 0.47762264494358153, 0.4458871822727345]
[2022-10-31 14:20:46] - Epoch 2 - Save Best Score: 0.4612 Model
Epoch: [3][0/391] Elapsed 0m 2s (remain 17m 26s) Loss: 0.2144(0.2144) Grad: inf  LR: 0.00000156
Epoch: [3][100/391] Elapsed 2m 15s (remain 6m 30s) Loss: 0.0541(0.1035) Grad: 31164.6855  LR: 0.00000156
Epoch: [3][200/391] Elapsed 4m 6s (remain 3m 53s) Loss: 0.1038(0.1028) Grad: 96253.4219  LR: 0.00000156
Epoch: [3][300/391] Elapsed 6m 14s (remain 1m 52s) Loss: 0.1051(0.1026) Grad: 128209.8906  LR: 0.00000156
Epoch: [3][390/391] Elapsed 7m 56s (remain 0m 0s) Loss: 0.1360(0.1011) Grad: 34958.5000  LR: 0.00000099
EVAL: [0/49] Elapsed 0m 3s (remain 2m 44s) Loss: 0.0943(0.0943)
EVAL: [48/49] Elapsed 1m 42s (remain 0m 0s) Loss: 0.1064(0.1043)
[2022-10-31 14:30:30] - Epoch 3 - avg_train_loss: 0.1011  avg_val_loss: 0.1043  time: 580s
[2022-10-31 14:30:30] - Epoch 3 - Score: 0.4574  Scores: [0.49603449368757324, 0.44978393144044515, 0.4134257487520128, 0.4715326478300229, 0.4679127662901466, 0.44569948133158777]
[2022-10-31 14:30:30] - Epoch 3 - Save Best Score: 0.4574 Model
Epoch: [4][0/391] Elapsed 0m 1s (remain 12m 57s) Loss: 0.0507(0.0507) Grad: 334078.2500  LR: 0.00000105
Epoch: [4][100/391] Elapsed 2m 2s (remain 5m 52s) Loss: 0.0682(0.1012) Grad: 61996.8438  LR: 0.00000105
Epoch: [4][200/391] Elapsed 4m 7s (remain 3m 54s) Loss: 0.0658(0.0978) Grad: 50209.8906  LR: 0.00000105
Epoch: [4][300/391] Elapsed 6m 1s (remain 1m 48s) Loss: 0.0840(0.0981) Grad: 90337.8359  LR: 0.00000105
Epoch: [4][390/391] Elapsed 7m 54s (remain 0m 0s) Loss: 0.1211(0.0982) Grad: 56177.7734  LR: 0.00000050
EVAL: [0/49] Elapsed 0m 3s (remain 2m 45s) Loss: 0.0946(0.0946)
EVAL: [48/49] Elapsed 1m 43s (remain 0m 0s) Loss: 0.1062(0.1036)
[2022-10-31 14:40:11] - Epoch 4 - avg_train_loss: 0.0982  avg_val_loss: 0.1036  time: 578s
[2022-10-31 14:40:11] - Epoch 4 - Score: 0.4561  Scores: [0.49485593040601716, 0.44825527028763285, 0.4144870513735253, 0.46745579102290485, 0.4699907511259118, 0.441380621651278]
[2022-10-31 14:40:11] - Epoch 4 - Save Best Score: 0.4561 Model
Epoch: [5][0/391] Elapsed 0m 1s (remain 11m 17s) Loss: 0.0737(0.0737) Grad: 282294.4375  LR: 0.00000055
Epoch: [5][100/391] Elapsed 2m 13s (remain 6m 24s) Loss: 0.0578(0.0987) Grad: 81471.7344  LR: 0.00000055
Epoch: [5][200/391] Elapsed 4m 11s (remain 3m 57s) Loss: 0.0626(0.0968) Grad: 172059.4531  LR: 0.00000055
Epoch: [5][300/391] Elapsed 6m 10s (remain 1m 50s) Loss: 0.0834(0.0957) Grad: 104778.2266  LR: 0.00000055
Epoch: [5][390/391] Elapsed 8m 3s (remain 0m 0s) Loss: 0.1904(0.0957) Grad: 106939.9219  LR: 0.00000017
EVAL: [0/49] Elapsed 0m 3s (remain 2m 45s) Loss: 0.0978(0.0978)
EVAL: [48/49] Elapsed 1m 42s (remain 0m 0s) Loss: 0.1051(0.1036)
[2022-10-31 14:50:02] - Epoch 5 - avg_train_loss: 0.0957  avg_val_loss: 0.1036  time: 587s
[2022-10-31 14:50:02] - Epoch 5 - Score: 0.4562  Scores: [0.4916522720380875, 0.4498696183295962, 0.4151792554443571, 0.4675146133193608, 0.47153344950801945, 0.4415932046703451]
[2022-10-31 14:50:04] - ========== fold: 1 result ==========
[2022-10-31 14:50:04] - Score: 0.4561  Scores: [0.49485593040601716, 0.44825527028763285, 0.4144870513735253, 0.46745579102290485, 0.4699907511259118, 0.441380621651278]
[2022-10-31 14:50:04] - ========== fold: 2 training ==========
[2022-10-31 14:50:04] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v2-xlarge",
  "attention_dropout": 0.0,
  "attention_head_size": 64,
  "attention_probs_dropout_prob": 0.0,
  "conv_act": "gelu",
  "conv_kernel_size": 3,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 24,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1536,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/391] Elapsed 0m 0s (remain 5m 5s) Loss: 2.8217(2.8217) Grad: inf  LR: 0.00000200
Epoch: [1][100/391] Elapsed 1m 56s (remain 5m 35s) Loss: 0.1698(1.1545) Grad: 96008.7812  LR: 0.00000200
Epoch: [1][200/391] Elapsed 3m 56s (remain 3m 43s) Loss: 0.0967(0.6422) Grad: 42486.1523  LR: 0.00000200
Epoch: [1][300/391] Elapsed 5m 56s (remain 1m 46s) Loss: 0.1282(0.4662) Grad: 66242.2969  LR: 0.00000200
Epoch: [1][390/391] Elapsed 7m 53s (remain 0m 0s) Loss: 0.1669(0.3853) Grad: 95026.0000  LR: 0.00000189
EVAL: [0/49] Elapsed 0m 2s (remain 1m 38s) Loss: 0.1345(0.1345)
EVAL: [48/49] Elapsed 1m 48s (remain 0m 0s) Loss: 0.1280(0.1531)
[2022-10-31 14:59:53] - Epoch 1 - avg_train_loss: 0.3853  avg_val_loss: 0.1531  time: 582s
[2022-10-31 14:59:53] - Epoch 1 - Score: 0.5546  Scores: [0.5883826290967914, 0.5382507094363308, 0.4845697884324259, 0.7136459138699232, 0.5011814468564161, 0.5013796196123053]
[2022-10-31 14:59:53] - Epoch 1 - Save Best Score: 0.5546 Model
Epoch: [2][0/391] Elapsed 0m 0s (remain 6m 29s) Loss: 0.1250(0.1250) Grad: inf  LR: 0.00000191
Epoch: [2][100/391] Elapsed 2m 8s (remain 6m 9s) Loss: 0.1517(0.1106) Grad: 271462.9688  LR: 0.00000191
Epoch: [2][200/391] Elapsed 4m 0s (remain 3m 47s) Loss: 0.1056(0.1048) Grad: 138052.0469  LR: 0.00000191
Epoch: [2][300/391] Elapsed 5m 58s (remain 1m 47s) Loss: 0.0542(0.1056) Grad: 70768.4219  LR: 0.00000191
Epoch: [2][390/391] Elapsed 7m 47s (remain 0m 0s) Loss: 0.0996(0.1053) Grad: 140642.4688  LR: 0.00000151
EVAL: [0/49] Elapsed 0m 2s (remain 1m 37s) Loss: 0.0735(0.0735)
EVAL: [48/49] Elapsed 1m 47s (remain 0m 0s) Loss: 0.0753(0.1079)
[2022-10-31 15:09:32] - Epoch 2 - avg_train_loss: 0.1053  avg_val_loss: 0.1079  time: 575s
[2022-10-31 15:09:32] - Epoch 2 - Score: 0.4658  Scores: [0.5080903330872211, 0.46251539220693033, 0.43717123102107885, 0.4456342869655807, 0.4779215398548347, 0.46332816637689855]
[2022-10-31 15:09:32] - Epoch 2 - Save Best Score: 0.4658 Model
Epoch: [3][0/391] Elapsed 0m 1s (remain 9m 58s) Loss: 0.0843(0.0843) Grad: 341993.8750  LR: 0.00000156
Epoch: [3][100/391] Elapsed 2m 1s (remain 5m 48s) Loss: 0.1141(0.1004) Grad: 99567.6797  LR: 0.00000156
Epoch: [3][200/391] Elapsed 4m 1s (remain 3m 48s) Loss: 0.0912(0.0998) Grad: 131276.6875  LR: 0.00000156
Epoch: [3][300/391] Elapsed 6m 4s (remain 1m 49s) Loss: 0.0752(0.0999) Grad: 92332.4922  LR: 0.00000156
Epoch: [3][390/391] Elapsed 7m 46s (remain 0m 0s) Loss: 0.1215(0.1003) Grad: 52402.2656  LR: 0.00000099
EVAL: [0/49] Elapsed 0m 2s (remain 1m 36s) Loss: 0.0775(0.0775)
EVAL: [48/49] Elapsed 1m 47s (remain 0m 0s) Loss: 0.0819(0.1082)
[2022-10-31 15:19:10] - Epoch 3 - avg_train_loss: 0.1003  avg_val_loss: 0.1082  time: 574s
[2022-10-31 15:19:10] - Epoch 3 - Score: 0.4665  Scores: [0.5040955638668253, 0.4751131852782796, 0.425664348937128, 0.4458954398318411, 0.48837646542653124, 0.45967746894594536]
Epoch: [4][0/391] Elapsed 0m 1s (remain 7m 20s) Loss: 0.0619(0.0619) Grad: inf  LR: 0.00000105
Epoch: [4][100/391] Elapsed 2m 6s (remain 6m 3s) Loss: 0.0951(0.0955) Grad: 256850.7969  LR: 0.00000105
Epoch: [4][200/391] Elapsed 4m 5s (remain 3m 52s) Loss: 0.0827(0.0985) Grad: 238089.8438  LR: 0.00000105
Epoch: [4][300/391] Elapsed 6m 11s (remain 1m 51s) Loss: 0.1022(0.0976) Grad: 205852.5781  LR: 0.00000105
Epoch: [4][390/391] Elapsed 7m 56s (remain 0m 0s) Loss: 0.0837(0.0972) Grad: 250749.7031  LR: 0.00000050
EVAL: [0/49] Elapsed 0m 2s (remain 1m 38s) Loss: 0.0723(0.0723)
EVAL: [48/49] Elapsed 1m 47s (remain 0m 0s) Loss: 0.0740(0.1044)
[2022-10-31 15:28:55] - Epoch 4 - avg_train_loss: 0.0972  avg_val_loss: 0.1044  time: 585s
[2022-10-31 15:28:55] - Epoch 4 - Score: 0.4578  Scores: [0.49688966701882814, 0.45791585173387217, 0.41774068794513486, 0.4446007659315519, 0.4731291360195143, 0.45624301230667236]
[2022-10-31 15:28:55] - Epoch 4 - Save Best Score: 0.4578 Model
Epoch: [5][0/391] Elapsed 0m 1s (remain 12m 37s) Loss: 0.0865(0.0865) Grad: 323705.5312  LR: 0.00000055
Epoch: [5][100/391] Elapsed 2m 4s (remain 5m 56s) Loss: 0.1226(0.0960) Grad: 64694.1289  LR: 0.00000055
Epoch: [5][200/391] Elapsed 4m 11s (remain 3m 57s) Loss: 0.1243(0.0983) Grad: 31390.4824  LR: 0.00000055
Epoch: [5][300/391] Elapsed 6m 5s (remain 1m 49s) Loss: 0.0773(0.0991) Grad: 22941.1582  LR: 0.00000055
Epoch: [5][390/391] Elapsed 7m 53s (remain 0m 0s) Loss: 0.0913(0.0994) Grad: 34768.7578  LR: 0.00000017
EVAL: [0/49] Elapsed 0m 2s (remain 1m 36s) Loss: 0.0732(0.0732)
EVAL: [48/49] Elapsed 1m 47s (remain 0m 0s) Loss: 0.0725(0.1069)
[2022-10-31 15:38:40] - Epoch 5 - avg_train_loss: 0.0994  avg_val_loss: 0.1069  time: 581s
[2022-10-31 15:38:40] - Epoch 5 - Score: 0.4634  Scores: [0.49964638449750814, 0.46600786867768124, 0.4227887645247117, 0.45803916638685976, 0.47576355494507, 0.4584483180278805]
[2022-10-31 15:38:42] - ========== fold: 2 result ==========
[2022-10-31 15:38:42] - Score: 0.4578  Scores: [0.49688966701882814, 0.45791585173387217, 0.41774068794513486, 0.4446007659315519, 0.4731291360195143, 0.45624301230667236]
[2022-10-31 15:38:42] - ========== fold: 3 training ==========
[2022-10-31 15:38:42] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v2-xlarge",
  "attention_dropout": 0.0,
  "attention_head_size": 64,
  "attention_probs_dropout_prob": 0.0,
  "conv_act": "gelu",
  "conv_kernel_size": 3,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 24,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1536,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/391] Elapsed 0m 1s (remain 10m 13s) Loss: 2.4024(2.4024) Grad: inf  LR: 0.00000200
Epoch: [1][100/391] Elapsed 2m 9s (remain 6m 10s) Loss: 0.2659(1.0099) Grad: 141348.8281  LR: 0.00000200
Epoch: [1][200/391] Elapsed 4m 6s (remain 3m 52s) Loss: 0.1375(0.5727) Grad: 71578.1328  LR: 0.00000200
Epoch: [1][300/391] Elapsed 6m 3s (remain 1m 48s) Loss: 0.1104(0.4225) Grad: 50590.2305  LR: 0.00000200
Epoch: [1][390/391] Elapsed 7m 53s (remain 0m 0s) Loss: 0.1027(0.3518) Grad: 19112.7363  LR: 0.00000189
EVAL: [0/49] Elapsed 0m 3s (remain 2m 40s) Loss: 0.1292(0.1292)
EVAL: [48/49] Elapsed 1m 49s (remain 0m 0s) Loss: 0.0847(0.1177)
[2022-10-31 15:48:30] - Epoch 1 - avg_train_loss: 0.3518  avg_val_loss: 0.1177  time: 583s
[2022-10-31 15:48:30] - Epoch 1 - Score: 0.4868  Scores: [0.5344249104468095, 0.4815872811026637, 0.4481131962445563, 0.5031279660302651, 0.4991334852249466, 0.4542311904577595]
[2022-10-31 15:48:30] - Epoch 1 - Save Best Score: 0.4868 Model
Epoch: [2][0/391] Elapsed 0m 1s (remain 8m 40s) Loss: 0.0696(0.0696) Grad: 232692.7969  LR: 0.00000191
Epoch: [2][100/391] Elapsed 2m 4s (remain 5m 57s) Loss: 0.1076(0.1066) Grad: 225108.5312  LR: 0.00000191
Epoch: [2][200/391] Elapsed 4m 3s (remain 3m 50s) Loss: 0.0947(0.1040) Grad: 64475.8828  LR: 0.00000191
Epoch: [2][300/391] Elapsed 5m 59s (remain 1m 47s) Loss: 0.1252(0.1053) Grad: 61940.4648  LR: 0.00000191
Epoch: [2][390/391] Elapsed 7m 51s (remain 0m 0s) Loss: 0.0913(0.1055) Grad: 46738.5469  LR: 0.00000151
EVAL: [0/49] Elapsed 0m 3s (remain 2m 42s) Loss: 0.1296(0.1296)
EVAL: [48/49] Elapsed 1m 49s (remain 0m 0s) Loss: 0.0695(0.1108)
[2022-10-31 15:58:15] - Epoch 2 - avg_train_loss: 0.1055  avg_val_loss: 0.1108  time: 581s
[2022-10-31 15:58:15] - Epoch 2 - Score: 0.4713  Scores: [0.5103264841327765, 0.4582942298466061, 0.4386477528317953, 0.46800422494183935, 0.5007984500868305, 0.4515492730337269]
[2022-10-31 15:58:15] - Epoch 2 - Save Best Score: 0.4713 Model
Epoch: [3][0/391] Elapsed 0m 1s (remain 9m 20s) Loss: 0.0903(0.0903) Grad: 180525.1562  LR: 0.00000156
Epoch: [3][100/391] Elapsed 2m 9s (remain 6m 12s) Loss: 0.0874(0.0964) Grad: 124514.8438  LR: 0.00000156
Epoch: [3][200/391] Elapsed 4m 9s (remain 3m 56s) Loss: 0.1193(0.0979) Grad: 47896.6016  LR: 0.00000156
Epoch: [3][300/391] Elapsed 6m 9s (remain 1m 50s) Loss: 0.1103(0.1006) Grad: 26865.2871  LR: 0.00000156
Epoch: [3][390/391] Elapsed 7m 53s (remain 0m 0s) Loss: 0.0761(0.1019) Grad: 18382.5879  LR: 0.00000099
EVAL: [0/49] Elapsed 0m 3s (remain 2m 41s) Loss: 0.1353(0.1353)
EVAL: [48/49] Elapsed 1m 49s (remain 0m 0s) Loss: 0.0743(0.1116)
[2022-10-31 16:08:02] - Epoch 3 - avg_train_loss: 0.1019  avg_val_loss: 0.1116  time: 583s
[2022-10-31 16:08:02] - Epoch 3 - Score: 0.4732  Scores: [0.5125220805218603, 0.4554187977187223, 0.4350234682264842, 0.4690371804369705, 0.4999162808516664, 0.4674286636408805]
Epoch: [4][0/391] Elapsed 0m 1s (remain 7m 33s) Loss: 0.0825(0.0825) Grad: 314713.9062  LR: 0.00000105
Epoch: [4][100/391] Elapsed 2m 4s (remain 5m 56s) Loss: 0.0834(0.0942) Grad: 95111.3828  LR: 0.00000105
Epoch: [4][200/391] Elapsed 4m 9s (remain 3m 55s) Loss: 0.1019(0.0930) Grad: 183535.2031  LR: 0.00000105
Epoch: [4][300/391] Elapsed 6m 12s (remain 1m 51s) Loss: 0.1147(0.0929) Grad: 219883.3750  LR: 0.00000105
Epoch: [4][390/391] Elapsed 8m 0s (remain 0m 0s) Loss: 0.0961(0.0934) Grad: 151296.7344  LR: 0.00000050
EVAL: [0/49] Elapsed 0m 3s (remain 2m 41s) Loss: 0.1182(0.1182)
EVAL: [48/49] Elapsed 1m 48s (remain 0m 0s) Loss: 0.0661(0.1067)
[2022-10-31 16:17:51] - Epoch 4 - avg_train_loss: 0.0934  avg_val_loss: 0.1067  time: 589s
[2022-10-31 16:17:51] - Epoch 4 - Score: 0.4625  Scores: [0.4862723294623886, 0.45433443683584335, 0.4282301312871245, 0.4666106435125172, 0.4965808206964192, 0.44322199512650423]
[2022-10-31 16:17:51] - Epoch 4 - Save Best Score: 0.4625 Model
Epoch: [5][0/391] Elapsed 0m 2s (remain 15m 59s) Loss: 0.0844(0.0844) Grad: 385519.8438  LR: 0.00000055
Epoch: [5][100/391] Elapsed 2m 1s (remain 5m 49s) Loss: 0.1292(0.0904) Grad: 194595.0781  LR: 0.00000055
Epoch: [5][200/391] Elapsed 4m 1s (remain 3m 48s) Loss: 0.0585(0.0901) Grad: 80554.7266  LR: 0.00000055
Epoch: [5][300/391] Elapsed 6m 3s (remain 1m 48s) Loss: 0.0629(0.0907) Grad: 145643.1875  LR: 0.00000055
Epoch: [5][390/391] Elapsed 7m 47s (remain 0m 0s) Loss: 0.0719(0.0905) Grad: 129490.8906  LR: 0.00000017
EVAL: [0/49] Elapsed 0m 3s (remain 2m 42s) Loss: 0.1231(0.1231)
EVAL: [48/49] Elapsed 1m 48s (remain 0m 0s) Loss: 0.0656(0.1073)
[2022-10-31 16:27:32] - Epoch 5 - avg_train_loss: 0.0905  avg_val_loss: 0.1073  time: 577s
[2022-10-31 16:27:32] - Epoch 5 - Score: 0.4640  Scores: [0.4856838533024202, 0.45364101481221136, 0.4278535973175758, 0.47177309949325424, 0.49921543246757155, 0.4456744459722681]
[2022-10-31 16:27:34] - ========== fold: 3 result ==========
[2022-10-31 16:27:34] - Score: 0.4625  Scores: [0.4862723294623886, 0.45433443683584335, 0.4282301312871245, 0.4666106435125172, 0.4965808206964192, 0.44322199512650423]
[2022-10-31 16:27:34] - ========== fold: 4 training ==========
[2022-10-31 16:27:34] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v2-xlarge",
  "attention_dropout": 0.0,
  "attention_head_size": 64,
  "attention_probs_dropout_prob": 0.0,
  "conv_act": "gelu",
  "conv_kernel_size": 3,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 24,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1536,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/391] Elapsed 0m 1s (remain 9m 39s) Loss: 2.9450(2.9450) Grad: 1047376.0000  LR: 0.00000200
Epoch: [1][100/391] Elapsed 2m 0s (remain 5m 47s) Loss: 0.1730(0.7016) Grad: 43269.7773  LR: 0.00000200
Epoch: [1][200/391] Elapsed 4m 3s (remain 3m 50s) Loss: 0.1227(0.4138) Grad: 56251.6328  LR: 0.00000200
Epoch: [1][300/391] Elapsed 5m 58s (remain 1m 47s) Loss: 0.1013(0.3160) Grad: 58593.2031  LR: 0.00000200
Epoch: [1][390/391] Elapsed 7m 55s (remain 0m 0s) Loss: 0.2292(0.2704) Grad: 113134.3281  LR: 0.00000189
EVAL: [0/49] Elapsed 0m 3s (remain 3m 4s) Loss: 0.1322(0.1322)
EVAL: [48/49] Elapsed 1m 45s (remain 0m 0s) Loss: 0.1009(0.1585)
[2022-10-31 16:37:21] - Epoch 1 - avg_train_loss: 0.2704  avg_val_loss: 0.1585  time: 581s
[2022-10-31 16:37:21] - Epoch 1 - Score: 0.5675  Scores: [0.582406949408215, 0.5448387588952447, 0.4991330859038518, 0.6086352982879031, 0.6155951940541439, 0.5542110889470275]
[2022-10-31 16:37:21] - Epoch 1 - Save Best Score: 0.5675 Model
Epoch: [2][0/391] Elapsed 0m 1s (remain 8m 23s) Loss: 0.2037(0.2037) Grad: inf  LR: 0.00000191
Epoch: [2][100/391] Elapsed 2m 4s (remain 5m 56s) Loss: 0.1145(0.1058) Grad: 88729.4219  LR: 0.00000191
Epoch: [2][200/391] Elapsed 4m 7s (remain 3m 53s) Loss: 0.1014(0.1064) Grad: 40740.8008  LR: 0.00000191
Epoch: [2][300/391] Elapsed 6m 5s (remain 1m 49s) Loss: 0.0903(0.1066) Grad: 18006.0898  LR: 0.00000191
Epoch: [2][390/391] Elapsed 7m 53s (remain 0m 0s) Loss: 0.0906(0.1071) Grad: 23041.9609  LR: 0.00000151
EVAL: [0/49] Elapsed 0m 3s (remain 3m 3s) Loss: 0.0990(0.0990)
EVAL: [48/49] Elapsed 1m 45s (remain 0m 0s) Loss: 0.0840(0.1066)
[2022-10-31 16:47:05] - Epoch 2 - avg_train_loss: 0.1071  avg_val_loss: 0.1066  time: 580s
[2022-10-31 16:47:05] - Epoch 2 - Score: 0.4633  Scores: [0.4994940589402836, 0.4475562774170978, 0.428146361082589, 0.46421164978024737, 0.487464888769646, 0.45310290617150273]
[2022-10-31 16:47:05] - Epoch 2 - Save Best Score: 0.4633 Model
Epoch: [3][0/391] Elapsed 0m 1s (remain 9m 5s) Loss: 0.0621(0.0621) Grad: 268037.6875  LR: 0.00000156
Epoch: [3][100/391] Elapsed 2m 4s (remain 5m 57s) Loss: 0.0829(0.1055) Grad: 88891.8828  LR: 0.00000156
Epoch: [3][200/391] Elapsed 4m 9s (remain 3m 56s) Loss: 0.1042(0.1026) Grad: 24161.9531  LR: 0.00000156
Epoch: [3][300/391] Elapsed 6m 13s (remain 1m 51s) Loss: 0.0847(0.1021) Grad: 51971.7578  LR: 0.00000156
Epoch: [3][390/391] Elapsed 8m 4s (remain 0m 0s) Loss: 0.0716(0.1015) Grad: 12866.7070  LR: 0.00000099
EVAL: [0/49] Elapsed 0m 3s (remain 3m 3s) Loss: 0.1272(0.1272)
EVAL: [48/49] Elapsed 1m 45s (remain 0m 0s) Loss: 0.1092(0.1173)
[2022-10-31 16:57:00] - Epoch 3 - avg_train_loss: 0.1015  avg_val_loss: 0.1173  time: 591s
[2022-10-31 16:57:00] - Epoch 3 - Score: 0.4858  Scores: [0.4980605474581748, 0.479092557040779, 0.422773099812469, 0.46962961094239347, 0.5607746069154892, 0.48420891759469514]
Epoch: [4][0/391] Elapsed 0m 2s (remain 15m 10s) Loss: 0.1510(0.1510) Grad: inf  LR: 0.00000105
Epoch: [4][100/391] Elapsed 1m 58s (remain 5m 40s) Loss: 0.0825(0.0967) Grad: 108415.8594  LR: 0.00000105
Epoch: [4][200/391] Elapsed 3m 59s (remain 3m 46s) Loss: 0.1027(0.0964) Grad: 104822.8438  LR: 0.00000105
Epoch: [4][300/391] Elapsed 6m 5s (remain 1m 49s) Loss: 0.0524(0.0966) Grad: 23801.4355  LR: 0.00000105
Epoch: [4][390/391] Elapsed 7m 58s (remain 0m 0s) Loss: 0.0760(0.0957) Grad: 26905.0430  LR: 0.00000050
EVAL: [0/49] Elapsed 0m 3s (remain 3m 4s) Loss: 0.1064(0.1064)
EVAL: [48/49] Elapsed 1m 45s (remain 0m 0s) Loss: 0.0820(0.1052)
[2022-10-31 17:06:44] - Epoch 4 - avg_train_loss: 0.0957  avg_val_loss: 0.1052  time: 584s
[2022-10-31 17:06:44] - Epoch 4 - Score: 0.4599  Scores: [0.48995715368165554, 0.44226690604316676, 0.42141723579070434, 0.4597595509897353, 0.47795276629930405, 0.46824302183458827]
[2022-10-31 17:06:44] - Epoch 4 - Save Best Score: 0.4599 Model
Epoch: [5][0/391] Elapsed 0m 1s (remain 9m 52s) Loss: 0.1003(0.1003) Grad: 189449.8750  LR: 0.00000055
Epoch: [5][100/391] Elapsed 1m 58s (remain 5m 41s) Loss: 0.1061(0.0905) Grad: 293007.8750  LR: 0.00000055
Epoch: [5][200/391] Elapsed 3m 58s (remain 3m 45s) Loss: 0.0636(0.0901) Grad: 52574.5273  LR: 0.00000055
Epoch: [5][300/391] Elapsed 6m 9s (remain 1m 50s) Loss: 0.0960(0.0915) Grad: 72464.2266  LR: 0.00000055
Epoch: [5][390/391] Elapsed 7m 55s (remain 0m 0s) Loss: 0.0790(0.0910) Grad: 44466.0703  LR: 0.00000017
EVAL: [0/49] Elapsed 0m 3s (remain 3m 2s) Loss: 0.1042(0.1042)
EVAL: [48/49] Elapsed 1m 45s (remain 0m 0s) Loss: 0.0895(0.1046)
[2022-10-31 17:16:30] - Epoch 5 - avg_train_loss: 0.0910  avg_val_loss: 0.1046  time: 581s
[2022-10-31 17:16:30] - Epoch 5 - Score: 0.4587  Scores: [0.4951693426643812, 0.4438649098439245, 0.4194853658545998, 0.460024474931831, 0.4762067089872124, 0.4574730217428544]
[2022-10-31 17:16:30] - Epoch 5 - Save Best Score: 0.4587 Model
[2022-10-31 17:16:36] - ========== fold: 4 result ==========
[2022-10-31 17:16:36] - Score: 0.4587  Scores: [0.4951693426643812, 0.4438649098439245, 0.4194853658545998, 0.460024474931831, 0.4762067089872124, 0.4574730217428544]
[2022-10-31 17:16:36] - ========== CV ==========
[2022-10-31 17:16:36] - Score: 0.4583  Scores: [0.49244239573654575, 0.45379677560256393, 0.4189655822518894, 0.45826155110362193, 0.4768140286304161, 0.4496467929460188]