Downloading tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 52.0/52.0 [00:00<00:00, 90.3kB/s]
Downloading config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 580/580 [00:00<00:00, 1.02MB/s]
Downloading spm.model: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.35M/2.35M [00:00<00:00, 64.2MB/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3911/3911 [00:04<00:00, 920.65it/s]
[2022-10-29 15:56:03] - max_len: 2048
[2022-10-29 15:56:03] - ========== fold: 0 training ==========
[2022-10-29 15:56:03] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-large",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1024,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}





Downloading pytorch_model.bin: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 833M/833M [00:10<00:00, 82.9MB/s]
Reinitializing Last 1 Layers ...
Done.!
Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/195] Elapsed 0m 3s (remain 10m 48s) Loss: 2.3341(2.3341) Grad: inf  LR: 0.00000100
Epoch: [1][100/195] Elapsed 3m 14s (remain 3m 1s) Loss: 0.4182(1.6198) Grad: 160954.3906  LR: 0.00000100
Epoch: [1][194/195] Elapsed 6m 5s (remain 0m 0s) Loss: 0.1057(0.9316) Grad: 81156.4375  LR: 0.00000099
EVAL: [0/25] Elapsed 0m 2s (remain 0m 52s) Loss: 0.1409(0.1409)
[2022-10-29 16:03:31] - Epoch 1 - avg_train_loss: 0.9316  avg_val_loss: 0.1415  time: 432s
[2022-10-29 16:03:31] - Epoch 1 - Score: 0.5346  Scores: [0.5607999910937556, 0.5156372357940023, 0.4719770236346374, 0.5038112172179353, 0.5289473713248437, 0.6262820357293148]
[2022-10-29 16:03:31] - Epoch 1 - Save Best Score: 0.5346 Model
EVAL: [24/25] Elapsed 1m 6s (remain 0m 0s) Loss: 0.1394(0.1415)
Epoch: [2][0/195] Elapsed 0m 1s (remain 5m 48s) Loss: 0.1450(0.1450) Grad: 531206.8125  LR: 0.00000099
Epoch: [2][100/195] Elapsed 3m 16s (remain 3m 2s) Loss: 0.1211(0.1383) Grad: 119534.4922  LR: 0.00000099
Epoch: [2][194/195] Elapsed 6m 6s (remain 0m 0s) Loss: 0.1076(0.1321) Grad: 280758.7500  LR: 0.00000093
EVAL: [0/25] Elapsed 0m 2s (remain 0m 51s) Loss: 0.1226(0.1226)
EVAL: [24/25] Elapsed 1m 6s (remain 0m 0s) Loss: 0.1141(0.1199)
[2022-10-29 16:10:46] - Epoch 2 - avg_train_loss: 0.1321  avg_val_loss: 0.1199  time: 433s
[2022-10-29 16:10:46] - Epoch 2 - Score: 0.4910  Scores: [0.5259552870286744, 0.47825800958669346, 0.4349192699245785, 0.48069309795308623, 0.4947157158956681, 0.531367813705985]
[2022-10-29 16:10:46] - Epoch 2 - Save Best Score: 0.4910 Model
Epoch: [3][0/195] Elapsed 0m 1s (remain 4m 57s) Loss: 0.1371(0.1371) Grad: 270678.0625  LR: 0.00000095
Epoch: [3][100/195] Elapsed 3m 4s (remain 2m 51s) Loss: 0.1604(0.1196) Grad: 764708.3125  LR: 0.00000095
Epoch: [3][194/195] Elapsed 5m 57s (remain 0m 0s) Loss: 0.1251(0.1187) Grad: 372904.0938  LR: 0.00000084
EVAL: [0/25] Elapsed 0m 2s (remain 0m 52s) Loss: 0.1197(0.1197)
EVAL: [24/25] Elapsed 1m 6s (remain 0m 0s) Loss: 0.1063(0.1137)
[2022-10-29 16:17:53] - Epoch 3 - avg_train_loss: 0.1187  avg_val_loss: 0.1137  time: 424s
[2022-10-29 16:17:53] - Epoch 3 - Score: 0.4777  Scores: [0.511547326902641, 0.47067007178582004, 0.42475573648880094, 0.4678491305188506, 0.4901066729672752, 0.501413519597141]
[2022-10-29 16:17:53] - Epoch 3 - Save Best Score: 0.4777 Model
Epoch: [4][0/195] Elapsed 0m 1s (remain 3m 51s) Loss: 0.1309(0.1309) Grad: 232017.2031  LR: 0.00000086
Epoch: [4][100/195] Elapsed 3m 18s (remain 3m 4s) Loss: 0.0902(0.1125) Grad: 353857.1250  LR: 0.00000086
Epoch: [4][194/195] Elapsed 6m 4s (remain 0m 0s) Loss: 0.1208(0.1133) Grad: 182193.0000  LR: 0.00000072
EVAL: [0/25] Elapsed 0m 2s (remain 0m 51s) Loss: 0.1147(0.1147)
EVAL: [24/25] Elapsed 1m 6s (remain 0m 0s) Loss: 0.1086(0.1107)
[2022-10-29 16:25:06] - Epoch 4 - avg_train_loss: 0.1133  avg_val_loss: 0.1107  time: 431s
[2022-10-29 16:25:06] - Epoch 4 - Score: 0.4713  Scores: [0.501715914135139, 0.4695649006662326, 0.4239777347103247, 0.4637928337669682, 0.4784873851393565, 0.49019664866511553]
[2022-10-29 16:25:06] - Epoch 4 - Save Best Score: 0.4713 Model
Epoch: [5][0/195] Elapsed 0m 1s (remain 5m 17s) Loss: 0.0993(0.0993) Grad: 409731.5000  LR: 0.00000074
Epoch: [5][100/195] Elapsed 3m 8s (remain 2m 55s) Loss: 0.0991(0.1146) Grad: 637867.3750  LR: 0.00000074
Epoch: [5][194/195] Elapsed 6m 12s (remain 0m 0s) Loss: 0.0973(0.1105) Grad: 421557.0625  LR: 0.00000058
EVAL: [0/25] Elapsed 0m 2s (remain 0m 51s) Loss: 0.1130(0.1130)
[2022-10-29 16:32:27] - Epoch 5 - avg_train_loss: 0.1105  avg_val_loss: 0.1085  time: 438s
[2022-10-29 16:32:27] - Epoch 5 - Score: 0.4666  Scores: [0.49722045722992103, 0.468577942802832, 0.4149706509914515, 0.4659477573005688, 0.4767150045639831, 0.47614012937142924]
EVAL: [24/25] Elapsed 1m 6s (remain 0m 0s) Loss: 0.1078(0.1085)
[2022-10-29 16:32:27] - Epoch 5 - Save Best Score: 0.4666 Model
[2022-10-29 16:32:31] - ========== fold: 0 result ==========
[2022-10-29 16:32:31] - Score: 0.4666  Scores: [0.49722045722992103, 0.468577942802832, 0.4149706509914515, 0.4659477573005688, 0.4767150045639831, 0.47614012937142924]
[2022-10-29 16:32:31] - ========== fold: 1 training ==========
[2022-10-29 16:32:31] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-large",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1024,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Reinitializing Last 1 Layers ...
Done.!
Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/195] Elapsed 0m 3s (remain 10m 5s) Loss: 2.7522(2.7522) Grad: inf  LR: 0.00000100
Epoch: [1][100/195] Elapsed 3m 5s (remain 2m 52s) Loss: 0.6397(1.7163) Grad: 210274.5156  LR: 0.00000100
Epoch: [1][194/195] Elapsed 6m 0s (remain 0m 0s) Loss: 0.1606(0.9958) Grad: 62745.1953  LR: 0.00000099
EVAL: [0/25] Elapsed 0m 3s (remain 1m 32s) Loss: 0.1994(0.1994)
EVAL: [24/25] Elapsed 1m 8s (remain 0m 0s) Loss: 0.1628(0.1519)
[2022-10-29 16:39:44] - Epoch 1 - avg_train_loss: 0.9958  avg_val_loss: 0.1519  time: 430s
[2022-10-29 16:39:44] - Epoch 1 - Score: 0.5569  Scores: [0.5765199769451254, 0.551228863937545, 0.5104035383705497, 0.5530376505169675, 0.5859083293279505, 0.5645391724788846]
[2022-10-29 16:39:44] - Epoch 1 - Save Best Score: 0.5569 Model
Epoch: [2][0/195] Elapsed 0m 1s (remain 6m 12s) Loss: 0.1698(0.1698) Grad: 398896.0938  LR: 0.00000099
Epoch: [2][100/195] Elapsed 3m 11s (remain 2m 58s) Loss: 0.1415(0.1398) Grad: 384408.0938  LR: 0.00000099
Epoch: [2][194/195] Elapsed 6m 1s (remain 0m 0s) Loss: 0.1117(0.1300) Grad: 215072.0938  LR: 0.00000093
EVAL: [0/25] Elapsed 0m 3s (remain 1m 32s) Loss: 0.1372(0.1372)
EVAL: [24/25] Elapsed 1m 8s (remain 0m 0s) Loss: 0.1316(0.1210)
[2022-10-29 16:46:56] - Epoch 2 - avg_train_loss: 0.1300  avg_val_loss: 0.1210  time: 430s
[2022-10-29 16:46:56] - Epoch 2 - Score: 0.4938  Scores: [0.5229169283187388, 0.4769547855323015, 0.44338321363557387, 0.5143162601855539, 0.5029971145258793, 0.5022800769221132]
[2022-10-29 16:46:56] - Epoch 2 - Save Best Score: 0.4938 Model
Epoch: [3][0/195] Elapsed 0m 3s (remain 12m 11s) Loss: 0.0910(0.0910) Grad: 275207.3438  LR: 0.00000095
Epoch: [3][100/195] Elapsed 3m 14s (remain 3m 1s) Loss: 0.1159(0.1195) Grad: 524210.4062  LR: 0.00000095
Epoch: [3][194/195] Elapsed 6m 7s (remain 0m 0s) Loss: 0.0971(0.1165) Grad: 305247.4688  LR: 0.00000084
EVAL: [0/25] Elapsed 0m 3s (remain 1m 32s) Loss: 0.1200(0.1200)
EVAL: [24/25] Elapsed 1m 8s (remain 0m 0s) Loss: 0.1206(0.1137)
[2022-10-29 16:54:14] - Epoch 3 - avg_train_loss: 0.1165  avg_val_loss: 0.1137  time: 436s
[2022-10-29 16:54:14] - Epoch 3 - Score: 0.4781  Scores: [0.5070182229672275, 0.4627415514923373, 0.4297683761245173, 0.49573644127340993, 0.49108529223792835, 0.48251613730011006]
[2022-10-29 16:54:14] - Epoch 3 - Save Best Score: 0.4781 Model
Epoch: [4][0/195] Elapsed 0m 1s (remain 4m 57s) Loss: 0.0824(0.0824) Grad: 118918.3438  LR: 0.00000086
Epoch: [4][100/195] Elapsed 3m 0s (remain 2m 47s) Loss: 0.1031(0.1107) Grad: 258041.8906  LR: 0.00000086
Epoch: [4][194/195] Elapsed 5m 58s (remain 0m 0s) Loss: 0.1302(0.1122) Grad: 499560.5625  LR: 0.00000072
EVAL: [0/25] Elapsed 0m 3s (remain 1m 32s) Loss: 0.1173(0.1173)
EVAL: [24/25] Elapsed 1m 8s (remain 0m 0s) Loss: 0.1202(0.1118)
[2022-10-29 17:01:24] - Epoch 4 - avg_train_loss: 0.1122  avg_val_loss: 0.1118  time: 427s
[2022-10-29 17:01:24] - Epoch 4 - Score: 0.4740  Scores: [0.5037091625468257, 0.4610219291230586, 0.42425017863354875, 0.4923983861678529, 0.4896206653358184, 0.47316698883464486]
[2022-10-29 17:01:24] - Epoch 4 - Save Best Score: 0.4740 Model
Epoch: [5][0/195] Elapsed 0m 1s (remain 4m 21s) Loss: 0.0802(0.0802) Grad: 286378.5625  LR: 0.00000074
Epoch: [5][100/195] Elapsed 3m 11s (remain 2m 57s) Loss: 0.0920(0.1105) Grad: 229924.7031  LR: 0.00000074
Epoch: [5][194/195] Elapsed 6m 4s (remain 0m 0s) Loss: 0.0698(0.1081) Grad: 258872.9375  LR: 0.00000058
EVAL: [0/25] Elapsed 0m 3s (remain 1m 32s) Loss: 0.1135(0.1135)
EVAL: [24/25] Elapsed 1m 8s (remain 0m 0s) Loss: 0.1200(0.1114)
[2022-10-29 17:08:39] - Epoch 5 - avg_train_loss: 0.1081  avg_val_loss: 0.1114  time: 433s
[2022-10-29 17:08:39] - Epoch 5 - Score: 0.4730  Scores: [0.5099872419788275, 0.45880256884391585, 0.42240377288550207, 0.4888346528999052, 0.49066690305453287, 0.46705307080719793]
[2022-10-29 17:08:39] - Epoch 5 - Save Best Score: 0.4730 Model
[2022-10-29 17:08:42] - ========== fold: 1 result ==========
[2022-10-29 17:08:42] - Score: 0.4730  Scores: [0.5099872419788275, 0.45880256884391585, 0.42240377288550207, 0.4888346528999052, 0.49066690305453287, 0.46705307080719793]
[2022-10-29 17:08:42] - ========== fold: 2 training ==========
[2022-10-29 17:08:42] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-large",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1024,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Reinitializing Last 1 Layers ...
Done.!
Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/195] Elapsed 0m 1s (remain 5m 46s) Loss: 2.6710(2.6710) Grad: inf  LR: 0.00000100
Epoch: [1][100/195] Elapsed 3m 1s (remain 2m 49s) Loss: 0.6779(1.6923) Grad: 282454.6562  LR: 0.00000100
Epoch: [1][194/195] Elapsed 5m 56s (remain 0m 0s) Loss: 0.1119(0.9758) Grad: 35840.8047  LR: 0.00000099
EVAL: [0/25] Elapsed 0m 3s (remain 1m 19s) Loss: 0.1263(0.1263)
EVAL: [24/25] Elapsed 1m 12s (remain 0m 0s) Loss: 0.0874(0.1473)
[2022-10-29 17:15:55] - Epoch 1 - avg_train_loss: 0.9758  avg_val_loss: 0.1473  time: 430s
[2022-10-29 17:15:55] - Epoch 1 - Score: 0.5471  Scores: [0.57392111923512, 0.5481769907258724, 0.49692924725968757, 0.49322427889289205, 0.5810487229723117, 0.5892575038902804]
[2022-10-29 17:15:55] - Epoch 1 - Save Best Score: 0.5471 Model
Epoch: [2][0/195] Elapsed 0m 1s (remain 5m 52s) Loss: 0.1690(0.1690) Grad: 207243.6562  LR: 0.00000099
Epoch: [2][100/195] Elapsed 3m 5s (remain 2m 52s) Loss: 0.1671(0.1263) Grad: 147374.9531  LR: 0.00000099
Epoch: [2][194/195] Elapsed 5m 57s (remain 0m 0s) Loss: 0.1360(0.1248) Grad: 247883.4375  LR: 0.00000093
EVAL: [0/25] Elapsed 0m 3s (remain 1m 19s) Loss: 0.1064(0.1064)
EVAL: [24/25] Elapsed 1m 12s (remain 0m 0s) Loss: 0.0706(0.1274)
[2022-10-29 17:23:07] - Epoch 2 - avg_train_loss: 0.1248  avg_val_loss: 0.1274  time: 430s
[2022-10-29 17:23:07] - Epoch 2 - Score: 0.5065  Scores: [0.5480743881065878, 0.5043465529097397, 0.4524612318680845, 0.46764444426911267, 0.5270831687934643, 0.5392962272377588]
[2022-10-29 17:23:07] - Epoch 2 - Save Best Score: 0.5065 Model
Epoch: [3][0/195] Elapsed 0m 3s (remain 11m 35s) Loss: 0.0788(0.0788) Grad: 174010.5000  LR: 0.00000095
Epoch: [3][100/195] Elapsed 3m 4s (remain 2m 51s) Loss: 0.1731(0.1171) Grad: 184846.9375  LR: 0.00000095
Epoch: [3][194/195] Elapsed 5m 56s (remain 0m 0s) Loss: 0.1287(0.1141) Grad: 152000.5312  LR: 0.00000084
EVAL: [0/25] Elapsed 0m 3s (remain 1m 20s) Loss: 0.1003(0.1003)
EVAL: [24/25] Elapsed 1m 12s (remain 0m 0s) Loss: 0.0724(0.1189)
[2022-10-29 17:30:18] - Epoch 3 - avg_train_loss: 0.1141  avg_val_loss: 0.1189  time: 429s
[2022-10-29 17:30:18] - Epoch 3 - Score: 0.4886  Scores: [0.5344512355053862, 0.4852548037044897, 0.441008688708021, 0.4539128355894387, 0.5014962708155408, 0.5156207883207187]
[2022-10-29 17:30:18] - Epoch 3 - Save Best Score: 0.4886 Model
Epoch: [4][0/195] Elapsed 0m 2s (remain 9m 37s) Loss: 0.1070(0.1070) Grad: 246225.9219  LR: 0.00000086
Epoch: [4][100/195] Elapsed 3m 7s (remain 2m 54s) Loss: 0.0927(0.1106) Grad: 154716.2656  LR: 0.00000086
Epoch: [4][194/195] Elapsed 5m 59s (remain 0m 0s) Loss: 0.0781(0.1090) Grad: 222234.7500  LR: 0.00000072
EVAL: [0/25] Elapsed 0m 3s (remain 1m 20s) Loss: 0.1054(0.1054)
EVAL: [24/25] Elapsed 1m 12s (remain 0m 0s) Loss: 0.0795(0.1190)
[2022-10-29 17:37:32] - Epoch 4 - avg_train_loss: 0.1090  avg_val_loss: 0.1190  time: 432s
[2022-10-29 17:37:32] - Epoch 4 - Score: 0.4890  Scores: [0.533875697386629, 0.48169422693668335, 0.4393875789541952, 0.4676733669101314, 0.5083177405052381, 0.5029582976560353]
Epoch: [5][0/195] Elapsed 0m 2s (remain 8m 13s) Loss: 0.1596(0.1596) Grad: 592802.0625  LR: 0.00000074
Epoch: [5][100/195] Elapsed 3m 16s (remain 3m 2s) Loss: 0.1147(0.1069) Grad: 143034.4688  LR: 0.00000074
Epoch: [5][194/195] Elapsed 6m 4s (remain 0m 0s) Loss: 0.0981(0.1059) Grad: 312977.5000  LR: 0.00000058
EVAL: [0/25] Elapsed 0m 3s (remain 1m 18s) Loss: 0.0873(0.0873)
EVAL: [24/25] Elapsed 1m 11s (remain 0m 0s) Loss: 0.0727(0.1115)
[2022-10-29 17:44:49] - Epoch 5 - avg_train_loss: 0.1059  avg_val_loss: 0.1115  time: 437s
[2022-10-29 17:44:49] - Epoch 5 - Score: 0.4729  Scores: [0.5160547042622793, 0.4723323219629439, 0.42705310112755507, 0.44530182825792025, 0.4837036567883341, 0.49320435943291624]
[2022-10-29 17:44:49] - Epoch 5 - Save Best Score: 0.4729 Model
[2022-10-29 17:44:52] - ========== fold: 2 result ==========
[2022-10-29 17:44:52] - Score: 0.4729  Scores: [0.5160547042622793, 0.4723323219629439, 0.42705310112755507, 0.44530182825792025, 0.4837036567883341, 0.49320435943291624]
[2022-10-29 17:44:52] - ========== fold: 3 training ==========
[2022-10-29 17:44:52] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-large",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1024,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Reinitializing Last 1 Layers ...
Done.!
Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/195] Elapsed 0m 2s (remain 6m 38s) Loss: 2.4823(2.4823) Grad: inf  LR: 0.00000100
Epoch: [1][100/195] Elapsed 2m 59s (remain 2m 47s) Loss: 0.8392(1.8507) Grad: 230792.6406  LR: 0.00000100
Epoch: [1][194/195] Elapsed 5m 58s (remain 0m 0s) Loss: 0.1359(1.0715) Grad: 46329.2344  LR: 0.00000099
EVAL: [0/25] Elapsed 0m 3s (remain 1m 31s) Loss: 0.1584(0.1584)
EVAL: [24/25] Elapsed 1m 10s (remain 0m 0s) Loss: 0.1054(0.1578)
[2022-10-29 17:52:05] - Epoch 1 - avg_train_loss: 1.0715  avg_val_loss: 0.1578  time: 430s
[2022-10-29 17:52:05] - Epoch 1 - Score: 0.5661  Scores: [0.5710245480420001, 0.5844158663077547, 0.49884826060893467, 0.5596818962721434, 0.5925666379410575, 0.5900324095735179]
[2022-10-29 17:52:05] - Epoch 1 - Save Best Score: 0.5661 Model
Epoch: [2][0/195] Elapsed 0m 2s (remain 7m 5s) Loss: 0.1320(0.1320) Grad: 270262.6875  LR: 0.00000099
Epoch: [2][100/195] Elapsed 3m 8s (remain 2m 55s) Loss: 0.1274(0.1364) Grad: 492936.4375  LR: 0.00000099
Epoch: [2][194/195] Elapsed 6m 2s (remain 0m 0s) Loss: 0.1369(0.1301) Grad: 378685.8750  LR: 0.00000093
EVAL: [0/25] Elapsed 0m 3s (remain 1m 30s) Loss: 0.1240(0.1240)
EVAL: [24/25] Elapsed 1m 10s (remain 0m 0s) Loss: 0.0855(0.1246)
[2022-10-29 17:59:19] - Epoch 2 - avg_train_loss: 0.1301  avg_val_loss: 0.1246  time: 433s
[2022-10-29 17:59:19] - Epoch 2 - Score: 0.5010  Scores: [0.5262222185137332, 0.48697154342394844, 0.46190182943123687, 0.4995124549409201, 0.521868507284405, 0.5094382280059353]
[2022-10-29 17:59:19] - Epoch 2 - Save Best Score: 0.5010 Model
Epoch: [3][0/195] Elapsed 0m 3s (remain 10m 23s) Loss: 0.1510(0.1510) Grad: 394604.9688  LR: 0.00000095
Epoch: [3][100/195] Elapsed 3m 9s (remain 2m 56s) Loss: 0.1118(0.1205) Grad: 316649.5312  LR: 0.00000095
Epoch: [3][194/195] Elapsed 6m 2s (remain 0m 0s) Loss: 0.0919(0.1173) Grad: 319467.7188  LR: 0.00000084
EVAL: [0/25] Elapsed 0m 3s (remain 1m 29s) Loss: 0.1106(0.1106)
EVAL: [24/25] Elapsed 1m 10s (remain 0m 0s) Loss: 0.0820(0.1172)
[2022-10-29 18:06:35] - Epoch 3 - avg_train_loss: 0.1173  avg_val_loss: 0.1172  time: 433s
[2022-10-29 18:06:35] - Epoch 3 - Score: 0.4852  Scores: [0.5200307618201919, 0.4649978208034743, 0.4452914990354379, 0.4782052209218845, 0.516803151664323, 0.48574341021360173]
[2022-10-29 18:06:35] - Epoch 3 - Save Best Score: 0.4852 Model
Epoch: [4][0/195] Elapsed 0m 1s (remain 4m 34s) Loss: 0.1177(0.1177) Grad: 356132.9375  LR: 0.00000086
Epoch: [4][100/195] Elapsed 2m 58s (remain 2m 46s) Loss: 0.1092(0.1142) Grad: 267502.2500  LR: 0.00000086
Epoch: [4][194/195] Elapsed 6m 3s (remain 0m 0s) Loss: 0.0919(0.1117) Grad: 207733.6406  LR: 0.00000072
EVAL: [0/25] Elapsed 0m 3s (remain 1m 29s) Loss: 0.1049(0.1049)
EVAL: [24/25] Elapsed 1m 10s (remain 0m 0s) Loss: 0.0771(0.1133)
[2022-10-29 18:13:50] - Epoch 4 - avg_train_loss: 0.1117  avg_val_loss: 0.1133  time: 433s
[2022-10-29 18:13:50] - Epoch 4 - Score: 0.4766  Scores: [0.5067384336251588, 0.45489607210142896, 0.4384966942496574, 0.47697772632155533, 0.5082294302324236, 0.4742193134093596]
[2022-10-29 18:13:50] - Epoch 4 - Save Best Score: 0.4766 Model
Epoch: [5][0/195] Elapsed 0m 1s (remain 4m 19s) Loss: 0.1060(0.1060) Grad: 366703.6562  LR: 0.00000074
Epoch: [5][100/195] Elapsed 3m 13s (remain 3m 0s) Loss: 0.1060(0.1090) Grad: 197673.0938  LR: 0.00000074
Epoch: [5][194/195] Elapsed 5m 59s (remain 0m 0s) Loss: 0.1317(0.1087) Grad: 289208.5625  LR: 0.00000058
EVAL: [0/25] Elapsed 0m 3s (remain 1m 30s) Loss: 0.1019(0.1019)
EVAL: [24/25] Elapsed 1m 10s (remain 0m 0s) Loss: 0.0715(0.1102)
[2022-10-29 18:21:03] - Epoch 5 - avg_train_loss: 0.1087  avg_val_loss: 0.1102  time: 430s
[2022-10-29 18:21:03] - Epoch 5 - Score: 0.4700  Scores: [0.5010071180651672, 0.45046813121440477, 0.4342500484427623, 0.46538350230348163, 0.5049149561534957, 0.4640078367805731]
[2022-10-29 18:21:03] - Epoch 5 - Save Best Score: 0.4700 Model
[2022-10-29 18:21:06] - ========== fold: 3 result ==========
[2022-10-29 18:21:06] - Score: 0.4700  Scores: [0.5010071180651672, 0.45046813121440477, 0.4342500484427623, 0.46538350230348163, 0.5049149561534957, 0.4640078367805731]
[2022-10-29 18:21:06] - ========== fold: 4 training ==========
[2022-10-29 18:21:06] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-large",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1024,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Reinitializing Last 1 Layers ...
Done.!
Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/195] Elapsed 0m 2s (remain 6m 46s) Loss: 2.4296(2.4296) Grad: inf  LR: 0.00000100
Epoch: [1][100/195] Elapsed 3m 9s (remain 2m 56s) Loss: 0.9021(1.7976) Grad: 235779.8281  LR: 0.00000100
Epoch: [1][194/195] Elapsed 6m 3s (remain 0m 0s) Loss: 0.1939(1.0264) Grad: 136398.7344  LR: 0.00000099
EVAL: [0/25] Elapsed 0m 4s (remain 1m 42s) Loss: 0.1334(0.1334)
EVAL: [24/25] Elapsed 1m 8s (remain 0m 0s) Loss: 0.1164(0.1490)
[2022-10-29 18:28:22] - Epoch 1 - avg_train_loss: 1.0264  avg_val_loss: 0.1490  time: 433s
[2022-10-29 18:28:22] - Epoch 1 - Score: 0.5514  Scores: [0.5626579562598939, 0.5624951913763463, 0.5285085094982721, 0.5220902815277195, 0.5878971730025737, 0.544598969975148]
[2022-10-29 18:28:22] - Epoch 1 - Save Best Score: 0.5514 Model
Epoch: [2][0/195] Elapsed 0m 1s (remain 4m 23s) Loss: 0.1353(0.1353) Grad: 319441.4688  LR: 0.00000099
Epoch: [2][100/195] Elapsed 3m 6s (remain 2m 53s) Loss: 0.1187(0.1359) Grad: 301522.3438  LR: 0.00000099
Epoch: [2][194/195] Elapsed 5m 58s (remain 0m 0s) Loss: 0.1093(0.1309) Grad: 330231.1875  LR: 0.00000093
EVAL: [0/25] Elapsed 0m 4s (remain 1m 43s) Loss: 0.1253(0.1253)
EVAL: [24/25] Elapsed 1m 8s (remain 0m 0s) Loss: 0.0888(0.1217)
[2022-10-29 18:35:31] - Epoch 2 - avg_train_loss: 0.1309  avg_val_loss: 0.1217  time: 427s
[2022-10-29 18:35:31] - Epoch 2 - Score: 0.4958  Scores: [0.5176004998174208, 0.48732404677242924, 0.4709513922739028, 0.4795909309240466, 0.5218413102904561, 0.497359746950711]
[2022-10-29 18:35:31] - Epoch 2 - Save Best Score: 0.4958 Model
Epoch: [3][0/195] Elapsed 0m 1s (remain 5m 53s) Loss: 0.1343(0.1343) Grad: 170837.8438  LR: 0.00000095
Epoch: [3][100/195] Elapsed 3m 12s (remain 2m 59s) Loss: 0.1279(0.1184) Grad: 198499.2031  LR: 0.00000095
Epoch: [3][194/195] Elapsed 6m 1s (remain 0m 0s) Loss: 0.0943(0.1159) Grad: 153623.5938  LR: 0.00000084
EVAL: [0/25] Elapsed 0m 4s (remain 1m 43s) Loss: 0.1224(0.1224)
EVAL: [24/25] Elapsed 1m 8s (remain 0m 0s) Loss: 0.0789(0.1139)
[2022-10-29 18:42:43] - Epoch 3 - avg_train_loss: 0.1159  avg_val_loss: 0.1139  time: 430s
[2022-10-29 18:42:43] - Epoch 3 - Score: 0.4789  Scores: [0.5041713672226776, 0.46022359498194015, 0.44989295785854694, 0.46507837018818227, 0.5120326930731914, 0.48228357165443386]
[2022-10-29 18:42:43] - Epoch 3 - Save Best Score: 0.4789 Model
Epoch: [4][0/195] Elapsed 0m 1s (remain 6m 10s) Loss: 0.1294(0.1294) Grad: 199309.0781  LR: 0.00000086
Epoch: [4][100/195] Elapsed 3m 3s (remain 2m 50s) Loss: 0.0750(0.1109) Grad: 117271.4844  LR: 0.00000086
Epoch: [4][194/195] Elapsed 5m 57s (remain 0m 0s) Loss: 0.1544(0.1097) Grad: 275632.3750  LR: 0.00000072
EVAL: [0/25] Elapsed 0m 4s (remain 1m 41s) Loss: 0.1192(0.1192)
EVAL: [24/25] Elapsed 1m 8s (remain 0m 0s) Loss: 0.0778(0.1106)
[2022-10-29 18:49:52] - Epoch 4 - avg_train_loss: 0.1097  avg_val_loss: 0.1106  time: 427s
[2022-10-29 18:49:52] - Epoch 4 - Score: 0.4714  Scores: [0.4984378677097424, 0.4471423043836372, 0.4388358026120859, 0.4613651989526535, 0.510611338066957, 0.47181719367116937]
[2022-10-29 18:49:52] - Epoch 4 - Save Best Score: 0.4714 Model
Epoch: [5][0/195] Elapsed 0m 1s (remain 5m 37s) Loss: 0.0921(0.0921) Grad: 135326.7969  LR: 0.00000074
Epoch: [5][100/195] Elapsed 3m 5s (remain 2m 52s) Loss: 0.1185(0.1052) Grad: 210747.0469  LR: 0.00000074
Epoch: [5][194/195] Elapsed 6m 3s (remain 0m 0s) Loss: 0.0938(0.1066) Grad: 360443.9375  LR: 0.00000058
EVAL: [0/25] Elapsed 0m 4s (remain 1m 41s) Loss: 0.1182(0.1182)
EVAL: [24/25] Elapsed 1m 8s (remain 0m 0s) Loss: 0.0726(0.1076)
[2022-10-29 18:57:06] - Epoch 5 - avg_train_loss: 0.1066  avg_val_loss: 0.1076  time: 432s
[2022-10-29 18:57:06] - Epoch 5 - Score: 0.4652  Scores: [0.495111610140561, 0.4421654221523165, 0.4346972388974648, 0.4573054502312546, 0.49564704878081706, 0.46598177279687386]
[2022-10-29 18:57:06] - Epoch 5 - Save Best Score: 0.4652 Model
[2022-10-29 18:57:09] - ========== fold: 4 result ==========
[2022-10-29 18:57:09] - Score: 0.4652  Scores: [0.495111610140561, 0.4421654221523165, 0.4346972388974648, 0.4573054502312546, 0.49564704878081706, 0.46598177279687386]
[2022-10-29 18:57:09] - ========== CV ==========
[2022-10-29 18:57:09] - Score: 0.4696  Scores: [0.5039402954861715, 0.4586054812213854, 0.4267388022097853, 0.46477946639529705, 0.49042549805201324, 0.4733990256191938]