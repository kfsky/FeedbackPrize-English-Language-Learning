Downloading tokenizer_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 52.0/52.0 [00:00<00:00, 55.0kB/s]
Downloading config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 633/633 [00:00<00:00, 536kB/s]
Downloading spm.model: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.33M/2.33M [00:00<00:00, 25.3MB/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
  0%|                                                                                                                                                                                                                                                | 0/3911 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors

100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3911/3911 [00:04<00:00, 953.66it/s]
[2022-10-22 09:50:29] - max_len: 2048
[2022-10-22 09:50:29] - ========== fold: 0 training ==========
[2022-10-22 09:50:29] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v2-xlarge",
  "attention_dropout": 0.0,
  "attention_head_size": 64,
  "attention_probs_dropout_prob": 0.0,
  "conv_act": "gelu",
  "conv_kernel_size": 3,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 24,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1536,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}














Downloading pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.65G/1.65G [00:28<00:00, 61.5MB/s]
Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/782] Elapsed 0m 3s (remain 45m 44s) Loss: 2.8792(2.8792) Grad: nan  LR: 0.00002994
Epoch: [1][100/782] Elapsed 0m 58s (remain 6m 36s) Loss: 0.2353(0.6518) Grad: 5691.5273  LR: 0.00002994
Epoch: [1][200/782] Elapsed 1m 54s (remain 5m 31s) Loss: 0.2750(0.4784) Grad: 2906.4758  LR: 0.00002994
Epoch: [1][300/782] Elapsed 2m 54s (remain 4m 39s) Loss: 0.4420(0.4103) Grad: 6939.5034  LR: 0.00002994
Epoch: [1][400/782] Elapsed 3m 53s (remain 3m 42s) Loss: 0.2272(0.3766) Grad: 4947.7568  LR: 0.00002994
Epoch: [1][500/782] Elapsed 4m 45s (remain 2m 40s) Loss: 0.1642(0.3534) Grad: 2878.7712  LR: 0.00002994
Epoch: [1][600/782] Elapsed 5m 41s (remain 1m 42s) Loss: 0.1886(0.3342) Grad: 2058.1587  LR: 0.00002994
Epoch: [1][700/782] Elapsed 6m 42s (remain 0m 46s) Loss: 0.1934(0.3201) Grad: 2303.1067  LR: 0.00002994
Epoch: [1][781/782] Elapsed 7m 34s (remain 0m 0s) Loss: 0.1840(0.3137) Grad: 2370.2065  LR: 0.00002231
EVAL: [0/98] Elapsed 0m 1s (remain 1m 43s) Loss: 0.2655(0.2655)
EVAL: [97/98] Elapsed 1m 30s (remain 0m 0s) Loss: 0.1858(0.2052)
[2022-10-22 10:00:13] - Epoch 1 - avg_train_loss: 0.3137  avg_val_loss: 0.2052  time: 545s
[2022-10-22 10:00:13] - Epoch 1 - Score: 0.6532  Scores: [0.6493402400715096, 0.6393167095081868, 0.5936690856833217, 0.6467942771723029, 0.7138247365939012, 0.6760298251252711]
[2022-10-22 10:00:13] - Epoch 1 - Save Best Score: 0.6532 Model
Epoch: [2][0/782] Elapsed 0m 1s (remain 13m 36s) Loss: 0.1685(0.1685) Grad: 511402.2500  LR: 0.00002312
Epoch: [2][100/782] Elapsed 0m 57s (remain 6m 26s) Loss: 0.6553(0.4390) Grad: 417889.7500  LR: 0.00002312
Epoch: [2][200/782] Elapsed 1m 52s (remain 5m 24s) Loss: 0.3304(0.3942) Grad: 212857.7344  LR: 0.00002312
Epoch: [2][300/782] Elapsed 2m 53s (remain 4m 36s) Loss: 0.4555(0.3571) Grad: 266118.8750  LR: 0.00002312
Epoch: [2][400/782] Elapsed 3m 56s (remain 3m 44s) Loss: 0.2428(0.3279) Grad: 351079.9688  LR: 0.00002312
Epoch: [2][500/782] Elapsed 5m 2s (remain 2m 49s) Loss: 0.1563(0.3095) Grad: 130494.3281  LR: 0.00002312
Epoch: [2][600/782] Elapsed 6m 0s (remain 1m 48s) Loss: 0.2192(0.2988) Grad: 119335.0547  LR: 0.00002312
Epoch: [2][700/782] Elapsed 6m 55s (remain 0m 48s) Loss: 0.2594(0.2882) Grad: 247446.4531  LR: 0.00002312
Epoch: [2][781/782] Elapsed 7m 39s (remain 0m 0s) Loss: 0.1085(0.2820) Grad: 162138.7500  LR: 0.00000633
EVAL: [0/98] Elapsed 0m 1s (remain 1m 39s) Loss: 0.2677(0.2677)
[2022-10-22 10:09:30] - Epoch 2 - avg_train_loss: 0.2820  avg_val_loss: 0.1996  time: 550s
[2022-10-22 10:09:30] - Epoch 2 - Score: 0.6422  Scores: [0.6497090468794752, 0.6325550819825183, 0.5723124335766179, 0.6430262700776354, 0.6805575535300243, 0.6749948256371073]
EVAL: [97/98] Elapsed 1m 30s (remain 0m 0s) Loss: 0.1928(0.1996)
[2022-10-22 10:09:30] - Epoch 2 - Save Best Score: 0.6422 Model
Epoch: [3][0/782] Elapsed 0m 0s (remain 12m 7s) Loss: 0.4690(0.4690) Grad: 303859.0938  LR: 0.00000711
Epoch: [3][100/782] Elapsed 1m 0s (remain 6m 48s) Loss: 0.1851(0.2457) Grad: 154603.5625  LR: 0.00000711
Epoch: [3][200/782] Elapsed 1m 58s (remain 5m 43s) Loss: 0.3058(0.2459) Grad: 221149.2969  LR: 0.00000711
Epoch: [3][300/782] Elapsed 2m 57s (remain 4m 43s) Loss: 0.2548(0.2455) Grad: 183423.2188  LR: 0.00000711
Epoch: [3][400/782] Elapsed 3m 54s (remain 3m 43s) Loss: 0.1439(0.2408) Grad: 111376.5625  LR: 0.00000711
Epoch: [3][500/782] Elapsed 4m 51s (remain 2m 43s) Loss: 0.3175(0.2421) Grad: 263989.5625  LR: 0.00000711
Epoch: [3][600/782] Elapsed 5m 48s (remain 1m 45s) Loss: 0.2138(0.2385) Grad: 115430.6953  LR: 0.00000711
Epoch: [3][700/782] Elapsed 6m 42s (remain 0m 46s) Loss: 0.1249(0.2372) Grad: 185782.6250  LR: 0.00000711
Epoch: [3][781/782] Elapsed 7m 31s (remain 0m 0s) Loss: 0.1764(0.2351) Grad: 146275.7656  LR: 0.00000040
EVAL: [0/98] Elapsed 0m 1s (remain 1m 40s) Loss: 0.2684(0.2684)
[2022-10-22 10:18:39] - Epoch 3 - avg_train_loss: 0.2351  avg_val_loss: 0.2006  time: 542s
[2022-10-22 10:18:39] - Epoch 3 - Score: 0.6438  Scores: [0.6478322135880357, 0.6365873366511782, 0.5722095549704208, 0.6442034010965718, 0.6862866410624454, 0.6758438143688524]
EVAL: [97/98] Elapsed 1m 30s (remain 0m 0s) Loss: 0.1997(0.2006)
Epoch: [4][0/782] Elapsed 0m 0s (remain 9m 44s) Loss: 0.3944(0.3944) Grad: 359852.0312  LR: 0.00000025
Epoch: [4][100/782] Elapsed 0m 57s (remain 6m 25s) Loss: 0.3635(0.2272) Grad: 97271.8516  LR: 0.00000025
Epoch: [4][200/782] Elapsed 1m 57s (remain 5m 38s) Loss: 0.1809(0.2384) Grad: 268043.7500  LR: 0.00000025
Epoch: [4][300/782] Elapsed 2m 54s (remain 4m 38s) Loss: 0.4107(0.2399) Grad: 269506.9375  LR: 0.00000025
Epoch: [4][400/782] Elapsed 3m 55s (remain 3m 43s) Loss: 0.2577(0.2384) Grad: 216475.9062  LR: 0.00000025
Epoch: [4][500/782] Elapsed 4m 54s (remain 2m 44s) Loss: 0.1515(0.2366) Grad: 111390.7500  LR: 0.00000025
Epoch: [4][600/782] Elapsed 5m 47s (remain 1m 44s) Loss: 0.2175(0.2349) Grad: 131618.0469  LR: 0.00000025
Epoch: [4][700/782] Elapsed 6m 47s (remain 0m 47s) Loss: 0.3674(0.2372) Grad: 190769.8594  LR: 0.00000025
Epoch: [4][781/782] Elapsed 7m 34s (remain 0m 0s) Loss: 0.0699(0.2365) Grad: 123781.4141  LR: 0.00001139
EVAL: [0/98] Elapsed 0m 1s (remain 1m 42s) Loss: 0.2728(0.2728)
EVAL: [97/98] Elapsed 1m 30s (remain 0m 0s) Loss: 0.2098(0.2040)
Epoch: [5][0/782] Elapsed 0m 1s (remain 14m 8s) Loss: 0.2468(0.2468) Grad: 322452.5938  LR: 0.00001048
[2022-10-22 10:27:44] - Epoch 4 - avg_train_loss: 0.2365  avg_val_loss: 0.2040  time: 545s
[2022-10-22 10:27:44] - Epoch 4 - Score: 0.6493  Scores: [0.6578758936238659, 0.6388493305525903, 0.5745021863727943, 0.6432675892889032, 0.707364798845059, 0.6740904821070938]
Epoch: [5][100/782] Elapsed 0m 57s (remain 6m 26s) Loss: 0.1132(0.2283) Grad: 206566.4531  LR: 0.00001048
Epoch: [5][200/782] Elapsed 1m 58s (remain 5m 41s) Loss: 0.3167(0.2321) Grad: 312190.9375  LR: 0.00001048
Epoch: [5][300/782] Elapsed 2m 59s (remain 4m 47s) Loss: 0.2981(0.2300) Grad: 288334.8125  LR: 0.00001048
Epoch: [5][400/782] Elapsed 3m 56s (remain 3m 44s) Loss: 0.5535(0.2324) Grad: 358780.5938  LR: 0.00001048
Epoch: [5][500/782] Elapsed 4m 53s (remain 2m 44s) Loss: 0.4190(0.2335) Grad: 183471.0625  LR: 0.00001048
Epoch: [5][600/782] Elapsed 5m 48s (remain 1m 45s) Loss: 0.3261(0.2358) Grad: 126317.6484  LR: 0.00001048
Epoch: [5][700/782] Elapsed 6m 43s (remain 0m 46s) Loss: 0.2609(0.2368) Grad: 175820.9062  LR: 0.00001048
Epoch: [5][781/782] Elapsed 7m 30s (remain 0m 0s) Loss: 0.2027(0.2367) Grad: 232340.5625  LR: 0.00002663
EVAL: [0/98] Elapsed 0m 1s (remain 1m 42s) Loss: 0.2745(0.2745)
EVAL: [97/98] Elapsed 1m 30s (remain 0m 0s) Loss: 0.1878(0.2027)
[2022-10-22 10:36:45] - Epoch 5 - avg_train_loss: 0.2367  avg_val_loss: 0.2027  time: 541s
[2022-10-22 10:36:45] - Epoch 5 - Score: 0.6479  Scores: [0.647791817071773, 0.6338720911743579, 0.6022687518875011, 0.6422040447544495, 0.6872558522649976, 0.6741124273400642]
[2022-10-22 10:36:48] - ========== fold: 0 result ==========
[2022-10-22 10:36:48] - Score: 0.6422  Scores: [0.6497090468794752, 0.6325550819825183, 0.5723124335766179, 0.6430262700776354, 0.6805575535300243, 0.6749948256371073]
[2022-10-22 10:36:48] - ========== fold: 1 training ==========
[2022-10-22 10:36:48] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v2-xlarge",
  "attention_dropout": 0.0,
  "attention_head_size": 64,
  "attention_probs_dropout_prob": 0.0,
  "conv_act": "gelu",
  "conv_kernel_size": 3,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 24,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1536,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Epoch: [1][0/782] Elapsed 0m 0s (remain 11m 18s) Loss: 2.8515(2.8515) Grad: nan  LR: 0.00002994
Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][100/782] Elapsed 0m 56s (remain 6m 22s) Loss: 0.3848(0.6877) Grad: 2039.1335  LR: 0.00002994
Epoch: [1][200/782] Elapsed 1m 58s (remain 5m 43s) Loss: 0.2264(0.4944) Grad: 1306.2249  LR: 0.00002994
Epoch: [1][300/782] Elapsed 2m 59s (remain 4m 46s) Loss: 0.2507(0.4161) Grad: 494.4353  LR: 0.00002994
Epoch: [1][400/782] Elapsed 3m 59s (remain 3m 47s) Loss: 0.2540(0.3792) Grad: 1169.1460  LR: 0.00002994
Epoch: [1][500/782] Elapsed 4m 52s (remain 2m 44s) Loss: 0.2575(0.3537) Grad: 806.9986  LR: 0.00002994
Epoch: [1][600/782] Elapsed 5m 51s (remain 1m 45s) Loss: 0.1170(0.3368) Grad: 514.4633  LR: 0.00002994
Epoch: [1][700/782] Elapsed 6m 46s (remain 0m 47s) Loss: 0.2368(0.3264) Grad: 1002.4369  LR: 0.00002994
Epoch: [1][781/782] Elapsed 7m 36s (remain 0m 0s) Loss: 0.3284(0.3166) Grad: 783.5500  LR: 0.00002231
EVAL: [0/98] Elapsed 0m 1s (remain 1m 50s) Loss: 0.2927(0.2927)
EVAL: [97/98] Elapsed 1m 29s (remain 0m 0s) Loss: 0.2230(0.2180)
[2022-10-22 10:46:01] - Epoch 1 - avg_train_loss: 0.3166  avg_val_loss: 0.2180  time: 546s
[2022-10-22 10:46:01] - Epoch 1 - Score: 0.6732  Scores: [0.6611946623190441, 0.6766599172826164, 0.6446013778183886, 0.6642069489046852, 0.7162886336095693, 0.6760695801975509]
[2022-10-22 10:46:01] - Epoch 1 - Save Best Score: 0.6732 Model
Epoch: [2][0/782] Elapsed 0m 0s (remain 10m 18s) Loss: 0.5367(0.5367) Grad: inf  LR: 0.00002312
Epoch: [2][100/782] Elapsed 0m 58s (remain 6m 35s) Loss: 0.4799(0.3597) Grad: 533629.7500  LR: 0.00002312
Epoch: [2][200/782] Elapsed 1m 57s (remain 5m 38s) Loss: 0.3283(0.3599) Grad: 253717.6250  LR: 0.00002312
Epoch: [2][300/782] Elapsed 2m 55s (remain 4m 40s) Loss: 0.1550(0.3517) Grad: 170483.1094  LR: 0.00002312
Epoch: [2][400/782] Elapsed 3m 52s (remain 3m 40s) Loss: 0.2083(0.3320) Grad: 215255.3438  LR: 0.00002312
Epoch: [2][500/782] Elapsed 4m 46s (remain 2m 40s) Loss: 0.1707(0.3222) Grad: 250502.9844  LR: 0.00002312
Epoch: [2][600/782] Elapsed 5m 47s (remain 1m 44s) Loss: 0.2113(0.3122) Grad: 120487.0781  LR: 0.00002312
Epoch: [2][700/782] Elapsed 6m 44s (remain 0m 46s) Loss: 0.1628(0.3031) Grad: 196223.2344  LR: 0.00002312
Epoch: [2][781/782] Elapsed 7m 33s (remain 0m 0s) Loss: 0.3057(0.2951) Grad: 372885.5000  LR: 0.00000633
EVAL: [0/98] Elapsed 0m 1s (remain 1m 46s) Loss: 0.2822(0.2822)
EVAL: [97/98] Elapsed 1m 29s (remain 0m 0s) Loss: 0.2126(0.2094)
[2022-10-22 10:55:09] - Epoch 2 - avg_train_loss: 0.2951  avg_val_loss: 0.2094  time: 543s
[2022-10-22 10:55:09] - Epoch 2 - Score: 0.6585  Scores: [0.6643703383924382, 0.6568268086932403, 0.578144281157949, 0.6789400121042435, 0.694157909376348, 0.6788445090417137]
[2022-10-22 10:55:09] - Epoch 2 - Save Best Score: 0.6585 Model
Epoch: [3][0/782] Elapsed 0m 0s (remain 11m 29s) Loss: 0.2502(0.2502) Grad: 305439.6250  LR: 0.00000711
Epoch: [3][100/782] Elapsed 0m 57s (remain 6m 29s) Loss: 0.1317(0.2352) Grad: 129652.1641  LR: 0.00000711
Epoch: [3][200/782] Elapsed 1m 56s (remain 5m 36s) Loss: 0.1725(0.2387) Grad: 230195.8438  LR: 0.00000711
Epoch: [3][300/782] Elapsed 2m 56s (remain 4m 41s) Loss: 0.1643(0.2403) Grad: 137815.4844  LR: 0.00000711
Epoch: [3][400/782] Elapsed 3m 52s (remain 3m 40s) Loss: 0.2520(0.2391) Grad: 246255.9688  LR: 0.00000711
Epoch: [3][500/782] Elapsed 4m 53s (remain 2m 44s) Loss: 0.1865(0.2402) Grad: 190914.7500  LR: 0.00000711
Epoch: [3][600/782] Elapsed 5m 49s (remain 1m 45s) Loss: 0.2147(0.2387) Grad: 242145.9219  LR: 0.00000711
Epoch: [3][700/782] Elapsed 6m 47s (remain 0m 47s) Loss: 0.1915(0.2393) Grad: 218518.0938  LR: 0.00000711
Epoch: [3][781/782] Elapsed 7m 36s (remain 0m 0s) Loss: 0.3431(0.2376) Grad: 123632.8906  LR: 0.00000040
EVAL: [0/98] Elapsed 0m 1s (remain 1m 50s) Loss: 0.2783(0.2783)
EVAL: [97/98] Elapsed 1m 29s (remain 0m 0s) Loss: 0.2007(0.2027)
[2022-10-22 11:04:19] - Epoch 3 - avg_train_loss: 0.2376  avg_val_loss: 0.2027  time: 546s
[2022-10-22 11:04:19] - Epoch 3 - Score: 0.6473  Scores: [0.6553902537843189, 0.6444895135137034, 0.5690375878695657, 0.6594851421596292, 0.6846860413471101, 0.6708419534304632]
[2022-10-22 11:04:19] - Epoch 3 - Save Best Score: 0.6473 Model
Epoch: [4][0/782] Elapsed 0m 0s (remain 10m 50s) Loss: 0.1836(0.1836) Grad: 368634.9688  LR: 0.00000025
Epoch: [4][100/782] Elapsed 1m 0s (remain 6m 50s) Loss: 0.4955(0.2392) Grad: 126508.9844  LR: 0.00000025
Epoch: [4][200/782] Elapsed 1m 55s (remain 5m 34s) Loss: 0.3691(0.2476) Grad: 256581.0469  LR: 0.00000025
Epoch: [4][300/782] Elapsed 2m 50s (remain 4m 32s) Loss: 0.2433(0.2400) Grad: 192484.5469  LR: 0.00000025
Epoch: [4][400/782] Elapsed 3m 52s (remain 3m 40s) Loss: 0.1935(0.2384) Grad: 240249.5000  LR: 0.00000025
Epoch: [4][500/782] Elapsed 4m 48s (remain 2m 42s) Loss: 0.1776(0.2372) Grad: 92544.5703  LR: 0.00000025
Epoch: [4][600/782] Elapsed 5m 48s (remain 1m 45s) Loss: 0.3238(0.2373) Grad: 216580.2031  LR: 0.00000025
Epoch: [4][700/782] Elapsed 6m 47s (remain 0m 47s) Loss: 0.2152(0.2365) Grad: 120427.4531  LR: 0.00000025
Epoch: [4][781/782] Elapsed 7m 34s (remain 0m 0s) Loss: 0.2832(0.2389) Grad: 250306.4375  LR: 0.00001139
EVAL: [0/98] Elapsed 0m 1s (remain 1m 50s) Loss: 0.2796(0.2796)
[2022-10-22 11:13:27] - Epoch 4 - avg_train_loss: 0.2389  avg_val_loss: 0.2031  time: 544s
[2022-10-22 11:13:27] - Epoch 4 - Score: 0.6480  Scores: [0.6559866749595658, 0.6451686840190128, 0.5694165088230222, 0.6593304920602829, 0.684684479005075, 0.6732533918123802]
EVAL: [97/98] Elapsed 1m 29s (remain 0m 0s) Loss: 0.2039(0.2031)
Epoch: [5][0/782] Elapsed 0m 1s (remain 15m 31s) Loss: 0.2244(0.2244) Grad: 500786.0625  LR: 0.00001048
Epoch: [5][100/782] Elapsed 0m 56s (remain 6m 19s) Loss: 0.2554(0.2374) Grad: 222622.2500  LR: 0.00001048
Epoch: [5][200/782] Elapsed 1m 51s (remain 5m 22s) Loss: 0.1109(0.2410) Grad: 201362.9844  LR: 0.00001048
Epoch: [5][300/782] Elapsed 2m 52s (remain 4m 35s) Loss: 0.2920(0.2450) Grad: 160864.2344  LR: 0.00001048
Epoch: [5][400/782] Elapsed 3m 53s (remain 3m 41s) Loss: 0.2323(0.2412) Grad: 134153.6562  LR: 0.00001048
Epoch: [5][500/782] Elapsed 4m 52s (remain 2m 43s) Loss: 0.3463(0.2401) Grad: 148743.3750  LR: 0.00001048
Epoch: [5][600/782] Elapsed 5m 46s (remain 1m 44s) Loss: 0.3906(0.2409) Grad: 201898.5625  LR: 0.00001048
Epoch: [5][700/782] Elapsed 6m 45s (remain 0m 46s) Loss: 0.1609(0.2383) Grad: 195399.1406  LR: 0.00001048
Epoch: [5][781/782] Elapsed 7m 34s (remain 0m 0s) Loss: 0.2027(0.2379) Grad: 224204.7500  LR: 0.00002663
EVAL: [0/98] Elapsed 0m 1s (remain 1m 50s) Loss: 0.2810(0.2810)
EVAL: [97/98] Elapsed 1m 29s (remain 0m 0s) Loss: 0.2019(0.2030)
[2022-10-22 11:22:31] - Epoch 5 - avg_train_loss: 0.2379  avg_val_loss: 0.2030  time: 544s
[2022-10-22 11:22:31] - Epoch 5 - Score: 0.6478  Scores: [0.6554257038779019, 0.6453564660928485, 0.5696782475869668, 0.6593036349137688, 0.6852630559875459, 0.6719369554346755]
[2022-10-22 11:22:33] - ========== fold: 1 result ==========
[2022-10-22 11:22:33] - Score: 0.6473  Scores: [0.6553902537843189, 0.6444895135137034, 0.5690375878695657, 0.6594851421596292, 0.6846860413471101, 0.6708419534304632]
[2022-10-22 11:22:33] - ========== fold: 2 training ==========
[2022-10-22 11:22:33] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v2-xlarge",
  "attention_dropout": 0.0,
  "attention_head_size": 64,
  "attention_probs_dropout_prob": 0.0,
  "conv_act": "gelu",
  "conv_kernel_size": 3,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 6144,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 24,
  "num_hidden_layers": 24,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 1536,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v2-xlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/782] Elapsed 0m 1s (remain 13m 58s) Loss: 3.9378(3.9378) Grad: nan  LR: 0.00002994
Epoch: [1][100/782] Elapsed 1m 2s (remain 7m 4s) Loss: 0.2792(0.5895) Grad: 30633.6074  LR: 0.00002994
Epoch: [1][200/782] Elapsed 1m 59s (remain 5m 44s) Loss: 0.2937(0.4443) Grad: 36922.4570  LR: 0.00002994
Epoch: [1][300/782] Elapsed 2m 58s (remain 4m 45s) Loss: 0.3723(0.4122) Grad: 20188.5684  LR: 0.00002994
Epoch: [1][400/782] Elapsed 3m 53s (remain 3m 41s) Loss: 0.3114(0.3819) Grad: 25161.0312  LR: 0.00002994
Traceback (most recent call last):
  File "/notebooks/code/exp023.py", line 739, in <module>
    main()
  File "/notebooks/code/exp023.py", line 678, in main
    _oof_df = train_loop(train, fold)
  File "/notebooks/code/exp023.py", line 590, in train_loop
    avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)
  File "/notebooks/code/exp023.py", line 441, in train_fn
    y_preds = model(inputs)
  File "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/notebooks/code/exp023.py", line 360, in forward
    feature = self.feature(inputs)
  File "/notebooks/code/exp023.py", line 349, in feature
    outputs = self.model(**inputs)
  File "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.9/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 1053, in forward
    encoder_outputs = self.encoder(
  File "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.9/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 493, in forward
    output_states = torch.utils.checkpoint.checkpoint(
  File "/usr/local/lib/python3.9/dist-packages/torch/utils/checkpoint.py", line 235, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/usr/local/lib/python3.9/dist-packages/torch/utils/checkpoint.py", line 96, in forward
    outputs = run_function(*args)
  File "/usr/local/lib/python3.9/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 489, in custom_forward
    return module(*inputs, output_attentions)
  File "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.9/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 346, in forward
    attention_output = self.attention(
  File "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.9/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 277, in forward
    self_output = self.self(
  File "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.9/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 702, in forward
    rel_att = self.disentangled_attention_bias(
  File "/usr/local/lib/python3.9/dist-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 746, in disentangled_attention_bias
    relative_pos = relative_pos.long().to(query_layer.device)
KeyboardInterrupt