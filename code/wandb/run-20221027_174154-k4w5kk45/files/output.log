Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3911/3911 [00:03<00:00, 991.49it/s]
[2022-10-27 17:42:01] - max_len: 2048
[2022-10-27 17:42:01] - ========== fold: 0 training ==========
[2022-10-27 17:42:01] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Reinitializing Last 1 Layers ...
Done.!
Epoch: [1][0/195] Elapsed 0m 2s (remain 7m 0s) Loss: 2.5898(2.5898) Grad: inf  LR: 0.00000100
Epoch: [1][100/195] Elapsed 2m 4s (remain 1m 56s) Loss: 1.7829(2.2368) Grad: 349957.9688  LR: 0.00000100
Epoch: [1][194/195] Elapsed 4m 2s (remain 0m 0s) Loss: 0.6021(1.6891) Grad: 322915.7812  LR: 0.00000099
EVAL: [0/25] Elapsed 0m 1s (remain 0m 30s) Loss: 0.4618(0.4618)
[2022-10-27 17:46:44] - Epoch 1 - avg_train_loss: 1.6891  avg_val_loss: 0.5415  time: 279s
[2022-10-27 17:46:44] - Epoch 1 - Score: 1.0978  Scores: [1.0973482688413947, 0.8492088768669935, 1.6263892540983103, 1.2605635302120741, 0.8840764365069483, 0.8693586599316641]
[2022-10-27 17:46:44] - Epoch 1 - Save Best Score: 1.0978 Model
EVAL: [24/25] Elapsed 0m 36s (remain 0m 0s) Loss: 0.5331(0.5415)
Epoch: [2][0/195] Elapsed 0m 0s (remain 2m 14s) Loss: 0.5320(0.5320) Grad: inf  LR: 0.00000099
Epoch: [2][100/195] Elapsed 2m 5s (remain 1m 57s) Loss: 0.1847(0.2488) Grad: 135636.7188  LR: 0.00000099
Epoch: [2][194/195] Elapsed 4m 7s (remain 0m 0s) Loss: 0.1039(0.2048) Grad: 133066.5156  LR: 0.00000093
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 0.1374(0.1374)
[2022-10-27 17:51:29] - Epoch 2 - avg_train_loss: 0.2048  avg_val_loss: 0.1355  time: 284s
[2022-10-27 17:51:29] - Epoch 2 - Score: 0.5240  Scores: [0.5481997006300882, 0.520304684372521, 0.48884202815171673, 0.5216284582875752, 0.5333226272906165, 0.5318167605658686]
[2022-10-27 17:51:29] - Epoch 2 - Save Best Score: 0.5240 Model
EVAL: [24/25] Elapsed 0m 36s (remain 0m 0s) Loss: 0.1316(0.1355)
Epoch: [3][0/195] Elapsed 0m 1s (remain 3m 16s) Loss: 0.1425(0.1425) Grad: 270473.2188  LR: 0.00000095
Epoch: [3][100/195] Elapsed 2m 5s (remain 1m 56s) Loss: 0.1039(0.1332) Grad: 300511.6875  LR: 0.00000095
Epoch: [3][194/195] Elapsed 4m 2s (remain 0m 0s) Loss: 0.1066(0.1294) Grad: 125891.3906  LR: 0.00000084
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 0.1247(0.1247)
[2022-10-27 17:56:09] - Epoch 3 - avg_train_loss: 0.1294  avg_val_loss: 0.1183  time: 278s
[2022-10-27 17:56:09] - Epoch 3 - Score: 0.4885  Scores: [0.5190061138791147, 0.4794030114129757, 0.4491077537438425, 0.48678814044390084, 0.5093647356420913, 0.4871536718947855]
EVAL: [24/25] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0982(0.1183)
[2022-10-27 17:56:09] - Epoch 3 - Save Best Score: 0.4885 Model
Epoch: [4][0/195] Elapsed 0m 1s (remain 3m 40s) Loss: 0.0905(0.0905) Grad: 215653.1875  LR: 0.00000086
Epoch: [4][100/195] Elapsed 2m 5s (remain 1m 56s) Loss: 0.1189(0.1231) Grad: 200037.6562  LR: 0.00000086
Epoch: [4][194/195] Elapsed 4m 4s (remain 0m 0s) Loss: 0.0938(0.1201) Grad: 143044.1562  LR: 0.00000072
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 0.1224(0.1224)
EVAL: [24/25] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0948(0.1144)
[2022-10-27 18:00:51] - Epoch 4 - avg_train_loss: 0.1201  avg_val_loss: 0.1144  time: 281s
[2022-10-27 18:00:51] - Epoch 4 - Score: 0.4797  Scores: [0.511234155183339, 0.4696374806605775, 0.4362521733439747, 0.4776507465825105, 0.5046950577274245, 0.4788519040465578]
[2022-10-27 18:00:51] - Epoch 4 - Save Best Score: 0.4797 Model
Epoch: [5][0/195] Elapsed 0m 1s (remain 4m 25s) Loss: 0.1118(0.1118) Grad: 189657.3438  LR: 0.00000074
Epoch: [5][100/195] Elapsed 2m 2s (remain 1m 54s) Loss: 0.1319(0.1138) Grad: 247713.7656  LR: 0.00000074
Epoch: [5][194/195] Elapsed 4m 2s (remain 0m 0s) Loss: 0.1784(0.1158) Grad: 143224.5312  LR: 0.00000058
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 0.1252(0.1252)
[2022-10-27 18:05:31] - Epoch 5 - avg_train_loss: 0.1158  avg_val_loss: 0.1147  time: 278s
[2022-10-27 18:05:31] - Epoch 5 - Score: 0.4800  Scores: [0.5120682376729139, 0.47244983490333226, 0.43181268795024413, 0.47754963665788064, 0.5110378552589543, 0.47525252380820765]
EVAL: [24/25] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0956(0.1147)
Epoch: [6][0/195] Elapsed 0m 2s (remain 7m 9s) Loss: 0.0995(0.0995) Grad: 113397.1172  LR: 0.00000061
Epoch: [6][100/195] Elapsed 2m 7s (remain 1m 58s) Loss: 0.0976(0.1136) Grad: 133033.1094  LR: 0.00000061
Epoch: [6][194/195] Elapsed 4m 3s (remain 0m 0s) Loss: 0.0838(0.1139) Grad: 248492.1406  LR: 0.00000044
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 0.1232(0.1232)
[2022-10-27 18:10:11] - Epoch 6 - avg_train_loss: 0.1139  avg_val_loss: 0.1123  time: 280s
[2022-10-27 18:10:11] - Epoch 6 - Score: 0.4748  Scores: [0.5086962211867493, 0.46777349598576046, 0.4273994627045601, 0.4735801770763271, 0.5004291460123856, 0.47120004471079185]
[2022-10-27 18:10:11] - Epoch 6 - Save Best Score: 0.4748 Model
EVAL: [24/25] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0946(0.1123)
Epoch: [7][0/195] Elapsed 0m 1s (remain 5m 3s) Loss: 0.1281(0.1281) Grad: 268584.5312  LR: 0.00000047
Epoch: [7][100/195] Elapsed 1m 59s (remain 1m 50s) Loss: 0.0926(0.1093) Grad: 144148.3906  LR: 0.00000047
Epoch: [7][194/195] Elapsed 4m 5s (remain 0m 0s) Loss: 0.0971(0.1122) Grad: 215573.2656  LR: 0.00000031
EVAL: [0/25] Elapsed 0m 1s (remain 0m 30s) Loss: 0.1213(0.1213)
EVAL: [24/25] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0971(0.1112)
[2022-10-27 18:14:54] - Epoch 7 - avg_train_loss: 0.1122  avg_val_loss: 0.1112  time: 282s
[2022-10-27 18:14:54] - Epoch 7 - Score: 0.4724  Scores: [0.5065392943398445, 0.466807032657104, 0.42268703363785115, 0.4707839369372342, 0.4950298358154675, 0.4725917935532235]
[2022-10-27 18:14:54] - Epoch 7 - Save Best Score: 0.4724 Model
Epoch: [8][0/195] Elapsed 0m 1s (remain 5m 53s) Loss: 0.1354(0.1354) Grad: 173153.6250  LR: 0.00000034
Epoch: [8][100/195] Elapsed 2m 6s (remain 1m 58s) Loss: 0.1182(0.1102) Grad: 240598.3125  LR: 0.00000034
Epoch: [8][194/195] Elapsed 4m 4s (remain 0m 0s) Loss: 0.1217(0.1098) Grad: 461046.2188  LR: 0.00000021
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 0.1193(0.1193)
[2022-10-27 18:19:36] - Epoch 8 - avg_train_loss: 0.1098  avg_val_loss: 0.1099  time: 281s
[2022-10-27 18:19:36] - Epoch 8 - Score: 0.4695  Scores: [0.5047520986931906, 0.46265125322875167, 0.4208696251585653, 0.4682267352239204, 0.49415779057582015, 0.46662766501270014]
[2022-10-27 18:19:36] - Epoch 8 - Save Best Score: 0.4695 Model
EVAL: [24/25] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0979(0.1099)
Epoch: [9][0/195] Elapsed 0m 2s (remain 7m 1s) Loss: 0.0981(0.0981) Grad: 125598.4766  LR: 0.00000022
Epoch: [9][100/195] Elapsed 1m 59s (remain 1m 51s) Loss: 0.1354(0.1129) Grad: 127696.2656  LR: 0.00000022
Epoch: [9][194/195] Elapsed 4m 2s (remain 0m 0s) Loss: 0.1291(0.1087) Grad: 256555.0156  LR: 0.00000013
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 0.1191(0.1191)
EVAL: [24/25] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0990(0.1097)
[2022-10-27 18:24:17] - Epoch 9 - avg_train_loss: 0.1087  avg_val_loss: 0.1097  time: 279s
[2022-10-27 18:24:17] - Epoch 9 - Score: 0.4691  Scores: [0.5042157329075594, 0.4624148908748313, 0.4206385646109212, 0.46538322553804823, 0.49095647520779767, 0.4708993146027926]
[2022-10-27 18:24:17] - Epoch 9 - Save Best Score: 0.4691 Model
Epoch: [10][0/195] Elapsed 0m 1s (remain 3m 41s) Loss: 0.1014(0.1014) Grad: 211334.0469  LR: 0.00000015
Epoch: [10][100/195] Elapsed 2m 10s (remain 2m 1s) Loss: 0.1324(0.1092) Grad: 167762.8125  LR: 0.00000015
Epoch: [10][194/195] Elapsed 4m 8s (remain 0m 0s) Loss: 0.1074(0.1073) Grad: 115849.5000  LR: 0.00000010
EVAL: [0/25] Elapsed 0m 1s (remain 0m 29s) Loss: 0.1214(0.1214)
[2022-10-27 18:29:03] - Epoch 10 - avg_train_loss: 0.1073  avg_val_loss: 0.1096  time: 285s
[2022-10-27 18:29:03] - Epoch 10 - Score: 0.4688  Scores: [0.5027731653540889, 0.46102844208044913, 0.4182488992764022, 0.4721295550323581, 0.4914627748281263, 0.467290631029533]
[2022-10-27 18:29:03] - Epoch 10 - Save Best Score: 0.4688 Model
EVAL: [24/25] Elapsed 0m 36s (remain 0m 0s) Loss: 0.0954(0.1096)
[2022-10-27 18:29:05] - ========== fold: 0 result ==========
[2022-10-27 18:29:05] - Score: 0.4688  Scores: [0.5027731653540889, 0.46102844208044913, 0.4182488992764022, 0.4721295550323581, 0.4914627748281263, 0.467290631029533]
[2022-10-27 18:29:05] - ========== fold: 1 training ==========
[2022-10-27 18:29:05] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Reinitializing Last 1 Layers ...
Done.!
Epoch: [1][0/195] Elapsed 0m 1s (remain 3m 38s) Loss: 2.7871(2.7871) Grad: inf  LR: 0.00000100
Epoch: [1][100/195] Elapsed 2m 7s (remain 1m 58s) Loss: 1.9190(2.3342) Grad: 324280.3438  LR: 0.00000100
Epoch: [1][194/195] Elapsed 4m 5s (remain 0m 0s) Loss: 0.8757(1.8664) Grad: 346854.0312  LR: 0.00000099
EVAL: [0/25] Elapsed 0m 2s (remain 0m 52s) Loss: 0.8725(0.8725)
EVAL: [24/25] Elapsed 0m 37s (remain 0m 0s) Loss: 0.8071(0.7801)
[2022-10-27 18:33:50] - Epoch 1 - avg_train_loss: 1.8664  avg_val_loss: 0.7801  time: 283s
[2022-10-27 18:33:50] - Epoch 1 - Score: 1.3824  Scores: [1.7408384393351912, 1.2298350986162143, 1.0063359484780454, 1.5524712877824902, 1.5352348710248922, 1.2293981218536434]
[2022-10-27 18:33:50] - Epoch 1 - Save Best Score: 1.3824 Model
Epoch: [2][0/195] Elapsed 0m 1s (remain 4m 53s) Loss: 0.9418(0.9418) Grad: inf  LR: 0.00000099
Epoch: [2][100/195] Elapsed 2m 5s (remain 1m 56s) Loss: 0.2254(0.2936) Grad: 143545.8594  LR: 0.00000099
Epoch: [2][194/195] Elapsed 4m 7s (remain 0m 0s) Loss: 0.1613(0.2267) Grad: 70678.7188  LR: 0.00000093
EVAL: [0/25] Elapsed 0m 2s (remain 0m 53s) Loss: 0.1772(0.1772)
[2022-10-27 18:38:36] - Epoch 2 - avg_train_loss: 0.2267  avg_val_loss: 0.1410  time: 286s
[2022-10-27 18:38:36] - Epoch 2 - Score: 0.5354  Scores: [0.5650281418432076, 0.5367322198984851, 0.484811987587333, 0.5353999218880202, 0.5528307636175934, 0.5378793355514598]
[2022-10-27 18:38:36] - Epoch 2 - Save Best Score: 0.5354 Model
EVAL: [24/25] Elapsed 0m 37s (remain 0m 0s) Loss: 0.1510(0.1410)
Epoch: [3][0/195] Elapsed 0m 2s (remain 7m 43s) Loss: 0.1031(0.1031) Grad: 119970.8516  LR: 0.00000095
Epoch: [3][100/195] Elapsed 2m 9s (remain 2m 0s) Loss: 0.1256(0.1297) Grad: 249848.0781  LR: 0.00000095
Epoch: [3][194/195] Elapsed 4m 4s (remain 0m 0s) Loss: 0.1860(0.1269) Grad: 399950.5938  LR: 0.00000084
EVAL: [0/25] Elapsed 0m 2s (remain 0m 53s) Loss: 0.1461(0.1461)
EVAL: [24/25] Elapsed 0m 37s (remain 0m 0s) Loss: 0.1249(0.1246)
[2022-10-27 18:43:20] - Epoch 3 - avg_train_loss: 0.1269  avg_val_loss: 0.1246  time: 282s
[2022-10-27 18:43:20] - Epoch 3 - Score: 0.5015  Scores: [0.5376836258624949, 0.4907412924960817, 0.4573878971498674, 0.5070416132645882, 0.5135221194957837, 0.5024997732077788]
[2022-10-27 18:43:20] - Epoch 3 - Save Best Score: 0.5015 Model
Epoch: [4][0/195] Elapsed 0m 1s (remain 5m 34s) Loss: 0.1844(0.1844) Grad: 309310.1562  LR: 0.00000086
Epoch: [4][100/195] Elapsed 2m 7s (remain 1m 59s) Loss: 0.1331(0.1213) Grad: 165481.7344  LR: 0.00000086
Epoch: [4][194/195] Elapsed 4m 8s (remain 0m 0s) Loss: 0.1379(0.1177) Grad: 193013.3438  LR: 0.00000072
EVAL: [0/25] Elapsed 0m 2s (remain 0m 53s) Loss: 0.1391(0.1391)
EVAL: [24/25] Elapsed 0m 37s (remain 0m 0s) Loss: 0.1130(0.1196)
[2022-10-27 18:48:08] - Epoch 4 - avg_train_loss: 0.1177  avg_val_loss: 0.1196  time: 287s
[2022-10-27 18:48:08] - Epoch 4 - Score: 0.4911  Scores: [0.5225555742526656, 0.47737936781411267, 0.45108770608788734, 0.4911772778804576, 0.5093625232981225, 0.4947455286572479]
[2022-10-27 18:48:08] - Epoch 4 - Save Best Score: 0.4911 Model
Epoch: [5][0/195] Elapsed 0m 0s (remain 2m 48s) Loss: 0.1150(0.1150) Grad: 174991.7344  LR: 0.00000074
Epoch: [5][100/195] Elapsed 2m 6s (remain 1m 57s) Loss: 0.1007(0.1149) Grad: 188523.0781  LR: 0.00000074
Epoch: [5][194/195] Elapsed 4m 4s (remain 0m 0s) Loss: 0.1226(0.1142) Grad: 235266.5469  LR: 0.00000058
EVAL: [0/25] Elapsed 0m 2s (remain 0m 54s) Loss: 0.1291(0.1291)
[2022-10-27 18:52:54] - Epoch 5 - avg_train_loss: 0.1142  avg_val_loss: 0.1160  time: 284s
[2022-10-27 18:52:54] - Epoch 5 - Score: 0.4831  Scores: [0.5140805130050049, 0.47172440844721797, 0.44029388720789064, 0.48765851482044154, 0.5028218158561345, 0.48212938450257764]
[2022-10-27 18:52:54] - Epoch 5 - Save Best Score: 0.4831 Model
EVAL: [24/25] Elapsed 0m 39s (remain 0m 0s) Loss: 0.1086(0.1160)
Epoch: [6][0/195] Elapsed 0m 1s (remain 5m 32s) Loss: 0.1151(0.1151) Grad: 394280.1875  LR: 0.00000061
Epoch: [6][100/195] Elapsed 2m 11s (remain 2m 2s) Loss: 0.1057(0.1089) Grad: 145070.6562  LR: 0.00000061
Epoch: [6][194/195] Elapsed 4m 12s (remain 0m 0s) Loss: 0.0924(0.1114) Grad: 223393.9375  LR: 0.00000044
EVAL: [0/25] Elapsed 0m 2s (remain 0m 54s) Loss: 0.1264(0.1264)
[2022-10-27 18:57:45] - Epoch 6 - avg_train_loss: 0.1114  avg_val_loss: 0.1143  time: 291s
[2022-10-27 18:57:45] - Epoch 6 - Score: 0.4795  Scores: [0.5125793339180371, 0.46717868241271915, 0.4349671362283635, 0.48528595142505704, 0.4992464843385026, 0.4779110080953939]
[2022-10-27 18:57:45] - Epoch 6 - Save Best Score: 0.4795 Model
EVAL: [24/25] Elapsed 0m 38s (remain 0m 0s) Loss: 0.1077(0.1143)
Epoch: [7][0/195] Elapsed 0m 1s (remain 3m 55s) Loss: 0.1006(0.1006) Grad: 255106.1875  LR: 0.00000047
Epoch: [7][100/195] Elapsed 2m 6s (remain 1m 57s) Loss: 0.0935(0.1081) Grad: 161325.8750  LR: 0.00000047
Epoch: [7][194/195] Elapsed 4m 11s (remain 0m 0s) Loss: 0.1053(0.1095) Grad: 346129.0000  LR: 0.00000031
EVAL: [0/25] Elapsed 0m 2s (remain 0m 54s) Loss: 0.1221(0.1221)
[2022-10-27 19:02:37] - Epoch 7 - avg_train_loss: 0.1095  avg_val_loss: 0.1132  time: 290s
EVAL: [24/25] Elapsed 0m 37s (remain 0m 0s) Loss: 0.1038(0.1132)
[2022-10-27 19:02:37] - Epoch 7 - Score: 0.4770  Scores: [0.5091466525544576, 0.4648324199823379, 0.43191089153143686, 0.4835965541256919, 0.49820052275342824, 0.4745454229114156]
[2022-10-27 19:02:37] - Epoch 7 - Save Best Score: 0.4770 Model
Epoch: [8][0/195] Elapsed 0m 1s (remain 5m 1s) Loss: 0.1213(0.1213) Grad: 202925.1719  LR: 0.00000034
Epoch: [8][100/195] Elapsed 2m 7s (remain 1m 58s) Loss: 0.0846(0.1100) Grad: 241477.3750  LR: 0.00000034
Epoch: [8][194/195] Elapsed 4m 7s (remain 0m 0s) Loss: 0.0842(0.1078) Grad: 184921.5781  LR: 0.00000021
EVAL: [0/25] Elapsed 0m 2s (remain 0m 53s) Loss: 0.1207(0.1207)
[2022-10-27 19:07:23] - Epoch 8 - avg_train_loss: 0.1078  avg_val_loss: 0.1122  time: 286s
[2022-10-27 19:07:23] - Epoch 8 - Score: 0.4749  Scores: [0.5072201552469427, 0.4635927132430509, 0.43040478732964255, 0.4796625805518603, 0.49598988342143535, 0.4724504823133908]
[2022-10-27 19:07:23] - Epoch 8 - Save Best Score: 0.4749 Model
EVAL: [24/25] Elapsed 0m 37s (remain 0m 0s) Loss: 0.1035(0.1122)
Epoch: [9][0/195] Elapsed 0m 2s (remain 6m 47s) Loss: 0.1150(0.1150) Grad: 142291.0000  LR: 0.00000022
Epoch: [9][100/195] Elapsed 2m 10s (remain 2m 1s) Loss: 0.1316(0.1053) Grad: 214020.6406  LR: 0.00000022
Epoch: [9][194/195] Elapsed 4m 10s (remain 0m 0s) Loss: 0.1135(0.1066) Grad: 235141.2500  LR: 0.00000013
EVAL: [0/25] Elapsed 0m 2s (remain 0m 53s) Loss: 0.1157(0.1157)
EVAL: [24/25] Elapsed 0m 38s (remain 0m 0s) Loss: 0.1070(0.1120)
[2022-10-27 19:12:13] - Epoch 9 - avg_train_loss: 0.1066  avg_val_loss: 0.1120  time: 289s
[2022-10-27 19:12:13] - Epoch 9 - Score: 0.4745  Scores: [0.5070845415793217, 0.4625702363316223, 0.43128830996849593, 0.47992985694716905, 0.4964213661807846, 0.4698164651269201]
[2022-10-27 19:12:13] - Epoch 9 - Save Best Score: 0.4745 Model
Epoch: [10][0/195] Elapsed 0m 1s (remain 3m 49s) Loss: 0.1155(0.1155) Grad: 184173.7188  LR: 0.00000015
Epoch: [10][100/195] Elapsed 2m 8s (remain 1m 59s) Loss: 0.0937(0.1063) Grad: 362238.0000  LR: 0.00000015
Epoch: [10][194/195] Elapsed 4m 7s (remain 0m 0s) Loss: 0.0865(0.1054) Grad: 89219.0938  LR: 0.00000010
EVAL: [0/25] Elapsed 0m 2s (remain 0m 53s) Loss: 0.1225(0.1225)
[2022-10-27 19:17:02] - Epoch 10 - avg_train_loss: 0.1054  avg_val_loss: 0.1122  time: 287s
[2022-10-27 19:17:02] - Epoch 10 - Score: 0.4749  Scores: [0.5092694677148082, 0.4636443605594525, 0.4293348774427868, 0.48070545709078477, 0.49792963102899235, 0.46865479944677063]
[2022-10-27 19:17:03] - ========== fold: 1 result ==========
[2022-10-27 19:17:03] - Score: 0.4745  Scores: [0.5070845415793217, 0.4625702363316223, 0.43128830996849593, 0.47992985694716905, 0.4964213661807846, 0.4698164651269201]
[2022-10-27 19:17:03] - ========== fold: 2 training ==========
[2022-10-27 19:17:03] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
EVAL: [24/25] Elapsed 0m 39s (remain 0m 0s) Loss: 0.1019(0.1122)
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Reinitializing Last 1 Layers ...
Done.!
Epoch: [1][0/195] Elapsed 0m 2s (remain 9m 3s) Loss: 2.2924(2.2924) Grad: inf  LR: 0.00000100
Epoch: [1][100/195] Elapsed 2m 10s (remain 2m 1s) Loss: 1.6472(2.1169) Grad: 352262.4375  LR: 0.00000100
Epoch: [1][194/195] Elapsed 4m 8s (remain 0m 0s) Loss: 0.3427(1.5399) Grad: 205010.5938  LR: 0.00000099
EVAL: [0/25] Elapsed 0m 1s (remain 0m 46s) Loss: 0.2871(0.2871)
[2022-10-27 19:21:55] - Epoch 1 - avg_train_loss: 1.5399  avg_val_loss: 0.3445  time: 290s
[2022-10-27 19:21:55] - Epoch 1 - Score: 0.8753  Scores: [0.7919857801916482, 0.9665085946918873, 0.786747455644519, 0.8708971601382153, 0.9415894246896547, 0.8941090204536817]
[2022-10-27 19:21:55] - Epoch 1 - Save Best Score: 0.8753 Model
EVAL: [24/25] Elapsed 0m 41s (remain 0m 0s) Loss: 0.2358(0.3445)
Epoch: [2][0/195] Elapsed 0m 1s (remain 6m 10s) Loss: 0.4705(0.4705) Grad: 489211.4688  LR: 0.00000099
Epoch: [2][100/195] Elapsed 2m 10s (remain 2m 1s) Loss: 0.1492(0.1955) Grad: 118321.0000  LR: 0.00000099
Epoch: [2][194/195] Elapsed 4m 4s (remain 0m 0s) Loss: 0.1541(0.1717) Grad: 159500.3906  LR: 0.00000093
EVAL: [0/25] Elapsed 0m 1s (remain 0m 46s) Loss: 0.1128(0.1128)
[2022-10-27 19:26:41] - Epoch 2 - avg_train_loss: 0.1717  avg_val_loss: 0.1433  time: 285s
[2022-10-27 19:26:41] - Epoch 2 - Score: 0.5395  Scores: [0.5600701167882236, 0.5418544774214981, 0.4734221652889825, 0.5074031890181856, 0.5644310056299953, 0.5899851555254565]
[2022-10-27 19:26:41] - Epoch 2 - Save Best Score: 0.5395 Model
EVAL: [24/25] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0864(0.1433)
Epoch: [3][0/195] Elapsed 0m 1s (remain 3m 42s) Loss: 0.2172(0.2172) Grad: 436489.8438  LR: 0.00000095
Epoch: [3][100/195] Elapsed 2m 5s (remain 1m 57s) Loss: 0.1462(0.1270) Grad: 181831.9688  LR: 0.00000095
Epoch: [3][194/195] Elapsed 4m 3s (remain 0m 0s) Loss: 0.1009(0.1253) Grad: 151348.0781  LR: 0.00000084
EVAL: [0/25] Elapsed 0m 1s (remain 0m 46s) Loss: 0.1032(0.1032)
[2022-10-27 19:31:28] - Epoch 3 - avg_train_loss: 0.1253  avg_val_loss: 0.1246  time: 285s
[2022-10-27 19:31:28] - Epoch 3 - Score: 0.5013  Scores: [0.5367567822430022, 0.4989640081614276, 0.45254991886332124, 0.471710829862708, 0.5180385174708736, 0.5295535361897127]
[2022-10-27 19:31:28] - Epoch 3 - Save Best Score: 0.5013 Model
EVAL: [24/25] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0772(0.1246)
Epoch: [4][0/195] Elapsed 0m 1s (remain 5m 27s) Loss: 0.0810(0.0810) Grad: 100506.4609  LR: 0.00000086
Epoch: [4][100/195] Elapsed 2m 5s (remain 1m 56s) Loss: 0.0924(0.1194) Grad: 129923.7188  LR: 0.00000086
Epoch: [4][194/195] Elapsed 4m 2s (remain 0m 0s) Loss: 0.0768(0.1163) Grad: 107439.1875  LR: 0.00000072
EVAL: [0/25] Elapsed 0m 1s (remain 0m 45s) Loss: 0.1006(0.1006)
[2022-10-27 19:36:12] - Epoch 4 - avg_train_loss: 0.1163  avg_val_loss: 0.1201  time: 283s
[2022-10-27 19:36:12] - Epoch 4 - Score: 0.4919  Scores: [0.5267800311497391, 0.4904287251580082, 0.44606751413098306, 0.46309155502139315, 0.5134198016572424, 0.51185133359789]
[2022-10-27 19:36:12] - Epoch 4 - Save Best Score: 0.4919 Model
EVAL: [24/25] Elapsed 0m 40s (remain 0m 0s) Loss: 0.0719(0.1201)
