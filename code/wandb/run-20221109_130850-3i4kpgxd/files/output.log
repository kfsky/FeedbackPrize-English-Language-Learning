Downloading tokenizer_config.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 52.0/52.0 [00:00<00:00, 64.0kB/s]
Downloading config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 579/579 [00:00<00:00, 943kB/s]
Downloading spm.model: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.35M/2.35M [00:00<00:00, 70.1MB/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3911/3911 [00:03<00:00, 985.26it/s]
[2022-11-09 13:08:57] - max_len: 2048
[2022-11-09 13:08:57] - ========== fold: 0 training ==========
[2022-11-09 13:08:57] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}


Downloading pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 354M/354M [00:05<00:00, 71.8MB/s]
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Reinitializing Last 1 Layers ...
Done.!
Epoch: [1][0/391] Elapsed 0m 1s (remain 10m 33s) Loss: 2.6500(2.6500) Grad: inf  LR: 0.00002994
Epoch: [1][100/391] Elapsed 0m 53s (remain 2m 32s) Loss: 0.1288(0.3363) Grad: 79331.8438  LR: 0.00002994
Epoch: [1][200/391] Elapsed 1m 44s (remain 1m 38s) Loss: 0.1074(0.2389) Grad: 113408.0547  LR: 0.00002994
Epoch: [1][300/391] Elapsed 2m 38s (remain 0m 47s) Loss: 0.1988(0.1989) Grad: 98132.7812  LR: 0.00002994
Epoch: [1][390/391] Elapsed 3m 28s (remain 0m 0s) Loss: 0.0804(0.1797) Grad: 66828.7891  LR: 0.00002821
EVAL: [0/49] Elapsed 0m 0s (remain 0m 35s) Loss: 0.1394(0.1394)
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.1113(0.1253)
[2022-11-09 13:13:06] - Epoch 1 - avg_train_loss: 0.1797  avg_val_loss: 0.1253  time: 240s
[2022-11-09 13:13:06] - Epoch 1 - Score: 0.5022  Scores: [0.5382973127510136, 0.4618850370709002, 0.48847928577796096, 0.48666593971278105, 0.5132720273055319, 0.5246881680458008]
[2022-11-09 13:13:06] - Epoch 1 - Save Best Score: 0.5022 Model
Epoch: [2][0/391] Elapsed 0m 0s (remain 4m 43s) Loss: 0.1285(0.1285) Grad: 179640.5312  LR: 0.00002864
Epoch: [2][100/391] Elapsed 0m 53s (remain 2m 33s) Loss: 0.1515(0.1114) Grad: 306873.3125  LR: 0.00002864
Epoch: [2][200/391] Elapsed 1m 47s (remain 1m 41s) Loss: 0.1126(0.1039) Grad: 219947.6562  LR: 0.00002864
Epoch: [2][300/391] Elapsed 2m 42s (remain 0m 48s) Loss: 0.1009(0.1058) Grad: 130758.2031  LR: 0.00002864
Epoch: [2][390/391] Elapsed 3m 27s (remain 0m 0s) Loss: 0.0737(0.1076) Grad: 97774.4297  LR: 0.00002231
EVAL: [0/49] Elapsed 0m 0s (remain 0m 34s) Loss: 0.1430(0.1430)
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.1221(0.1157)
[2022-11-09 13:17:06] - Epoch 2 - avg_train_loss: 0.1076  avg_val_loss: 0.1157  time: 239s
[2022-11-09 13:17:06] - Epoch 2 - Score: 0.4819  Scores: [0.5413054989191329, 0.4732249208636767, 0.4593826247118905, 0.5090818714897196, 0.4623528757708846, 0.4459476200159619]
[2022-11-09 13:17:06] - Epoch 2 - Save Best Score: 0.4819 Model
Epoch: [3][0/391] Elapsed 0m 0s (remain 4m 7s) Loss: 0.1006(0.1006) Grad: 225103.5938  LR: 0.00002312
Epoch: [3][100/391] Elapsed 0m 54s (remain 2m 35s) Loss: 0.1076(0.1048) Grad: 132839.0625  LR: 0.00002312
Epoch: [3][200/391] Elapsed 1m 47s (remain 1m 41s) Loss: 0.1517(0.1043) Grad: 143604.1406  LR: 0.00002312
Epoch: [3][300/391] Elapsed 2m 36s (remain 0m 46s) Loss: 0.1374(0.1042) Grad: 253034.9531  LR: 0.00002312
Epoch: [3][390/391] Elapsed 3m 24s (remain 0m 0s) Loss: 0.0641(0.1047) Grad: 91875.0703  LR: 0.00001417
EVAL: [0/49] Elapsed 0m 0s (remain 0m 35s) Loss: 0.1226(0.1226)
[2022-11-09 13:21:03] - Epoch 3 - avg_train_loss: 0.1047  avg_val_loss: 0.1022  time: 236s
[2022-11-09 13:21:03] - Epoch 3 - Score: 0.4523  Scores: [0.4882924471317712, 0.46931579238637045, 0.40202626977263617, 0.4542948927447451, 0.45626975320712504, 0.44373060945521886]
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.1032(0.1022)
[2022-11-09 13:21:03] - Epoch 3 - Save Best Score: 0.4523 Model
Epoch: [4][0/391] Elapsed 0m 0s (remain 3m 32s) Loss: 0.1078(0.1078) Grad: 114665.7031  LR: 0.00001511
Epoch: [4][100/391] Elapsed 0m 54s (remain 2m 37s) Loss: 0.0795(0.1032) Grad: 96378.8828  LR: 0.00001511
Epoch: [4][200/391] Elapsed 1m 46s (remain 1m 40s) Loss: 0.1093(0.1002) Grad: 120290.1016  LR: 0.00001511
Epoch: [4][300/391] Elapsed 2m 37s (remain 0m 47s) Loss: 0.0966(0.1000) Grad: 175410.5625  LR: 0.00001511
Epoch: [4][390/391] Elapsed 3m 28s (remain 0m 0s) Loss: 0.1508(0.1011) Grad: 270217.2500  LR: 0.00000633
EVAL: [0/49] Elapsed 0m 0s (remain 0m 34s) Loss: 0.1283(0.1283)
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.1022(0.1011)
[2022-11-09 13:25:04] - Epoch 4 - avg_train_loss: 0.1011  avg_val_loss: 0.1011  time: 239s
[2022-11-09 13:25:04] - Epoch 4 - Score: 0.4499  Scores: [0.4902285456362887, 0.4608556691505904, 0.4054188098165776, 0.45163730476013914, 0.4510064156085141, 0.4404938431098832]
[2022-11-09 13:25:04] - Epoch 4 - Save Best Score: 0.4499 Model
Epoch: [5][0/391] Elapsed 0m 1s (remain 6m 40s) Loss: 0.1697(0.1697) Grad: 637417.8750  LR: 0.00000711
Epoch: [5][100/391] Elapsed 0m 55s (remain 2m 40s) Loss: 0.0938(0.1005) Grad: 182986.0000  LR: 0.00000711
Epoch: [5][200/391] Elapsed 1m 47s (remain 1m 41s) Loss: 0.0764(0.1006) Grad: 106680.7891  LR: 0.00000711
Epoch: [5][300/391] Elapsed 2m 39s (remain 0m 47s) Loss: 0.0801(0.0991) Grad: 220671.3281  LR: 0.00000711
Epoch: [5][390/391] Elapsed 3m 29s (remain 0m 0s) Loss: 0.0975(0.0992) Grad: 144782.7969  LR: 0.00000124
EVAL: [0/49] Elapsed 0m 0s (remain 0m 35s) Loss: 0.1183(0.1183)
[2022-11-09 13:29:05] - Epoch 5 - avg_train_loss: 0.0992  avg_val_loss: 0.1015  time: 240s
[2022-11-09 13:29:05] - Epoch 5 - Score: 0.4510  Scores: [0.4831738945539015, 0.4607404081103803, 0.41147428344928905, 0.45414767085876595, 0.4521267974931773, 0.4441503366530464]
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.1060(0.1015)
[2022-11-09 13:29:06] - ========== fold: 0 result ==========
[2022-11-09 13:29:06] - Score: 0.4499  Scores: [0.4902285456362887, 0.4608556691505904, 0.4054188098165776, 0.45163730476013914, 0.4510064156085141, 0.4404938431098832]
[2022-11-09 13:29:06] - ========== fold: 1 training ==========
[2022-11-09 13:29:06] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Reinitializing Last 1 Layers ...
Done.!
Epoch: [1][0/391] Elapsed 0m 0s (remain 4m 29s) Loss: 2.2790(2.2790) Grad: inf  LR: 0.00002994
Epoch: [1][100/391] Elapsed 0m 56s (remain 2m 43s) Loss: 0.0918(0.3389) Grad: 147249.6406  LR: 0.00002994
Epoch: [1][200/391] Elapsed 1m 51s (remain 1m 45s) Loss: 0.1324(0.2328) Grad: 64342.9883  LR: 0.00002994
Epoch: [1][300/391] Elapsed 2m 45s (remain 0m 49s) Loss: 0.1301(0.1937) Grad: 93326.7734  LR: 0.00002994
Epoch: [1][390/391] Elapsed 3m 30s (remain 0m 0s) Loss: 0.0764(0.1747) Grad: 48362.7344  LR: 0.00002821
EVAL: [0/49] Elapsed 0m 1s (remain 0m 59s) Loss: 0.1116(0.1116)
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.1210(0.1179)
[2022-11-09 13:33:09] - Epoch 1 - avg_train_loss: 0.1747  avg_val_loss: 0.1179  time: 241s
[2022-11-09 13:33:09] - Epoch 1 - Score: 0.4867  Scores: [0.5169504752645849, 0.47721639850260145, 0.43558422464432234, 0.5241531683577414, 0.49491453683668685, 0.47108590790354604]
[2022-11-09 13:33:09] - Epoch 1 - Save Best Score: 0.4867 Model
Epoch: [2][0/391] Elapsed 0m 0s (remain 6m 28s) Loss: 0.0979(0.0979) Grad: 129713.3438  LR: 0.00002864
Epoch: [2][100/391] Elapsed 0m 55s (remain 2m 39s) Loss: 0.1052(0.1040) Grad: 134060.4688  LR: 0.00002864
Epoch: [2][200/391] Elapsed 1m 49s (remain 1m 43s) Loss: 0.0946(0.1044) Grad: 282136.1250  LR: 0.00002864
Epoch: [2][300/391] Elapsed 2m 42s (remain 0m 48s) Loss: 0.0773(0.1056) Grad: 129330.4766  LR: 0.00002864
Epoch: [2][390/391] Elapsed 3m 29s (remain 0m 0s) Loss: 0.0877(0.1057) Grad: 132409.9219  LR: 0.00002231
EVAL: [0/49] Elapsed 0m 1s (remain 0m 59s) Loss: 0.1156(0.1156)
EVAL: [48/49] Elapsed 0m 31s (remain 0m 0s) Loss: 0.1320(0.1198)
[2022-11-09 13:37:11] - Epoch 2 - avg_train_loss: 0.1057  avg_val_loss: 0.1198  time: 241s
[2022-11-09 13:37:11] - Epoch 2 - Score: 0.4916  Scores: [0.5087934213038539, 0.5113336747743843, 0.48253925031382217, 0.4916989621255121, 0.4805088421042715, 0.47469038718292805]
Epoch: [3][0/391] Elapsed 0m 0s (remain 3m 20s) Loss: 0.1173(0.1173) Grad: 190500.3906  LR: 0.00002312
Epoch: [3][100/391] Elapsed 0m 54s (remain 2m 35s) Loss: 0.1287(0.1069) Grad: 135966.7656  LR: 0.00002312
Epoch: [3][200/391] Elapsed 1m 48s (remain 1m 42s) Loss: 0.1083(0.1035) Grad: 310068.5000  LR: 0.00002312
Epoch: [3][300/391] Elapsed 2m 39s (remain 0m 47s) Loss: 0.1355(0.1033) Grad: 206328.2031  LR: 0.00002312
Epoch: [3][390/391] Elapsed 3m 28s (remain 0m 0s) Loss: 0.1028(0.1023) Grad: 230711.8750  LR: 0.00001417
EVAL: [0/49] Elapsed 0m 1s (remain 0m 58s) Loss: 0.0955(0.0955)
EVAL: [48/49] Elapsed 0m 31s (remain 0m 0s) Loss: 0.1122(0.1052)
[2022-11-09 13:41:11] - Epoch 3 - avg_train_loss: 0.1023  avg_val_loss: 0.1052  time: 240s
[2022-11-09 13:41:11] - Epoch 3 - Score: 0.4598  Scores: [0.4941228806157016, 0.4443527746147676, 0.42278048745765606, 0.4810537046735799, 0.47065547103472005, 0.4455527474669075]
[2022-11-09 13:41:11] - Epoch 3 - Save Best Score: 0.4598 Model
Epoch: [4][0/391] Elapsed 0m 0s (remain 6m 1s) Loss: 0.1244(0.1244) Grad: 374148.3438  LR: 0.00001511
Epoch: [4][100/391] Elapsed 0m 56s (remain 2m 40s) Loss: 0.0715(0.0927) Grad: 209285.9844  LR: 0.00001511
Epoch: [4][200/391] Elapsed 1m 49s (remain 1m 43s) Loss: 0.0734(0.0941) Grad: 104120.8281  LR: 0.00001511
Epoch: [4][300/391] Elapsed 2m 41s (remain 0m 48s) Loss: 0.0874(0.0963) Grad: 147379.8906  LR: 0.00001511
Epoch: [4][390/391] Elapsed 3m 29s (remain 0m 0s) Loss: 0.0793(0.0975) Grad: 148715.7656  LR: 0.00000633
EVAL: [0/49] Elapsed 0m 1s (remain 1m 0s) Loss: 0.0886(0.0886)
EVAL: [48/49] Elapsed 0m 31s (remain 0m 0s) Loss: 0.1088(0.1066)
[2022-11-09 13:45:14] - Epoch 4 - avg_train_loss: 0.0975  avg_val_loss: 0.1066  time: 241s
[2022-11-09 13:45:14] - Epoch 4 - Score: 0.4629  Scores: [0.5000805995436718, 0.44829025451254, 0.42946904274857506, 0.4805712945024776, 0.4702243788825282, 0.4488665102249536]
Epoch: [5][0/391] Elapsed 0m 0s (remain 5m 18s) Loss: 0.1547(0.1547) Grad: 520363.6250  LR: 0.00000711
Epoch: [5][100/391] Elapsed 0m 57s (remain 2m 45s) Loss: 0.0867(0.0894) Grad: 182689.4531  LR: 0.00000711
Epoch: [5][200/391] Elapsed 1m 48s (remain 1m 42s) Loss: 0.0910(0.0915) Grad: 133881.8594  LR: 0.00000711
Epoch: [5][300/391] Elapsed 2m 44s (remain 0m 49s) Loss: 0.0823(0.0930) Grad: 185401.6250  LR: 0.00000711
Epoch: [5][390/391] Elapsed 3m 31s (remain 0m 0s) Loss: 0.0645(0.0921) Grad: 77227.3672  LR: 0.00000124
EVAL: [0/49] Elapsed 0m 1s (remain 0m 59s) Loss: 0.0960(0.0960)
EVAL: [48/49] Elapsed 0m 31s (remain 0m 0s) Loss: 0.1164(0.1085)
[2022-11-09 13:49:16] - Epoch 5 - avg_train_loss: 0.0921  avg_val_loss: 0.1085  time: 243s
[2022-11-09 13:49:16] - Epoch 5 - Score: 0.4671  Scores: [0.49266087935711445, 0.4575873376669204, 0.43100918045689846, 0.48166021202148407, 0.4820977150146255, 0.4573001111063827]
[2022-11-09 13:49:17] - ========== fold: 1 result ==========
[2022-11-09 13:49:17] - Score: 0.4598  Scores: [0.4941228806157016, 0.4443527746147676, 0.42278048745765606, 0.4810537046735799, 0.47065547103472005, 0.4455527474669075]
[2022-11-09 13:49:17] - ========== fold: 2 training ==========
[2022-11-09 13:49:17] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Reinitializing Last 1 Layers ...
Done.!
Epoch: [1][0/391] Elapsed 0m 0s (remain 3m 47s) Loss: 2.1833(2.1833) Grad: inf  LR: 0.00002994
Epoch: [1][100/391] Elapsed 0m 51s (remain 2m 26s) Loss: 0.1085(0.2976) Grad: 126234.1562  LR: 0.00002994
Epoch: [1][200/391] Elapsed 1m 43s (remain 1m 37s) Loss: 0.0900(0.2124) Grad: 93395.4219  LR: 0.00002994
Epoch: [1][300/391] Elapsed 2m 40s (remain 0m 47s) Loss: 0.1220(0.1834) Grad: 181093.9375  LR: 0.00002994
Epoch: [1][390/391] Elapsed 3m 26s (remain 0m 0s) Loss: 0.1916(0.1688) Grad: 370258.7500  LR: 0.00002821
EVAL: [0/49] Elapsed 0m 0s (remain 0m 33s) Loss: 0.0845(0.0845)
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0780(0.1142)
[2022-11-09 13:53:18] - Epoch 1 - avg_train_loss: 0.1688  avg_val_loss: 0.1142  time: 239s
[2022-11-09 13:53:18] - Epoch 1 - Score: 0.4788  Scores: [0.5323667558092575, 0.4733995272805489, 0.43018219406213287, 0.4568211228269611, 0.5070486478849808, 0.47316325765476397]
[2022-11-09 13:53:18] - Epoch 1 - Save Best Score: 0.4788 Model
Epoch: [2][0/391] Elapsed 0m 0s (remain 5m 21s) Loss: 0.1419(0.1419) Grad: 250245.4688  LR: 0.00002864
Epoch: [2][100/391] Elapsed 0m 55s (remain 2m 38s) Loss: 0.1299(0.1122) Grad: 169046.0938  LR: 0.00002864
Epoch: [2][200/391] Elapsed 1m 47s (remain 1m 41s) Loss: 0.0879(0.1110) Grad: 192838.6562  LR: 0.00002864
Epoch: [2][300/391] Elapsed 2m 39s (remain 0m 47s) Loss: 0.1307(0.1092) Grad: 196257.9844  LR: 0.00002864
Epoch: [2][390/391] Elapsed 3m 28s (remain 0m 0s) Loss: 0.1071(0.1082) Grad: 154318.4375  LR: 0.00002231
EVAL: [0/49] Elapsed 0m 0s (remain 0m 32s) Loss: 0.0886(0.0886)
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0736(0.1099)
[2022-11-09 13:57:20] - Epoch 2 - avg_train_loss: 0.1082  avg_val_loss: 0.1099  time: 241s
[2022-11-09 13:57:20] - Epoch 2 - Score: 0.4704  Scores: [0.5122108118552241, 0.47442840474306625, 0.43732771839654283, 0.4467128137990993, 0.47931283224184096, 0.4723674768327329]
[2022-11-09 13:57:20] - Epoch 2 - Save Best Score: 0.4704 Model
Epoch: [3][0/391] Elapsed 0m 0s (remain 3m 51s) Loss: 0.0734(0.0734) Grad: 84968.9375  LR: 0.00002312
Epoch: [3][100/391] Elapsed 0m 53s (remain 2m 34s) Loss: 0.1111(0.1035) Grad: 250350.9688  LR: 0.00002312
Epoch: [3][200/391] Elapsed 1m 46s (remain 1m 40s) Loss: 0.0773(0.1035) Grad: 133397.5938  LR: 0.00002312
Epoch: [3][300/391] Elapsed 2m 42s (remain 0m 48s) Loss: 0.1184(0.1047) Grad: 171248.0469  LR: 0.00002312
Epoch: [3][390/391] Elapsed 3m 28s (remain 0m 0s) Loss: 0.1301(0.1044) Grad: 216063.0781  LR: 0.00001417
EVAL: [0/49] Elapsed 0m 0s (remain 0m 34s) Loss: 0.0819(0.0819)
[2022-11-09 14:01:23] - Epoch 3 - avg_train_loss: 0.1044  avg_val_loss: 0.1085  time: 242s
[2022-11-09 14:01:23] - Epoch 3 - Score: 0.4669  Scores: [0.5062916320544649, 0.4621909347120193, 0.41929317152581963, 0.4640446880335079, 0.47886560252706445, 0.4709844826776696]
[2022-11-09 14:01:23] - Epoch 3 - Save Best Score: 0.4669 Model
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0777(0.1085)
Epoch: [4][0/391] Elapsed 0m 0s (remain 5m 34s) Loss: 0.1068(0.1068) Grad: 156629.3594  LR: 0.00001511
Epoch: [4][100/391] Elapsed 0m 55s (remain 2m 39s) Loss: 0.0932(0.0996) Grad: 177358.4219  LR: 0.00001511
Epoch: [4][200/391] Elapsed 1m 45s (remain 1m 39s) Loss: 0.1223(0.1008) Grad: 183279.1250  LR: 0.00001511
Epoch: [4][300/391] Elapsed 2m 37s (remain 0m 47s) Loss: 0.0986(0.0997) Grad: 155477.2500  LR: 0.00001511
Epoch: [4][390/391] Elapsed 3m 25s (remain 0m 0s) Loss: 0.1285(0.0993) Grad: 303026.0000  LR: 0.00000633
EVAL: [0/49] Elapsed 0m 0s (remain 0m 34s) Loss: 0.0762(0.0762)
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0768(0.1066)
[2022-11-09 14:05:23] - Epoch 4 - avg_train_loss: 0.0993  avg_val_loss: 0.1066  time: 238s
[2022-11-09 14:05:23] - Epoch 4 - Score: 0.4629  Scores: [0.5011244831471712, 0.4670196428918657, 0.4168838769639674, 0.4432615122703618, 0.4863965852003539, 0.4626360256061565]
[2022-11-09 14:05:23] - Epoch 4 - Save Best Score: 0.4629 Model
Epoch: [5][0/391] Elapsed 0m 0s (remain 3m 11s) Loss: 0.0722(0.0722) Grad: 101175.1328  LR: 0.00000711
Epoch: [5][100/391] Elapsed 0m 52s (remain 2m 31s) Loss: 0.0929(0.0947) Grad: 98522.7344  LR: 0.00000711
Epoch: [5][200/391] Elapsed 1m 47s (remain 1m 42s) Loss: 0.0897(0.0963) Grad: 134085.2812  LR: 0.00000711
Epoch: [5][300/391] Elapsed 2m 42s (remain 0m 48s) Loss: 0.1588(0.0959) Grad: 139754.8281  LR: 0.00000711
Epoch: [5][390/391] Elapsed 3m 28s (remain 0m 0s) Loss: 0.0678(0.0960) Grad: 118452.0312  LR: 0.00000124
EVAL: [0/49] Elapsed 0m 0s (remain 0m 33s) Loss: 0.0743(0.0743)
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0735(0.1074)
[2022-11-09 14:09:26] - Epoch 5 - avg_train_loss: 0.0960  avg_val_loss: 0.1074  time: 242s
[2022-11-09 14:09:26] - Epoch 5 - Score: 0.4644  Scores: [0.4998454728023616, 0.49222948869292, 0.41870563418513346, 0.4408564662471414, 0.4734635702109846, 0.4612694095484737]
[2022-11-09 14:09:27] - ========== fold: 2 result ==========
[2022-11-09 14:09:27] - Score: 0.4629  Scores: [0.5011244831471712, 0.4670196428918657, 0.4168838769639674, 0.4432615122703618, 0.4863965852003539, 0.4626360256061565]
[2022-11-09 14:09:27] - ========== fold: 3 training ==========
[2022-11-09 14:09:27] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Reinitializing Last 1 Layers ...
Done.!
Epoch: [1][0/391] Elapsed 0m 0s (remain 3m 57s) Loss: 2.6954(2.6954) Grad: inf  LR: 0.00002994
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][100/391] Elapsed 0m 54s (remain 2m 37s) Loss: 0.0902(0.3397) Grad: 86015.0469  LR: 0.00002994
Epoch: [1][200/391] Elapsed 1m 48s (remain 1m 42s) Loss: 0.1019(0.2369) Grad: 233412.9688  LR: 0.00002994
Epoch: [1][300/391] Elapsed 2m 42s (remain 0m 48s) Loss: 0.1656(0.1968) Grad: 138602.7344  LR: 0.00002994
Epoch: [1][390/391] Elapsed 3m 28s (remain 0m 0s) Loss: 0.1018(0.1813) Grad: 79123.0859  LR: 0.00002821
EVAL: [0/49] Elapsed 0m 1s (remain 0m 56s) Loss: 0.1400(0.1400)
[2022-11-09 14:13:31] - Epoch 1 - avg_train_loss: 0.1813  avg_val_loss: 0.1152  time: 242s
[2022-11-09 14:13:31] - Epoch 1 - Score: 0.4801  Scores: [0.5268653997865058, 0.4518186841064095, 0.4359219294390496, 0.4709404632001376, 0.5467026301839804, 0.4485223994992039]
EVAL: [48/49] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0802(0.1152)
[2022-11-09 14:13:31] - Epoch 1 - Save Best Score: 0.4801 Model
Epoch: [2][0/391] Elapsed 0m 0s (remain 4m 30s) Loss: 0.0856(0.0856) Grad: 143262.0312  LR: 0.00002864
Epoch: [2][100/391] Elapsed 0m 51s (remain 2m 26s) Loss: 0.2151(0.1079) Grad: 352351.6562  LR: 0.00002864
Epoch: [2][200/391] Elapsed 1m 40s (remain 1m 35s) Loss: 0.1838(0.1060) Grad: 459057.6250  LR: 0.00002864
Epoch: [2][300/391] Elapsed 2m 35s (remain 0m 46s) Loss: 0.0958(0.1043) Grad: 208646.1250  LR: 0.00002864
Epoch: [2][390/391] Elapsed 3m 25s (remain 0m 0s) Loss: 0.0879(0.1059) Grad: 285633.5938  LR: 0.00002231
EVAL: [0/49] Elapsed 0m 1s (remain 0m 55s) Loss: 0.1284(0.1284)
EVAL: [48/49] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0781(0.1107)
[2022-11-09 14:17:31] - Epoch 2 - avg_train_loss: 0.1059  avg_val_loss: 0.1107  time: 239s
[2022-11-09 14:17:31] - Epoch 2 - Score: 0.4715  Scores: [0.4851439677000173, 0.45494916447168615, 0.4395252292860807, 0.5060692521803569, 0.4924265278516326, 0.450667547725397]
[2022-11-09 14:17:31] - Epoch 2 - Save Best Score: 0.4715 Model
Epoch: [3][0/391] Elapsed 0m 1s (remain 8m 1s) Loss: 0.0871(0.0871) Grad: 162711.9062  LR: 0.00002312
Epoch: [3][100/391] Elapsed 0m 53s (remain 2m 33s) Loss: 0.1600(0.1019) Grad: 122241.8203  LR: 0.00002312
Epoch: [3][200/391] Elapsed 1m 48s (remain 1m 42s) Loss: 0.1301(0.1009) Grad: 132236.7188  LR: 0.00002312
Epoch: [3][300/391] Elapsed 2m 39s (remain 0m 47s) Loss: 0.0729(0.0998) Grad: 194775.1719  LR: 0.00002312
Epoch: [3][390/391] Elapsed 3m 27s (remain 0m 0s) Loss: 0.0853(0.1006) Grad: 121528.7500  LR: 0.00001417
EVAL: [0/49] Elapsed 0m 1s (remain 0m 56s) Loss: 0.1355(0.1355)
[2022-11-09 14:21:33] - Epoch 3 - avg_train_loss: 0.1006  avg_val_loss: 0.1047  time: 241s
[2022-11-09 14:21:33] - Epoch 3 - Score: 0.4582  Scores: [0.4769727108994607, 0.4382880631856899, 0.43033638651945394, 0.46254441541756386, 0.4899599546839603, 0.45137474309420716]
EVAL: [48/49] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0700(0.1047)
[2022-11-09 14:21:33] - Epoch 3 - Save Best Score: 0.4582 Model
Epoch: [4][0/391] Elapsed 0m 0s (remain 4m 51s) Loss: 0.1393(0.1393) Grad: 234360.8281  LR: 0.00001511
Epoch: [4][100/391] Elapsed 0m 51s (remain 2m 27s) Loss: 0.1531(0.0981) Grad: 309935.0625  LR: 0.00001511
Epoch: [4][200/391] Elapsed 1m 41s (remain 1m 35s) Loss: 0.0669(0.0977) Grad: 137874.2031  LR: 0.00001511
Epoch: [4][300/391] Elapsed 2m 36s (remain 0m 46s) Loss: 0.0564(0.0964) Grad: 95551.5391  LR: 0.00001511
Epoch: [4][390/391] Elapsed 3m 29s (remain 0m 0s) Loss: 0.0804(0.0972) Grad: 208747.7500  LR: 0.00000633
EVAL: [0/49] Elapsed 0m 1s (remain 0m 56s) Loss: 0.1266(0.1266)
EVAL: [48/49] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0699(0.1032)
[2022-11-09 14:25:37] - Epoch 4 - avg_train_loss: 0.0972  avg_val_loss: 0.1032  time: 243s
[2022-11-09 14:25:37] - Epoch 4 - Score: 0.4550  Scores: [0.47831003519093285, 0.44026172797331486, 0.4233031987624751, 0.45768289773049453, 0.48863413884939366, 0.4418403977003748]
[2022-11-09 14:25:37] - Epoch 4 - Save Best Score: 0.4550 Model
Epoch: [5][0/391] Elapsed 0m 0s (remain 6m 2s) Loss: 0.1349(0.1349) Grad: 260857.7188  LR: 0.00000711
Epoch: [5][100/391] Elapsed 0m 55s (remain 2m 38s) Loss: 0.1098(0.0873) Grad: 207618.7031  LR: 0.00000711
Epoch: [5][200/391] Elapsed 1m 47s (remain 1m 41s) Loss: 0.0733(0.0904) Grad: 129388.2734  LR: 0.00000711
Epoch: [5][300/391] Elapsed 2m 41s (remain 0m 48s) Loss: 0.1056(0.0909) Grad: 99519.8906  LR: 0.00000711
Epoch: [5][390/391] Elapsed 3m 27s (remain 0m 0s) Loss: 0.0749(0.0927) Grad: 195182.2188  LR: 0.00000124
EVAL: [0/49] Elapsed 0m 1s (remain 0m 57s) Loss: 0.1289(0.1289)
EVAL: [48/49] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0753(0.1100)
[2022-11-09 14:29:40] - Epoch 5 - avg_train_loss: 0.0927  avg_val_loss: 0.1100  time: 241s
[2022-11-09 14:29:40] - Epoch 5 - Score: 0.4703  Scores: [0.478558027403754, 0.4621432046885442, 0.44725451052342635, 0.48348258905714697, 0.5018808761599054, 0.44832829161054877]
[2022-11-09 14:29:40] - ========== fold: 3 result ==========
[2022-11-09 14:29:40] - Score: 0.4550  Scores: [0.47831003519093285, 0.44026172797331486, 0.4233031987624751, 0.45768289773049453, 0.48863413884939366, 0.4418403977003748]
[2022-11-09 14:29:40] - ========== fold: 4 training ==========
[2022-11-09 14:29:40] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Reinitializing Last 1 Layers ...
Done.!
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/391] Elapsed 0m 1s (remain 6m 40s) Loss: 2.7355(2.7355) Grad: inf  LR: 0.00002994
Epoch: [1][100/391] Elapsed 0m 54s (remain 2m 36s) Loss: 0.1696(0.3348) Grad: 114729.8516  LR: 0.00002994
Epoch: [1][200/391] Elapsed 1m 48s (remain 1m 42s) Loss: 0.1434(0.2284) Grad: 126830.5234  LR: 0.00002994
Epoch: [1][300/391] Elapsed 2m 40s (remain 0m 47s) Loss: 0.1945(0.1916) Grad: 243218.5938  LR: 0.00002994
Epoch: [1][390/391] Elapsed 3m 30s (remain 0m 0s) Loss: 0.1593(0.1750) Grad: 156231.3438  LR: 0.00002821
EVAL: [0/49] Elapsed 0m 1s (remain 1m 5s) Loss: 0.1133(0.1133)
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0920(0.1345)
[2022-11-09 14:33:45] - Epoch 1 - avg_train_loss: 0.1750  avg_val_loss: 0.1345  time: 243s
[2022-11-09 14:33:45] - Epoch 1 - Score: 0.5183  Scores: [0.6415989400072603, 0.43796127208432084, 0.47924671878284225, 0.5157419051067302, 0.5740541064208444, 0.4611423575493087]
[2022-11-09 14:33:45] - Epoch 1 - Save Best Score: 0.5183 Model
Epoch: [2][0/391] Elapsed 0m 0s (remain 4m 8s) Loss: 0.1150(0.1150) Grad: 152592.3125  LR: 0.00002864
Epoch: [2][100/391] Elapsed 0m 52s (remain 2m 30s) Loss: 0.0742(0.1098) Grad: 251794.5000  LR: 0.00002864
Epoch: [2][200/391] Elapsed 1m 44s (remain 1m 38s) Loss: 0.0925(0.1050) Grad: 181552.8125  LR: 0.00002864
Epoch: [2][300/391] Elapsed 2m 35s (remain 0m 46s) Loss: 0.1179(0.1058) Grad: 308657.3125  LR: 0.00002864
Epoch: [2][390/391] Elapsed 3m 27s (remain 0m 0s) Loss: 0.1199(0.1060) Grad: 238452.0156  LR: 0.00002231
EVAL: [0/49] Elapsed 0m 1s (remain 1m 4s) Loss: 0.1017(0.1017)
EVAL: [48/49] Elapsed 0m 31s (remain 0m 0s) Loss: 0.0773(0.1042)
[2022-11-09 14:37:45] - Epoch 2 - avg_train_loss: 0.1060  avg_val_loss: 0.1042  time: 239s
[2022-11-09 14:37:45] - Epoch 2 - Score: 0.4575  Scores: [0.48732700824572, 0.43320940546812686, 0.4305315292701296, 0.45627402587939714, 0.483431428156646, 0.4541695604386207]
[2022-11-09 14:37:45] - Epoch 2 - Save Best Score: 0.4575 Model
Epoch: [3][0/391] Elapsed 0m 0s (remain 6m 6s) Loss: 0.1241(0.1241) Grad: 182544.2031  LR: 0.00002312
Epoch: [3][100/391] Elapsed 0m 56s (remain 2m 41s) Loss: 0.0976(0.1049) Grad: 172639.5625  LR: 0.00002312
Epoch: [3][200/391] Elapsed 1m 50s (remain 1m 44s) Loss: 0.0648(0.1050) Grad: 89553.0469  LR: 0.00002312
Epoch: [3][300/391] Elapsed 2m 46s (remain 0m 49s) Loss: 0.1032(0.1049) Grad: 206614.5156  LR: 0.00002312
Epoch: [3][390/391] Elapsed 3m 30s (remain 0m 0s) Loss: 0.1363(0.1045) Grad: 139133.3594  LR: 0.00001417
EVAL: [0/49] Elapsed 0m 1s (remain 1m 5s) Loss: 0.1180(0.1180)
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.1125(0.1161)
[2022-11-09 14:41:49] - Epoch 3 - avg_train_loss: 0.1045  avg_val_loss: 0.1161  time: 242s
[2022-11-09 14:41:49] - Epoch 3 - Score: 0.4824  Scores: [0.5545701465231863, 0.43599521437192595, 0.43020984796507505, 0.47775939021108926, 0.5055550741915844, 0.49013859206289806]
Epoch: [4][0/391] Elapsed 0m 1s (remain 8m 30s) Loss: 0.1185(0.1185) Grad: 311416.5625  LR: 0.00001511
Epoch: [4][100/391] Elapsed 0m 52s (remain 2m 31s) Loss: 0.0809(0.1058) Grad: 129562.2109  LR: 0.00001511
Epoch: [4][200/391] Elapsed 1m 49s (remain 1m 43s) Loss: 0.0904(0.1009) Grad: 239160.0469  LR: 0.00001511
Epoch: [4][300/391] Elapsed 2m 42s (remain 0m 48s) Loss: 0.0817(0.1029) Grad: 119634.3672  LR: 0.00001511
Epoch: [4][390/391] Elapsed 3m 28s (remain 0m 0s) Loss: 0.0741(0.1025) Grad: 117177.1406  LR: 0.00000633
EVAL: [0/49] Elapsed 0m 1s (remain 1m 5s) Loss: 0.1084(0.1084)
EVAL: [48/49] Elapsed 0m 31s (remain 0m 0s) Loss: 0.0840(0.1033)
[2022-11-09 14:45:50] - Epoch 4 - avg_train_loss: 0.1025  avg_val_loss: 0.1033  time: 241s
[2022-11-09 14:45:50] - Epoch 4 - Score: 0.4557  Scores: [0.47956090330196954, 0.4376779571520787, 0.42580789362809673, 0.45875954373542854, 0.48158862484122544, 0.45051094347898135]
[2022-11-09 14:45:50] - Epoch 4 - Save Best Score: 0.4557 Model
Epoch: [5][0/391] Elapsed 0m 0s (remain 4m 0s) Loss: 0.0840(0.0840) Grad: 130034.0312  LR: 0.00000711
Epoch: [5][100/391] Elapsed 0m 53s (remain 2m 33s) Loss: 0.0904(0.0974) Grad: 194534.7188  LR: 0.00000711
Epoch: [5][200/391] Elapsed 1m 47s (remain 1m 41s) Loss: 0.0767(0.0980) Grad: 143864.0000  LR: 0.00000711
Epoch: [5][300/391] Elapsed 2m 42s (remain 0m 48s) Loss: 0.0958(0.0977) Grad: 203700.6875  LR: 0.00000711
Epoch: [5][390/391] Elapsed 3m 27s (remain 0m 0s) Loss: 0.1255(0.0981) Grad: 200892.7344  LR: 0.00000124
EVAL: [0/49] Elapsed 0m 1s (remain 1m 5s) Loss: 0.1010(0.1010)
EVAL: [48/49] Elapsed 0m 31s (remain 0m 0s) Loss: 0.0728(0.1054)
[2022-11-09 14:49:51] - Epoch 5 - avg_train_loss: 0.0981  avg_val_loss: 0.1054  time: 240s
[2022-11-09 14:49:51] - Epoch 5 - Score: 0.4602  Scores: [0.4771331100012431, 0.44332441327125177, 0.4302704992570323, 0.4787005987551952, 0.479509820877546, 0.4523535698579208]
[2022-11-09 14:49:52] - ========== fold: 4 result ==========
[2022-11-09 14:49:52] - Score: 0.4557  Scores: [0.47956090330196954, 0.4376779571520787, 0.42580789362809673, 0.45875954373542854, 0.48158862484122544, 0.45051094347898135]
[2022-11-09 14:49:52] - ========== CV ==========
[2022-11-09 14:49:52] - Score: 0.4568  Scores: [0.488748012978927, 0.45018449446371583, 0.41890381991672704, 0.4586568559802011, 0.47585494204940293, 0.4482776152066939]