Downloading tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:00<00:00, 160kB/s]
Downloading config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 701/701 [00:00<00:00, 559kB/s]
Downloading vocab.txt: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 226k/226k [00:00<00:00, 6.40MB/s]
Downloading tokenizer.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 455k/455k [00:00<00:00, 7.99MB/s]
Downloading special_tokens_map.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 153/153 [00:00<00:00, 174kB/s]
  0%|                                                                                                                                                                      | 0/3911 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors

100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3911/3911 [00:03<00:00, 1042.27it/s]
[2022-11-09 12:59:21] - max_len: 2048
[2022-11-09 12:59:21] - ========== fold: 0 training ==========
[2022-11-09 12:59:21] - FunnelConfig {
  "_name_or_path": "funnel-transformer/large",
  "activation_dropout": 0.0,
  "architectures": [
    "FunnelModel"
  ],
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "attention_type": "relative_shift",
  "block_repeats": [
    1,
    1,
    1
  ],
  "block_sizes": [
    8,
    8,
    8
  ],
  "d_head": 64,
  "d_inner": 4096,
  "d_model": 1024,
  "hidden_act": "gelu_new",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "initializer_range": 0.1,
  "initializer_std": null,
  "layer_norm_eps": 1e-09,
  "max_position_embeddings": 512,
  "model_type": "funnel",
  "n_head": 16,
  "num_decoder_layers": 2,
  "output_hidden_states": true,
  "pool_q_only": true,
  "pooling_type": "mean",
  "rel_attn_type": "factorized",
  "separate_cls": true,
  "transformers_version": "4.21.1",
  "truncate_seq": true,
  "type_vocab_size": 3,
  "vocab_size": 30522
}

























Downloading pytorch_model.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.44G/1.44G [00:50<00:00, 30.8MB/s]
Traceback (most recent call last):
  File "/notebooks/code/exp050.py", line 788, in <module>
    main()
  File "/notebooks/code/exp050.py", line 726, in main
    _oof_df = train_loop(train, fold)
  File "/notebooks/code/exp050.py", line 551, in train_loop
    model = CustomModel(CFG, config_path=None, pretrained=True)
  File "/notebooks/code/exp050.py", line 325, in __init__
    self.model.gradient_checkpointing_enable()
  File "/usr/local/lib/python3.9/dist-packages/transformers/modeling_utils.py", line 1418, in gradient_checkpointing_enable
    raise ValueError(f"{self.__class__.__name__} does not support gradient checkpointing.")
ValueError: FunnelModel does not support gradient checkpointing.