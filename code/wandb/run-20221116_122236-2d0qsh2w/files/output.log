

100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3911/3911 [00:03<00:00, 995.26it/s]
[2022-11-16 12:22:42] - comment: abhishek/deberta-v3-base-autotrain, LLRD 0.7, reinit=1 Attention Pooling, exp050=LLRD=0.8
[2022-11-16 12:22:42] - max_len: 2048
[2022-11-16 12:22:42] - ========== fold: 0 training ==========
[2022-11-16 12:22:42] - DebertaV2Config {
  "_name_or_path": "yevheniimaslov/deberta-v3-base-cola",
  "architectures": [
    "DebertaV2ForSequenceClassification"
  ],
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "torch_dtype": "float32",
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Reinitializing Last 1 Layers ...
Done.!
Some weights of the model checkpoint at yevheniimaslov/deberta-v3-base-cola were not used when initializing DebertaV2Model: ['classifier.bias', 'classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/391] Elapsed 0m 1s (remain 10m 1s) Loss: 2.5453(2.5453) Grad: inf  LR: 0.00002994
Epoch: [1][100/391] Elapsed 0m 53s (remain 2m 32s) Loss: 0.1688(0.3248) Grad: 444739.0938  LR: 0.00002994
Epoch: [1][200/391] Elapsed 1m 44s (remain 1m 39s) Loss: 0.1237(0.2293) Grad: 111298.9844  LR: 0.00002994
Epoch: [1][300/391] Elapsed 2m 38s (remain 0m 47s) Loss: 0.1779(0.1913) Grad: 97675.3984  LR: 0.00002994
Epoch: [1][390/391] Elapsed 3m 29s (remain 0m 0s) Loss: 0.0878(0.1757) Grad: 73793.7266  LR: 0.00002821
EVAL: [0/49] Elapsed 0m 0s (remain 0m 35s) Loss: 0.1506(0.1506)
[2022-11-16 12:26:46] - Epoch 1 - avg_train_loss: 0.1757  avg_val_loss: 0.1448  time: 240s
[2022-11-16 12:26:46] - Epoch 1 - Score: 0.5399  Scores: [0.5151677638982869, 0.4719980601910664, 0.5365987669855427, 0.6136907077027706, 0.5338667354332857, 0.5677799337650852]
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.1224(0.1448)
[2022-11-16 12:26:46] - Epoch 1 - Save Best Score: 0.5399 Model
Epoch: [2][0/391] Elapsed 0m 0s (remain 4m 32s) Loss: 0.1468(0.1468) Grad: 244057.2969  LR: 0.00002864
Epoch: [2][100/391] Elapsed 0m 53s (remain 2m 32s) Loss: 0.1458(0.1119) Grad: 309701.5625  LR: 0.00002864
Epoch: [2][200/391] Elapsed 1m 47s (remain 1m 41s) Loss: 0.1055(0.1047) Grad: 201637.0938  LR: 0.00002864
Epoch: [2][300/391] Elapsed 2m 42s (remain 0m 48s) Loss: 0.0908(0.1067) Grad: 114894.6328  LR: 0.00002864
Epoch: [2][390/391] Elapsed 3m 27s (remain 0m 0s) Loss: 0.0722(0.1082) Grad: 127178.0078  LR: 0.00002231
EVAL: [0/49] Elapsed 0m 0s (remain 0m 36s) Loss: 0.1416(0.1416)
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.1195(0.1177)
[2022-11-16 12:30:46] - Epoch 2 - avg_train_loss: 0.1082  avg_val_loss: 0.1177  time: 239s
[2022-11-16 12:30:46] - Epoch 2 - Score: 0.4861  Scores: [0.5324803447477703, 0.48319027245144175, 0.4580902511587716, 0.5295069369238311, 0.46242268932946445, 0.4511746432249517]
Epoch: [3][0/391] Elapsed 0m 0s (remain 4m 3s) Loss: 0.1150(0.1150) Grad: 243718.3281  LR: 0.00002312
[2022-11-16 12:30:46] - Epoch 2 - Save Best Score: 0.4861 Model
Epoch: [3][100/391] Elapsed 0m 54s (remain 2m 35s) Loss: 0.1233(0.1057) Grad: 139632.1406  LR: 0.00002312
Epoch: [3][200/391] Elapsed 1m 47s (remain 1m 42s) Loss: 0.1562(0.1048) Grad: 179029.9688  LR: 0.00002312
Epoch: [3][300/391] Elapsed 2m 36s (remain 0m 46s) Loss: 0.1465(0.1042) Grad: 422041.4375  LR: 0.00002312
Epoch: [3][390/391] Elapsed 3m 25s (remain 0m 0s) Loss: 0.0677(0.1049) Grad: 109515.9375  LR: 0.00001417
EVAL: [0/49] Elapsed 0m 0s (remain 0m 36s) Loss: 0.1201(0.1201)
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.0994(0.1034)
[2022-11-16 12:34:44] - Epoch 3 - avg_train_loss: 0.1049  avg_val_loss: 0.1034  time: 236s
[2022-11-16 12:34:44] - Epoch 3 - Score: 0.4552  Scores: [0.4960501485108767, 0.4698532107994096, 0.40709360368484077, 0.45350231538630564, 0.45733435624728686, 0.4473663921528246]
[2022-11-16 12:34:44] - Epoch 3 - Save Best Score: 0.4552 Model
Epoch: [4][0/391] Elapsed 0m 0s (remain 3m 29s) Loss: 0.1022(0.1022) Grad: 109144.5391  LR: 0.00001511
Epoch: [4][100/391] Elapsed 0m 54s (remain 2m 37s) Loss: 0.0830(0.1032) Grad: 114553.5000  LR: 0.00001511
Epoch: [4][200/391] Elapsed 1m 46s (remain 1m 40s) Loss: 0.1202(0.1015) Grad: 98097.0703  LR: 0.00001511
Epoch: [4][300/391] Elapsed 2m 38s (remain 0m 47s) Loss: 0.1023(0.1014) Grad: 276518.0625  LR: 0.00001511
Epoch: [4][390/391] Elapsed 3m 29s (remain 0m 0s) Loss: 0.1423(0.1026) Grad: 295891.3125  LR: 0.00000633
EVAL: [0/49] Elapsed 0m 0s (remain 0m 35s) Loss: 0.1192(0.1192)
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.0983(0.1014)
[2022-11-16 12:38:45] - Epoch 4 - avg_train_loss: 0.1026  avg_val_loss: 0.1014  time: 240s
[2022-11-16 12:38:45] - Epoch 4 - Score: 0.4505  Scores: [0.490783478822773, 0.4590342224507107, 0.4052247575079497, 0.45206720145360607, 0.4527529378475392, 0.4430964688323874]
[2022-11-16 12:38:45] - Epoch 4 - Save Best Score: 0.4505 Model
Epoch: [5][0/391] Elapsed 0m 1s (remain 6m 41s) Loss: 0.1975(0.1975) Grad: 1083863.6250  LR: 0.00000711
Epoch: [5][100/391] Elapsed 0m 56s (remain 2m 40s) Loss: 0.1066(0.1026) Grad: 474856.5000  LR: 0.00000711
Epoch: [5][200/391] Elapsed 1m 47s (remain 1m 42s) Loss: 0.0813(0.1044) Grad: 124793.0078  LR: 0.00000711
Epoch: [5][300/391] Elapsed 2m 40s (remain 0m 47s) Loss: 0.0854(0.1025) Grad: 180123.2031  LR: 0.00000711
Epoch: [5][390/391] Elapsed 3m 30s (remain 0m 0s) Loss: 0.1113(0.1023) Grad: 134615.6562  LR: 0.00000124
EVAL: [0/49] Elapsed 0m 0s (remain 0m 35s) Loss: 0.1201(0.1201)
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.1052(0.1033)
[2022-11-16 12:42:48] - Epoch 5 - avg_train_loss: 0.1023  avg_val_loss: 0.1033  time: 241s
[2022-11-16 12:42:48] - Epoch 5 - Score: 0.4549  Scores: [0.48932998056233334, 0.45950904288282657, 0.4079684944884303, 0.45691975676781854, 0.4658291866430405, 0.44961601825837333]
[2022-11-16 12:42:48] - ========== fold: 0 result ==========
[2022-11-16 12:42:48] - Score: 0.4505  Scores: [0.490783478822773, 0.4590342224507107, 0.4052247575079497, 0.45206720145360607, 0.4527529378475392, 0.4430964688323874]
[2022-11-16 12:42:48] - ========== fold: 1 training ==========
[2022-11-16 12:42:48] - DebertaV2Config {
  "_name_or_path": "yevheniimaslov/deberta-v3-base-cola",
  "architectures": [
    "DebertaV2ForSequenceClassification"
  ],
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "torch_dtype": "float32",
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Reinitializing Last 1 Layers ...
Done.!
Some weights of the model checkpoint at yevheniimaslov/deberta-v3-base-cola were not used when initializing DebertaV2Model: ['classifier.bias', 'classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/391] Elapsed 0m 0s (remain 4m 48s) Loss: 2.3072(2.3072) Grad: inf  LR: 0.00002994
Epoch: [1][100/391] Elapsed 0m 56s (remain 2m 42s) Loss: 0.0969(0.3403) Grad: 115101.8594  LR: 0.00002994
Epoch: [1][200/391] Elapsed 1m 50s (remain 1m 44s) Loss: 0.1374(0.2360) Grad: 75233.4609  LR: 0.00002994
Epoch: [1][300/391] Elapsed 2m 43s (remain 0m 49s) Loss: 0.1287(0.1962) Grad: 145663.4688  LR: 0.00002994
Epoch: [1][390/391] Elapsed 3m 29s (remain 0m 0s) Loss: 0.0820(0.1769) Grad: 66339.7109  LR: 0.00002821
EVAL: [0/49] Elapsed 0m 1s (remain 1m 0s) Loss: 0.1080(0.1080)
EVAL: [48/49] Elapsed 0m 31s (remain 0m 0s) Loss: 0.1261(0.1157)
[2022-11-16 12:46:51] - Epoch 1 - avg_train_loss: 0.1769  avg_val_loss: 0.1157  time: 241s
[2022-11-16 12:46:51] - Epoch 1 - Score: 0.4821  Scores: [0.5255119368405463, 0.4646654388311616, 0.43495100245092194, 0.5123420807084527, 0.47648574616946243, 0.4786208200363688]
[2022-11-16 12:46:51] - Epoch 1 - Save Best Score: 0.4821 Model
Epoch: [2][0/391] Elapsed 0m 1s (remain 6m 34s) Loss: 0.0909(0.0909) Grad: 277586.8750  LR: 0.00002864
Epoch: [2][100/391] Elapsed 0m 55s (remain 2m 38s) Loss: 0.1103(0.1060) Grad: 120991.8359  LR: 0.00002864
Epoch: [2][200/391] Elapsed 1m 49s (remain 1m 43s) Loss: 0.1012(0.1053) Grad: 270156.2188  LR: 0.00002864
Epoch: [2][300/391] Elapsed 2m 41s (remain 0m 48s) Loss: 0.0776(0.1066) Grad: 119589.5312  LR: 0.00002864
Epoch: [2][390/391] Elapsed 3m 28s (remain 0m 0s) Loss: 0.0858(0.1069) Grad: 113771.9062  LR: 0.00002231
EVAL: [0/49] Elapsed 0m 1s (remain 1m 0s) Loss: 0.1148(0.1148)
EVAL: [48/49] Elapsed 0m 31s (remain 0m 0s) Loss: 0.1302(0.1189)
Epoch: [3][0/391] Elapsed 0m 0s (remain 3m 23s) Loss: 0.1200(0.1200) Grad: 193723.1406  LR: 0.00002312
[2022-11-16 12:50:52] - Epoch 2 - avg_train_loss: 0.1069  avg_val_loss: 0.1189  time: 240s
[2022-11-16 12:50:52] - Epoch 2 - Score: 0.4896  Scores: [0.5213951149312546, 0.49635064149307073, 0.4661411408124138, 0.5031665919557786, 0.48881996340642314, 0.4618403925590092]
Epoch: [3][100/391] Elapsed 0m 54s (remain 2m 35s) Loss: 0.1239(0.1077) Grad: 125216.3203  LR: 0.00002312
Epoch: [3][200/391] Elapsed 1m 48s (remain 1m 42s) Loss: 0.1072(0.1045) Grad: 274205.5625  LR: 0.00002312
Epoch: [3][300/391] Elapsed 2m 39s (remain 0m 47s) Loss: 0.1479(0.1047) Grad: 260865.2656  LR: 0.00002312
Epoch: [3][390/391] Elapsed 3m 28s (remain 0m 0s) Loss: 0.1045(0.1037) Grad: 234261.7344  LR: 0.00001417
EVAL: [0/49] Elapsed 0m 1s (remain 0m 59s) Loss: 0.0999(0.0999)
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.1145(0.1061)
[2022-11-16 12:54:52] - Epoch 3 - avg_train_loss: 0.1037  avg_val_loss: 0.1061  time: 239s
[2022-11-16 12:54:52] - Epoch 3 - Score: 0.4616  Scores: [0.4965152461769606, 0.45130136338419574, 0.41828012380282253, 0.48604731998777384, 0.47241461371018323, 0.44504522540786356]
[2022-11-16 12:54:52] - Epoch 3 - Save Best Score: 0.4616 Model
Epoch: [4][0/391] Elapsed 0m 0s (remain 6m 0s) Loss: 0.1237(0.1237) Grad: 321050.2500  LR: 0.00001511
Epoch: [4][100/391] Elapsed 0m 55s (remain 2m 39s) Loss: 0.0682(0.0934) Grad: 208328.7344  LR: 0.00001511
Epoch: [4][200/391] Elapsed 1m 48s (remain 1m 42s) Loss: 0.0725(0.0962) Grad: 93352.2734  LR: 0.00001511
Epoch: [4][300/391] Elapsed 2m 40s (remain 0m 48s) Loss: 0.0903(0.0984) Grad: 179982.5625  LR: 0.00001511
Epoch: [4][390/391] Elapsed 3m 28s (remain 0m 0s) Loss: 0.0818(0.0997) Grad: 159913.5781  LR: 0.00000633
EVAL: [0/49] Elapsed 0m 1s (remain 0m 59s) Loss: 0.0923(0.0923)
EVAL: [48/49] Elapsed 0m 31s (remain 0m 0s) Loss: 0.1118(0.1056)
[2022-11-16 12:58:53] - Epoch 4 - avg_train_loss: 0.0997  avg_val_loss: 0.1056  time: 240s
[2022-11-16 12:58:53] - Epoch 4 - Score: 0.4606  Scores: [0.49675049412434824, 0.4489872346978028, 0.42555208261676175, 0.4790771166528383, 0.46683498065689827, 0.4461523630330441]
[2022-11-16 12:58:53] - Epoch 4 - Save Best Score: 0.4606 Model
Epoch: [5][0/391] Elapsed 0m 0s (remain 5m 21s) Loss: 0.1588(0.1588) Grad: 432578.7812  LR: 0.00000711
Epoch: [5][100/391] Elapsed 0m 57s (remain 2m 44s) Loss: 0.0892(0.0935) Grad: 505433.7500  LR: 0.00000711
Epoch: [5][200/391] Elapsed 1m 47s (remain 1m 41s) Loss: 0.1004(0.0950) Grad: 132358.7344  LR: 0.00000711
Epoch: [5][300/391] Elapsed 2m 43s (remain 0m 48s) Loss: 0.0779(0.0958) Grad: 115319.9844  LR: 0.00000711
Epoch: [5][390/391] Elapsed 3m 30s (remain 0m 0s) Loss: 0.0752(0.0953) Grad: 71430.5703  LR: 0.00000124
EVAL: [0/49] Elapsed 0m 1s (remain 0m 59s) Loss: 0.0962(0.0962)
EVAL: [48/49] Elapsed 0m 30s (remain 0m 0s) Loss: 0.1123(0.1066)
[2022-11-16 13:02:55] - Epoch 5 - avg_train_loss: 0.0953  avg_val_loss: 0.1066  time: 241s
[2022-11-16 13:02:55] - Epoch 5 - Score: 0.4628  Scores: [0.4918607694038438, 0.4551168480106181, 0.4255639026504303, 0.4799545648771236, 0.4720191744685655, 0.452578924925981]
[2022-11-16 13:02:56] - ========== fold: 1 result ==========
[2022-11-16 13:02:56] - Score: 0.4606  Scores: [0.49675049412434824, 0.4489872346978028, 0.42555208261676175, 0.4790771166528383, 0.46683498065689827, 0.4461523630330441]
[2022-11-16 13:02:56] - ========== fold: 2 training ==========
[2022-11-16 13:02:56] - DebertaV2Config {
  "_name_or_path": "yevheniimaslov/deberta-v3-base-cola",
  "architectures": [
    "DebertaV2ForSequenceClassification"
  ],
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "torch_dtype": "float32",
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Reinitializing Last 1 Layers ...
Done.!
Some weights of the model checkpoint at yevheniimaslov/deberta-v3-base-cola were not used when initializing DebertaV2Model: ['classifier.bias', 'classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/391] Elapsed 0m 0s (remain 3m 58s) Loss: 2.1873(2.1873) Grad: inf  LR: 0.00002994
Epoch: [1][100/391] Elapsed 0m 50s (remain 2m 25s) Loss: 0.1085(0.2897) Grad: 178942.3125  LR: 0.00002994
Epoch: [1][200/391] Elapsed 1m 42s (remain 1m 37s) Loss: 0.0762(0.2061) Grad: 74210.3125  LR: 0.00002994
Epoch: [1][300/391] Elapsed 2m 39s (remain 0m 47s) Loss: 0.1007(0.1770) Grad: 145528.1719  LR: 0.00002994
Epoch: [1][390/391] Elapsed 3m 24s (remain 0m 0s) Loss: 0.1582(0.1643) Grad: 267782.3125  LR: 0.00002821
EVAL: [0/49] Elapsed 0m 0s (remain 0m 35s) Loss: 0.0892(0.0892)
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0843(0.1139)
[2022-11-16 13:06:56] - Epoch 1 - avg_train_loss: 0.1643  avg_val_loss: 0.1139  time: 238s
[2022-11-16 13:06:56] - Epoch 1 - Score: 0.4786  Scores: [0.5286067789927175, 0.4750600521416841, 0.43740406297607815, 0.4637854464357665, 0.49496244183052807, 0.4716713880378024]
[2022-11-16 13:06:56] - Epoch 1 - Save Best Score: 0.4786 Model
Epoch: [2][0/391] Elapsed 0m 0s (remain 5m 34s) Loss: 0.1474(0.1474) Grad: 292545.9688  LR: 0.00002864
Epoch: [2][100/391] Elapsed 0m 54s (remain 2m 37s) Loss: 0.1322(0.1090) Grad: 167053.4219  LR: 0.00002864
Epoch: [2][200/391] Elapsed 1m 47s (remain 1m 41s) Loss: 0.0951(0.1089) Grad: 267814.5312  LR: 0.00002864
Epoch: [2][300/391] Elapsed 2m 38s (remain 0m 47s) Loss: 0.1186(0.1076) Grad: 203037.7656  LR: 0.00002864
Epoch: [2][390/391] Elapsed 3m 27s (remain 0m 0s) Loss: 0.1065(0.1067) Grad: 189401.7188  LR: 0.00002231
EVAL: [0/49] Elapsed 0m 0s (remain 0m 36s) Loss: 0.0833(0.0833)
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0728(0.1096)
[2022-11-16 13:10:57] - Epoch 2 - avg_train_loss: 0.1067  avg_val_loss: 0.1096  time: 240s
[2022-11-16 13:10:57] - Epoch 2 - Score: 0.4698  Scores: [0.5065579518132145, 0.4690118983170304, 0.4389527470564751, 0.4516245017531314, 0.48201284342630113, 0.47069095275289347]
[2022-11-16 13:10:57] - Epoch 2 - Save Best Score: 0.4698 Model
Epoch: [3][0/391] Elapsed 0m 0s (remain 4m 10s) Loss: 0.0885(0.0885) Grad: 114929.2344  LR: 0.00002312
Epoch: [3][100/391] Elapsed 0m 53s (remain 2m 34s) Loss: 0.1127(0.1061) Grad: 261953.5312  LR: 0.00002312
Epoch: [3][200/391] Elapsed 1m 45s (remain 1m 39s) Loss: 0.0781(0.1045) Grad: 109873.3828  LR: 0.00002312
Epoch: [3][300/391] Elapsed 2m 41s (remain 0m 48s) Loss: 0.1288(0.1052) Grad: 208755.9688  LR: 0.00002312
Epoch: [3][390/391] Elapsed 3m 28s (remain 0m 0s) Loss: 0.1522(0.1052) Grad: 253494.5781  LR: 0.00001417
EVAL: [0/49] Elapsed 0m 0s (remain 0m 35s) Loss: 0.0771(0.0771)
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0772(0.1069)
[2022-11-16 13:15:00] - Epoch 3 - avg_train_loss: 0.1052  avg_val_loss: 0.1069  time: 241s
[2022-11-16 13:15:00] - Epoch 3 - Score: 0.4634  Scores: [0.5007236827274764, 0.4578488850572027, 0.4227219025914103, 0.45054550340653327, 0.48301467959141886, 0.4655145394594308]
[2022-11-16 13:15:00] - Epoch 3 - Save Best Score: 0.4634 Model
Epoch: [4][0/391] Elapsed 0m 0s (remain 5m 53s) Loss: 0.1184(0.1184) Grad: 173299.2969  LR: 0.00001511
Epoch: [4][100/391] Elapsed 0m 55s (remain 2m 38s) Loss: 0.1021(0.0995) Grad: 165823.2031  LR: 0.00001511
Epoch: [4][200/391] Elapsed 1m 45s (remain 1m 39s) Loss: 0.1245(0.1015) Grad: 176606.6406  LR: 0.00001511
Epoch: [4][300/391] Elapsed 2m 37s (remain 0m 47s) Loss: 0.1131(0.1004) Grad: 149612.2656  LR: 0.00001511
Epoch: [4][390/391] Elapsed 3m 25s (remain 0m 0s) Loss: 0.1198(0.1000) Grad: 190331.9688  LR: 0.00000633
EVAL: [0/49] Elapsed 0m 0s (remain 0m 37s) Loss: 0.0745(0.0745)
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0714(0.1037)
Epoch: [5][0/391] Elapsed 0m 0s (remain 3m 31s) Loss: 0.0765(0.0765) Grad: 101975.4375  LR: 0.00000711
[2022-11-16 13:18:59] - Epoch 4 - avg_train_loss: 0.1000  avg_val_loss: 0.1037  time: 238s
[2022-11-16 13:18:59] - Epoch 4 - Score: 0.4562  Scores: [0.49235832117252837, 0.45749698699004737, 0.41394790912960794, 0.4415938796606118, 0.47900261080394185, 0.4526190759786822]
[2022-11-16 13:18:59] - Epoch 4 - Save Best Score: 0.4562 Model
Epoch: [5][100/391] Elapsed 0m 53s (remain 2m 32s) Loss: 0.0920(0.0967) Grad: 106422.7344  LR: 0.00000711
Epoch: [5][200/391] Elapsed 1m 48s (remain 1m 42s) Loss: 0.0892(0.0986) Grad: 138840.0000  LR: 0.00000711
Epoch: [5][300/391] Elapsed 2m 42s (remain 0m 48s) Loss: 0.1607(0.0979) Grad: 173781.1094  LR: 0.00000711
Epoch: [5][390/391] Elapsed 3m 29s (remain 0m 0s) Loss: 0.0760(0.0979) Grad: 124591.8359  LR: 0.00000124
EVAL: [0/49] Elapsed 0m 0s (remain 0m 36s) Loss: 0.0721(0.0721)
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0727(0.1078)
[2022-11-16 13:23:03] - Epoch 5 - avg_train_loss: 0.0979  avg_val_loss: 0.1078  time: 242s
[2022-11-16 13:23:03] - Epoch 5 - Score: 0.4651  Scores: [0.49450537639176345, 0.49452143163904033, 0.4195437600398989, 0.4478282942435626, 0.47801437218974097, 0.45642887595182147]
[2022-11-16 13:23:03] - ========== fold: 2 result ==========
[2022-11-16 13:23:03] - Score: 0.4562  Scores: [0.49235832117252837, 0.45749698699004737, 0.41394790912960794, 0.4415938796606118, 0.47900261080394185, 0.4526190759786822]
[2022-11-16 13:23:03] - ========== fold: 3 training ==========
[2022-11-16 13:23:03] - DebertaV2Config {
  "_name_or_path": "yevheniimaslov/deberta-v3-base-cola",
  "architectures": [
    "DebertaV2ForSequenceClassification"
  ],
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "torch_dtype": "float32",
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Reinitializing Last 1 Layers ...
Done.!
Some weights of the model checkpoint at yevheniimaslov/deberta-v3-base-cola were not used when initializing DebertaV2Model: ['classifier.bias', 'classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/391] Elapsed 0m 0s (remain 4m 3s) Loss: 2.7800(2.7800) Grad: inf  LR: 0.00002994
Epoch: [1][100/391] Elapsed 0m 54s (remain 2m 37s) Loss: 0.0947(0.3361) Grad: 105679.7656  LR: 0.00002994
Epoch: [1][200/391] Elapsed 1m 48s (remain 1m 42s) Loss: 0.1214(0.2331) Grad: 315117.7812  LR: 0.00002994
Epoch: [1][300/391] Elapsed 2m 42s (remain 0m 48s) Loss: 0.1642(0.1940) Grad: 147028.4844  LR: 0.00002994
Epoch: [1][390/391] Elapsed 3m 28s (remain 0m 0s) Loss: 0.0944(0.1781) Grad: 71133.4375  LR: 0.00002821
EVAL: [0/49] Elapsed 0m 1s (remain 0m 59s) Loss: 0.1263(0.1263)
EVAL: [48/49] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0778(0.1129)
[2022-11-16 13:27:07] - Epoch 1 - avg_train_loss: 0.1781  avg_val_loss: 0.1129  time: 242s
[2022-11-16 13:27:07] - Epoch 1 - Score: 0.4757  Scores: [0.5164879492698994, 0.4514774270453561, 0.43515335124227755, 0.47522692602583777, 0.5313100289948501, 0.44453412762926425]
[2022-11-16 13:27:07] - Epoch 1 - Save Best Score: 0.4757 Model
Epoch: [2][0/391] Elapsed 0m 0s (remain 4m 50s) Loss: 0.0865(0.0865) Grad: 177022.6406  LR: 0.00002864
Epoch: [2][100/391] Elapsed 0m 50s (remain 2m 26s) Loss: 0.2217(0.1061) Grad: 344409.3125  LR: 0.00002864
Epoch: [2][200/391] Elapsed 1m 40s (remain 1m 35s) Loss: 0.1833(0.1060) Grad: 411026.7188  LR: 0.00002864
Epoch: [2][300/391] Elapsed 2m 35s (remain 0m 46s) Loss: 0.0961(0.1045) Grad: 307787.9375  LR: 0.00002864
Epoch: [2][390/391] Elapsed 3m 25s (remain 0m 0s) Loss: 0.0902(0.1066) Grad: 313303.5000  LR: 0.00002231
EVAL: [0/49] Elapsed 0m 1s (remain 1m 0s) Loss: 0.1179(0.1179)
EVAL: [48/49] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0751(0.1094)
[2022-11-16 13:31:07] - Epoch 2 - avg_train_loss: 0.1066  avg_val_loss: 0.1094  time: 239s
[2022-11-16 13:31:07] - Epoch 2 - Score: 0.4684  Scores: [0.4806992379440019, 0.45068934221799783, 0.4381493806806939, 0.5063040820793292, 0.49175476035328747, 0.44268274458298223]
[2022-11-16 13:31:07] - Epoch 2 - Save Best Score: 0.4684 Model
Epoch: [3][0/391] Elapsed 0m 1s (remain 8m 34s) Loss: 0.0908(0.0908) Grad: 158352.5625  LR: 0.00002312
Epoch: [3][100/391] Elapsed 0m 53s (remain 2m 33s) Loss: 0.1664(0.1037) Grad: 129534.2891  LR: 0.00002312
Epoch: [3][200/391] Elapsed 1m 47s (remain 1m 41s) Loss: 0.1363(0.1025) Grad: 142638.6406  LR: 0.00002312
Epoch: [3][300/391] Elapsed 2m 39s (remain 0m 47s) Loss: 0.0794(0.1012) Grad: 205628.1406  LR: 0.00002312
Epoch: [3][390/391] Elapsed 3m 26s (remain 0m 0s) Loss: 0.0963(0.1017) Grad: 142133.8750  LR: 0.00001417
EVAL: [0/49] Elapsed 0m 1s (remain 0m 59s) Loss: 0.1223(0.1223)
[2022-11-16 13:35:08] - Epoch 3 - avg_train_loss: 0.1017  avg_val_loss: 0.1042  time: 240s
[2022-11-16 13:35:08] - Epoch 3 - Score: 0.4572  Scores: [0.4752288723504176, 0.4398023600918456, 0.43117949235927694, 0.4635447823684568, 0.48954469677300577, 0.44415230039670445]
EVAL: [48/49] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0680(0.1042)
[2022-11-16 13:35:08] - Epoch 3 - Save Best Score: 0.4572 Model
Epoch: [4][0/391] Elapsed 0m 0s (remain 5m 8s) Loss: 0.1460(0.1460) Grad: 239131.3438  LR: 0.00001511
Epoch: [4][100/391] Elapsed 0m 51s (remain 2m 28s) Loss: 0.1386(0.1006) Grad: 219388.2812  LR: 0.00001511
Epoch: [4][200/391] Elapsed 1m 41s (remain 1m 35s) Loss: 0.0624(0.1003) Grad: 118230.4922  LR: 0.00001511
Epoch: [4][300/391] Elapsed 2m 37s (remain 0m 47s) Loss: 0.0636(0.0992) Grad: 108496.2422  LR: 0.00001511
Epoch: [4][390/391] Elapsed 3m 29s (remain 0m 0s) Loss: 0.0806(0.0998) Grad: 143906.2812  LR: 0.00000633
EVAL: [0/49] Elapsed 0m 1s (remain 1m 0s) Loss: 0.1158(0.1158)
EVAL: [48/49] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0684(0.1027)
[2022-11-16 13:39:12] - Epoch 4 - avg_train_loss: 0.0998  avg_val_loss: 0.1027  time: 243s
[2022-11-16 13:39:12] - Epoch 4 - Score: 0.4538  Scores: [0.4771550556722137, 0.4411513903723275, 0.42367267596718927, 0.4579201959205778, 0.4854150509849284, 0.43774378648859563]
[2022-11-16 13:39:12] - Epoch 4 - Save Best Score: 0.4538 Model
Epoch: [5][0/391] Elapsed 0m 0s (remain 6m 21s) Loss: 0.1386(0.1386) Grad: 334079.8750  LR: 0.00000711
Epoch: [5][100/391] Elapsed 0m 55s (remain 2m 38s) Loss: 0.1229(0.0919) Grad: 261973.2969  LR: 0.00000711
Epoch: [5][200/391] Elapsed 1m 47s (remain 1m 41s) Loss: 0.0725(0.0953) Grad: 128795.8906  LR: 0.00000711
Epoch: [5][300/391] Elapsed 2m 40s (remain 0m 48s) Loss: 0.1075(0.0953) Grad: 128735.6562  LR: 0.00000711
Epoch: [5][390/391] Elapsed 3m 26s (remain 0m 0s) Loss: 0.0667(0.0968) Grad: 186815.5000  LR: 0.00000124
EVAL: [0/49] Elapsed 0m 1s (remain 1m 0s) Loss: 0.1223(0.1223)
EVAL: [48/49] Elapsed 0m 33s (remain 0m 0s) Loss: 0.0735(0.1081)
[2022-11-16 13:43:14] - Epoch 5 - avg_train_loss: 0.0968  avg_val_loss: 0.1081  time: 241s
[2022-11-16 13:43:14] - Epoch 5 - Score: 0.4660  Scores: [0.4815023894865883, 0.4576440774259327, 0.44350063529407246, 0.4802870129650033, 0.4906761617764069, 0.44243577685115754]
[2022-11-16 13:43:15] - ========== fold: 3 result ==========
[2022-11-16 13:43:15] - Score: 0.4538  Scores: [0.4771550556722137, 0.4411513903723275, 0.42367267596718927, 0.4579201959205778, 0.4854150509849284, 0.43774378648859563]
[2022-11-16 13:43:15] - ========== fold: 4 training ==========
[2022-11-16 13:43:15] - DebertaV2Config {
  "_name_or_path": "yevheniimaslov/deberta-v3-base-cola",
  "architectures": [
    "DebertaV2ForSequenceClassification"
  ],
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "torch_dtype": "float32",
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Reinitializing Last 1 Layers ...
Done.!
Some weights of the model checkpoint at yevheniimaslov/deberta-v3-base-cola were not used when initializing DebertaV2Model: ['classifier.bias', 'classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/391] Elapsed 0m 1s (remain 6m 54s) Loss: 2.7338(2.7338) Grad: inf  LR: 0.00002994
Epoch: [1][100/391] Elapsed 0m 54s (remain 2m 35s) Loss: 0.1808(0.3214) Grad: 120723.4922  LR: 0.00002994
Epoch: [1][200/391] Elapsed 1m 47s (remain 1m 41s) Loss: 0.1410(0.2214) Grad: 85786.8203  LR: 0.00002994
Epoch: [1][300/391] Elapsed 2m 39s (remain 0m 47s) Loss: 0.2115(0.1862) Grad: 281077.0000  LR: 0.00002994
Epoch: [1][390/391] Elapsed 3m 29s (remain 0m 0s) Loss: 0.1856(0.1706) Grad: 171059.9062  LR: 0.00002821
EVAL: [0/49] Elapsed 0m 1s (remain 1m 9s) Loss: 0.1298(0.1298)
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.1060(0.1409)
[2022-11-16 13:47:19] - Epoch 1 - avg_train_loss: 0.1706  avg_val_loss: 0.1409  time: 242s
[2022-11-16 13:47:19] - Epoch 1 - Score: 0.5308  Scores: [0.6293464115440985, 0.44190310813094413, 0.48794030506125874, 0.5703612133103334, 0.5962014450465458, 0.4588919385630468]
[2022-11-16 13:47:19] - Epoch 1 - Save Best Score: 0.5308 Model
Epoch: [2][0/391] Elapsed 0m 0s (remain 4m 33s) Loss: 0.1158(0.1158) Grad: 198152.9688  LR: 0.00002864
Epoch: [2][100/391] Elapsed 0m 52s (remain 2m 31s) Loss: 0.0802(0.1110) Grad: 262241.5312  LR: 0.00002864
Epoch: [2][200/391] Elapsed 1m 44s (remain 1m 38s) Loss: 0.0829(0.1057) Grad: 169145.1250  LR: 0.00002864
Epoch: [2][300/391] Elapsed 2m 35s (remain 0m 46s) Loss: 0.1134(0.1061) Grad: 223978.4531  LR: 0.00002864
Epoch: [2][390/391] Elapsed 3m 26s (remain 0m 0s) Loss: 0.1227(0.1066) Grad: 169594.1250  LR: 0.00002231
EVAL: [0/49] Elapsed 0m 1s (remain 1m 7s) Loss: 0.0963(0.0963)
EVAL: [48/49] Elapsed 0m 31s (remain 0m 0s) Loss: 0.0780(0.1027)
[2022-11-16 13:51:19] - Epoch 2 - avg_train_loss: 0.1066  avg_val_loss: 0.1027  time: 239s
[2022-11-16 13:51:19] - Epoch 2 - Score: 0.4543  Scores: [0.47890424502485407, 0.4306091639863543, 0.4309781946982743, 0.4553478890701556, 0.48083196246076526, 0.44940385068969346]
[2022-11-16 13:51:19] - Epoch 2 - Save Best Score: 0.4543 Model
Epoch: [3][0/391] Elapsed 0m 0s (remain 6m 18s) Loss: 0.1330(0.1330) Grad: 144265.9531  LR: 0.00002312
Epoch: [3][100/391] Elapsed 0m 55s (remain 2m 40s) Loss: 0.0955(0.1045) Grad: 173996.7031  LR: 0.00002312
Epoch: [3][200/391] Elapsed 1m 49s (remain 1m 43s) Loss: 0.0714(0.1047) Grad: 116226.8828  LR: 0.00002312
Epoch: [3][300/391] Elapsed 2m 45s (remain 0m 49s) Loss: 0.1070(0.1050) Grad: 190043.1719  LR: 0.00002312
Epoch: [3][390/391] Elapsed 3m 28s (remain 0m 0s) Loss: 0.1460(0.1046) Grad: 150371.5938  LR: 0.00001417
EVAL: [0/49] Elapsed 0m 1s (remain 1m 6s) Loss: 0.1108(0.1108)
EVAL: [48/49] Elapsed 0m 31s (remain 0m 0s) Loss: 0.1062(0.1131)
[2022-11-16 13:55:21] - Epoch 3 - avg_train_loss: 0.1046  avg_val_loss: 0.1131  time: 241s
[2022-11-16 13:55:21] - Epoch 3 - Score: 0.4764  Scores: [0.5373799405647512, 0.4330316937520093, 0.4281466884890319, 0.47475926686283226, 0.5076760250847944, 0.4775954540867702]
Epoch: [4][0/391] Elapsed 0m 1s (remain 8m 40s) Loss: 0.1152(0.1152) Grad: 237641.7812  LR: 0.00001511
Epoch: [4][100/391] Elapsed 0m 52s (remain 2m 31s) Loss: 0.0802(0.1047) Grad: 113409.0391  LR: 0.00001511
Epoch: [4][200/391] Elapsed 1m 48s (remain 1m 43s) Loss: 0.0996(0.1022) Grad: 242839.8438  LR: 0.00001511
Epoch: [4][300/391] Elapsed 2m 42s (remain 0m 48s) Loss: 0.0826(0.1038) Grad: 158381.1094  LR: 0.00001511
Epoch: [4][390/391] Elapsed 3m 27s (remain 0m 0s) Loss: 0.0771(0.1027) Grad: 161247.3594  LR: 0.00000633
EVAL: [0/49] Elapsed 0m 1s (remain 1m 7s) Loss: 0.0982(0.0982)
EVAL: [48/49] Elapsed 0m 31s (remain 0m 0s) Loss: 0.0810(0.1012)
[2022-11-16 13:59:21] - Epoch 4 - avg_train_loss: 0.1027  avg_val_loss: 0.1012  time: 240s
[2022-11-16 13:59:21] - Epoch 4 - Score: 0.4510  Scores: [0.47574635440180896, 0.4322074228293185, 0.4234938993123689, 0.4539443809896522, 0.4731651660947149, 0.44731727177751623]
[2022-11-16 13:59:21] - Epoch 4 - Save Best Score: 0.4510 Model
Epoch: [5][0/391] Elapsed 0m 0s (remain 4m 9s) Loss: 0.0833(0.0833) Grad: 116422.5391  LR: 0.00000711
Epoch: [5][100/391] Elapsed 0m 53s (remain 2m 32s) Loss: 0.0916(0.0985) Grad: 163611.0469  LR: 0.00000711
Epoch: [5][200/391] Elapsed 1m 46s (remain 1m 41s) Loss: 0.0820(0.0995) Grad: 132786.1094  LR: 0.00000711
Epoch: [5][300/391] Elapsed 2m 41s (remain 0m 48s) Loss: 0.1013(0.0988) Grad: 245073.8281  LR: 0.00000711
Epoch: [5][390/391] Elapsed 3m 27s (remain 0m 0s) Loss: 0.1102(0.0986) Grad: 196601.7969  LR: 0.00000124
EVAL: [0/49] Elapsed 0m 1s (remain 1m 6s) Loss: 0.0961(0.0961)
EVAL: [48/49] Elapsed 0m 32s (remain 0m 0s) Loss: 0.0756(0.1038)
[2022-11-16 14:03:22] - Epoch 5 - avg_train_loss: 0.0986  avg_val_loss: 0.1038  time: 239s
[2022-11-16 14:03:22] - Epoch 5 - Score: 0.4567  Scores: [0.477056317318352, 0.43723930834303104, 0.4281171754261132, 0.4713645882909877, 0.47502441682019236, 0.45155135716481226]
[2022-11-16 14:03:23] - ========== fold: 4 result ==========
[2022-11-16 14:03:23] - Score: 0.4510  Scores: [0.47574635440180896, 0.4322074228293185, 0.4234938993123689, 0.4539443809896522, 0.4731651660947149, 0.44731727177751623]
[2022-11-16 14:03:23] - ========== CV ==========
[2022-11-16 14:03:23] - Score: 0.4545  Scores: [0.48663548035614446, 0.4478893863683489, 0.4184513910597351, 0.45709244549619077, 0.47156568838843893, 0.44541298743530805]