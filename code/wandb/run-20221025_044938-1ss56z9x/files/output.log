Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

100%|██████████████████████████████████████████████| 3911/3911 [00:04<00:00, 932.05it/s]
[2022-10-25 04:49:46] - max_len: 2048
[2022-10-25 04:49:46] - ========== fold: 0 training ==========
[2022-10-25 04:49:46] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/782] Elapsed 0m 1s (remain 18m 51s) Loss: 1.8402(1.8402) Grad: inf  LR: 0.00000299
Epoch: [1][100/782] Elapsed 0m 18s (remain 2m 7s) Loss: 0.2854(1.1787) Grad: 87348.3047  LR: 0.00000299
Epoch: [1][200/782] Elapsed 0m 37s (remain 1m 47s) Loss: 0.3154(0.6834) Grad: 218334.6875  LR: 0.00000299
Epoch: [1][300/782] Elapsed 0m 53s (remain 1m 26s) Loss: 0.0672(0.5002) Grad: 93061.8281  LR: 0.00000299
Epoch: [1][400/782] Elapsed 1m 9s (remain 1m 6s) Loss: 0.1707(0.4073) Grad: 355528.5000  LR: 0.00000299
Epoch: [1][500/782] Elapsed 1m 27s (remain 0m 48s) Loss: 0.0771(0.3503) Grad: 64893.1836  LR: 0.00000299
Epoch: [1][600/782] Elapsed 1m 42s (remain 0m 30s) Loss: 0.1114(0.3141) Grad: 249983.2812  LR: 0.00000299
Epoch: [1][700/782] Elapsed 2m 0s (remain 0m 13s) Loss: 0.0634(0.2859) Grad: 90845.5625  LR: 0.00000299
Epoch: [1][781/782] Elapsed 2m 13s (remain 0m 0s) Loss: 0.0792(0.2687) Grad: 104594.3672  LR: 0.00000225
EVAL: [0/98] Elapsed 0m 0s (remain 0m 40s) Loss: 0.1257(0.1257)
EVAL: [97/98] Elapsed 0m 19s (remain 0m 0s) Loss: 0.0673(0.1166)
[2022-10-25 04:52:23] - Epoch 1 - avg_train_loss: 0.2687  avg_val_loss: 0.1166  time: 153s
[2022-10-25 04:52:23] - Epoch 1 - Score: 0.4846  Scores: [0.5073080008606555, 0.4785244919693054, 0.4358866855540823, 0.48893640859817084, 0.5060103546029541, 0.490819041251202]
[2022-10-25 04:52:23] - Epoch 1 - Save Best Score: 0.4846 Model
Epoch: [2][0/782] Elapsed 0m 0s (remain 5m 19s) Loss: 0.0810(0.0810) Grad: 401844.0312  LR: 0.00000233
Epoch: [2][100/782] Elapsed 0m 17s (remain 1m 56s) Loss: 0.1523(0.1182) Grad: 307403.1250  LR: 0.00000233
Epoch: [2][200/782] Elapsed 0m 34s (remain 1m 39s) Loss: 0.1250(0.1156) Grad: 164616.6719  LR: 0.00000233
Epoch: [2][300/782] Elapsed 0m 51s (remain 1m 22s) Loss: 0.1370(0.1159) Grad: 78125.1484  LR: 0.00000233
Epoch: [2][400/782] Elapsed 1m 8s (remain 1m 5s) Loss: 0.0755(0.1168) Grad: 39379.0586  LR: 0.00000233
Epoch: [2][500/782] Elapsed 1m 24s (remain 0m 47s) Loss: 0.1037(0.1165) Grad: 142425.4062  LR: 0.00000233
Epoch: [2][600/782] Elapsed 1m 41s (remain 0m 30s) Loss: 0.0694(0.1155) Grad: 65694.9688  LR: 0.00000233
Epoch: [2][700/782] Elapsed 1m 58s (remain 0m 13s) Loss: 0.1474(0.1159) Grad: 163357.3281  LR: 0.00000233
Epoch: [2][781/782] Elapsed 2m 12s (remain 0m 0s) Loss: 0.1214(0.1152) Grad: 170102.6094  LR: 0.00000070
EVAL: [0/98] Elapsed 0m 0s (remain 0m 38s) Loss: 0.1136(0.1136)
EVAL: [97/98] Elapsed 0m 19s (remain 0m 0s) Loss: 0.0729(0.1076)
[2022-10-25 04:54:58] - Epoch 2 - avg_train_loss: 0.1152  avg_val_loss: 0.1076  time: 152s
[2022-10-25 04:54:58] - Epoch 2 - Score: 0.4647  Scores: [0.49430755989337843, 0.4625071475472126, 0.4209700578666101, 0.45934278001400364, 0.4887429755340533, 0.462078543724222]
[2022-10-25 04:54:58] - Epoch 2 - Save Best Score: 0.4647 Model
Epoch: [3][0/782] Elapsed 0m 0s (remain 3m 42s) Loss: 0.1034(0.1034) Grad: 254714.6875  LR: 0.00000078
Epoch: [3][100/782] Elapsed 0m 18s (remain 2m 4s) Loss: 0.0563(0.1061) Grad: 171926.2031  LR: 0.00000078
Epoch: [3][200/782] Elapsed 0m 34s (remain 1m 40s) Loss: 0.1180(0.1095) Grad: 156535.3125  LR: 0.00000078
Epoch: [3][300/782] Elapsed 0m 51s (remain 1m 22s) Loss: 0.1185(0.1085) Grad: 220742.7188  LR: 0.00000078
Epoch: [3][400/782] Elapsed 1m 8s (remain 1m 4s) Loss: 0.0279(0.1085) Grad: 130268.8672  LR: 0.00000078
Epoch: [3][500/782] Elapsed 1m 24s (remain 0m 47s) Loss: 0.0738(0.1082) Grad: 139752.1719  LR: 0.00000078
Epoch: [3][600/782] Elapsed 1m 43s (remain 0m 31s) Loss: 0.1871(0.1075) Grad: 254402.8906  LR: 0.00000078
Epoch: [3][700/782] Elapsed 1m 59s (remain 0m 13s) Loss: 0.0795(0.1078) Grad: 201874.4062  LR: 0.00000078
Epoch: [3][781/782] Elapsed 2m 12s (remain 0m 0s) Loss: 0.1054(0.1071) Grad: 156483.8281  LR: 0.00000013
EVAL: [0/98] Elapsed 0m 0s (remain 0m 36s) Loss: 0.1117(0.1117)
EVAL: [97/98] Elapsed 0m 19s (remain 0m 0s) Loss: 0.0880(0.1074)
[2022-10-25 04:57:35] - Epoch 3 - avg_train_loss: 0.1071  avg_val_loss: 0.1074  time: 152s
[2022-10-25 04:57:35] - Epoch 3 - Score: 0.4640  Scores: [0.4913315523756356, 0.4734555912730906, 0.4179449351334331, 0.46279762992605783, 0.4864027618526993, 0.45189164933655396]
[2022-10-25 04:57:35] - Epoch 3 - Save Best Score: 0.4640 Model
Epoch: [4][0/782] Elapsed 0m 0s (remain 5m 21s) Loss: 0.0559(0.0559) Grad: 302342.1250  LR: 0.00000011
Epoch: [4][100/782] Elapsed 0m 16s (remain 1m 52s) Loss: 0.0930(0.1086) Grad: 155822.4062  LR: 0.00000011
Epoch: [4][200/782] Elapsed 0m 33s (remain 1m 37s) Loss: 0.0514(0.1058) Grad: 124459.4453  LR: 0.00000011
Epoch: [4][300/782] Elapsed 0m 51s (remain 1m 22s) Loss: 0.1153(0.1080) Grad: 354397.0312  LR: 0.00000011
Epoch: [4][400/782] Elapsed 1m 7s (remain 1m 4s) Loss: 0.0827(0.1064) Grad: 105602.9375  LR: 0.00000011
Epoch: [4][500/782] Elapsed 1m 25s (remain 0m 48s) Loss: 0.0574(0.1060) Grad: 156309.1719  LR: 0.00000011
Epoch: [4][600/782] Elapsed 1m 42s (remain 0m 30s) Loss: 0.0719(0.1059) Grad: 114948.1016  LR: 0.00000011
Epoch: [4][700/782] Elapsed 1m 59s (remain 0m 13s) Loss: 0.1273(0.1061) Grad: 267241.6562  LR: 0.00000011
Epoch: [4][781/782] Elapsed 2m 13s (remain 0m 0s) Loss: 0.1024(0.1059) Grad: 163897.3594  LR: 0.00000119
EVAL: [0/98] Elapsed 0m 0s (remain 0m 34s) Loss: 0.1057(0.1057)
[2022-10-25 05:00:12] - Epoch 4 - avg_train_loss: 0.1059  avg_val_loss: 0.1140  time: 153s
[2022-10-25 05:00:12] - Epoch 4 - Score: 0.4784  Scores: [0.5217744268957765, 0.481120489961237, 0.4334071337075855, 0.4857445581336185, 0.4705799742611435, 0.4777042415540932]
EVAL: [97/98] Elapsed 0m 19s (remain 0m 0s) Loss: 0.0977(0.1140)
Epoch: [5][0/782] Elapsed 0m 0s (remain 4m 57s) Loss: 0.3741(0.3741) Grad: inf  LR: 0.00000111
Epoch: [5][100/782] Elapsed 0m 17s (remain 1m 57s) Loss: 0.0554(0.1004) Grad: 112369.2266  LR: 0.00000111
Epoch: [5][200/782] Elapsed 0m 34s (remain 1m 38s) Loss: 0.0747(0.1017) Grad: 135741.6406  LR: 0.00000111
Epoch: [5][300/782] Elapsed 0m 51s (remain 1m 22s) Loss: 0.0536(0.1004) Grad: 129871.0859  LR: 0.00000111
Epoch: [5][400/782] Elapsed 1m 7s (remain 1m 4s) Loss: 0.1441(0.1024) Grad: 320647.5000  LR: 0.00000111
Epoch: [5][500/782] Elapsed 1m 25s (remain 0m 47s) Loss: 0.0494(0.1037) Grad: 108342.6641  LR: 0.00000111
Epoch: [5][600/782] Elapsed 1m 42s (remain 0m 30s) Loss: 0.1062(0.1041) Grad: 120634.3672  LR: 0.00000111
Epoch: [5][700/782] Elapsed 1m 59s (remain 0m 13s) Loss: 0.0535(0.1045) Grad: 83551.8359  LR: 0.00000111
Epoch: [5][781/782] Elapsed 2m 12s (remain 0m 0s) Loss: 0.0707(0.1033) Grad: 195629.2812  LR: 0.00000267
EVAL: [0/98] Elapsed 0m 0s (remain 0m 40s) Loss: 0.1134(0.1134)
EVAL: [97/98] Elapsed 0m 19s (remain 0m 0s) Loss: 0.0797(0.1102)
[2022-10-25 05:02:44] - Epoch 5 - avg_train_loss: 0.1033  avg_val_loss: 0.1102  time: 152s
[2022-10-25 05:02:44] - Epoch 5 - Score: 0.4700  Scores: [0.5224965332849968, 0.4625049335648815, 0.418832528034962, 0.4621756289899382, 0.47156657966234733, 0.4821407151599908]
[2022-10-25 05:02:45] - ========== fold: 0 result ==========
[2022-10-25 05:02:45] - Score: 0.4640  Scores: [0.4913315523756356, 0.4734555912730906, 0.4179449351334331, 0.46279762992605783, 0.4864027618526993, 0.45189164933655396]
[2022-10-25 05:02:45] - ========== fold: 1 training ==========
[2022-10-25 05:02:45] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/782] Elapsed 0m 0s (remain 9m 8s) Loss: 2.8618(2.8618) Grad: inf  LR: 0.00000299
Epoch: [1][100/782] Elapsed 0m 18s (remain 2m 6s) Loss: 0.6348(2.0276) Grad: 187081.9688  LR: 0.00000299
Epoch: [1][200/782] Elapsed 0m 35s (remain 1m 43s) Loss: 0.1333(1.1442) Grad: 97132.3750  LR: 0.00000299
Epoch: [1][300/782] Elapsed 0m 51s (remain 1m 22s) Loss: 0.1489(0.8138) Grad: 83422.2734  LR: 0.00000299
Epoch: [1][400/782] Elapsed 1m 6s (remain 1m 3s) Loss: 0.0747(0.6445) Grad: 106883.7188  LR: 0.00000299
Epoch: [1][500/782] Elapsed 1m 23s (remain 0m 46s) Loss: 0.1084(0.5423) Grad: 89159.1250  LR: 0.00000299
Epoch: [1][600/782] Elapsed 1m 41s (remain 0m 30s) Loss: 0.1266(0.4738) Grad: 75402.6719  LR: 0.00000299
Epoch: [1][700/782] Elapsed 1m 58s (remain 0m 13s) Loss: 0.1646(0.4225) Grad: 107580.0938  LR: 0.00000299
Epoch: [1][781/782] Elapsed 2m 12s (remain 0m 0s) Loss: 0.1012(0.3923) Grad: 175037.2656  LR: 0.00000225
EVAL: [0/98] Elapsed 0m 0s (remain 0m 37s) Loss: 0.1306(0.1306)
EVAL: [97/98] Elapsed 0m 18s (remain 0m 0s) Loss: 0.1377(0.1186)
[2022-10-25 05:05:18] - Epoch 1 - avg_train_loss: 0.3923  avg_val_loss: 0.1186  time: 151s
[2022-10-25 05:05:18] - Epoch 1 - Score: 0.4887  Scores: [0.5309413301078797, 0.47625021191110817, 0.4372307695280082, 0.48493444694829574, 0.518309726665079, 0.48445915639326037]
[2022-10-25 05:05:18] - Epoch 1 - Save Best Score: 0.4887 Model
Epoch: [2][0/782] Elapsed 0m 0s (remain 6m 32s) Loss: 0.1022(0.1022) Grad: 460413.4062  LR: 0.00000233
Epoch: [2][100/782] Elapsed 0m 15s (remain 1m 47s) Loss: 0.1096(0.1119) Grad: 178432.4688  LR: 0.00000233
Epoch: [2][200/782] Elapsed 0m 32s (remain 1m 34s) Loss: 0.0662(0.1086) Grad: 175718.4062  LR: 0.00000233
Epoch: [2][300/782] Elapsed 0m 49s (remain 1m 19s) Loss: 0.1074(0.1125) Grad: 336733.4062  LR: 0.00000233
Epoch: [2][400/782] Elapsed 1m 6s (remain 1m 2s) Loss: 0.1233(0.1143) Grad: 271748.3125  LR: 0.00000233
Epoch: [2][500/782] Elapsed 1m 22s (remain 0m 46s) Loss: 0.1056(0.1140) Grad: 117934.5781  LR: 0.00000233
Epoch: [2][600/782] Elapsed 1m 40s (remain 0m 30s) Loss: 0.2008(0.1138) Grad: 221444.8438  LR: 0.00000233
Epoch: [2][700/782] Elapsed 1m 57s (remain 0m 13s) Loss: 0.1545(0.1139) Grad: 320223.7812  LR: 0.00000233
Epoch: [2][781/782] Elapsed 2m 10s (remain 0m 0s) Loss: 0.0881(0.1144) Grad: 186513.2188  LR: 0.00000070
EVAL: [0/98] Elapsed 0m 0s (remain 0m 37s) Loss: 0.1236(0.1236)
EVAL: [97/98] Elapsed 0m 18s (remain 0m 0s) Loss: 0.1196(0.1116)
[2022-10-25 05:07:52] - Epoch 2 - avg_train_loss: 0.1144  avg_val_loss: 0.1116  time: 150s
[2022-10-25 05:07:52] - Epoch 2 - Score: 0.4737  Scores: [0.5097169256464174, 0.46337688107329733, 0.4265099475376935, 0.4763663415134144, 0.5010892368134829, 0.465286461420772]
[2022-10-25 05:07:52] - Epoch 2 - Save Best Score: 0.4737 Model
Epoch: [3][0/782] Elapsed 0m 0s (remain 4m 37s) Loss: 0.1018(0.1018) Grad: 222367.4531  LR: 0.00000078
Epoch: [3][100/782] Elapsed 0m 16s (remain 1m 53s) Loss: 0.1010(0.1030) Grad: 147614.9844  LR: 0.00000078
Epoch: [3][200/782] Elapsed 0m 33s (remain 1m 37s) Loss: 0.1104(0.1084) Grad: 152356.5938  LR: 0.00000078
Epoch: [3][300/782] Elapsed 0m 50s (remain 1m 21s) Loss: 0.0959(0.1092) Grad: 215978.6250  LR: 0.00000078
Epoch: [3][400/782] Elapsed 1m 7s (remain 1m 4s) Loss: 0.1271(0.1095) Grad: 297389.6875  LR: 0.00000078
Epoch: [3][500/782] Elapsed 1m 24s (remain 0m 47s) Loss: 0.1263(0.1101) Grad: 232540.8281  LR: 0.00000078
Epoch: [3][600/782] Elapsed 1m 41s (remain 0m 30s) Loss: 0.1017(0.1090) Grad: 141262.0469  LR: 0.00000078
Epoch: [3][700/782] Elapsed 1m 57s (remain 0m 13s) Loss: 0.1406(0.1093) Grad: 123944.5000  LR: 0.00000078
Epoch: [3][781/782] Elapsed 2m 9s (remain 0m 0s) Loss: 0.1157(0.1105) Grad: 126566.9609  LR: 0.00000013
EVAL: [0/98] Elapsed 0m 0s (remain 0m 34s) Loss: 0.1116(0.1116)
[2022-10-25 05:10:25] - Epoch 3 - avg_train_loss: 0.1105  avg_val_loss: 0.1135  time: 149s
[2022-10-25 05:10:25] - Epoch 3 - Score: 0.4778  Scores: [0.5130938615784427, 0.4716552273760328, 0.4303862236920256, 0.48183484728571285, 0.5017926002322216, 0.4681168644112056]
EVAL: [97/98] Elapsed 0m 18s (remain 0m 0s) Loss: 0.1180(0.1135)
Epoch: [4][0/782] Elapsed 0m 0s (remain 6m 19s) Loss: 0.1864(0.1864) Grad: inf  LR: 0.00000011
Epoch: [4][100/782] Elapsed 0m 16s (remain 1m 52s) Loss: 0.1547(0.1083) Grad: 174066.2344  LR: 0.00000011
Epoch: [4][200/782] Elapsed 0m 33s (remain 1m 36s) Loss: 0.1056(0.1060) Grad: 255778.4219  LR: 0.00000011
Epoch: [4][300/782] Elapsed 0m 50s (remain 1m 20s) Loss: 0.0503(0.1066) Grad: 127587.5469  LR: 0.00000011
Epoch: [4][400/782] Elapsed 1m 7s (remain 1m 4s) Loss: 0.1269(0.1065) Grad: 139029.9219  LR: 0.00000011
Epoch: [4][500/782] Elapsed 1m 23s (remain 0m 46s) Loss: 0.1508(0.1075) Grad: 365115.7188  LR: 0.00000011
Epoch: [4][600/782] Elapsed 1m 39s (remain 0m 30s) Loss: 0.1121(0.1070) Grad: 213785.7969  LR: 0.00000011
Epoch: [4][700/782] Elapsed 1m 55s (remain 0m 13s) Loss: 0.0672(0.1071) Grad: 117807.7031  LR: 0.00000011
Epoch: [4][781/782] Elapsed 2m 9s (remain 0m 0s) Loss: 0.0817(0.1063) Grad: 117997.9219  LR: 0.00000119
EVAL: [0/98] Elapsed 0m 0s (remain 0m 40s) Loss: 0.1288(0.1288)
EVAL: [97/98] Elapsed 0m 18s (remain 0m 0s) Loss: 0.1044(0.1093)
[2022-10-25 05:12:54] - Epoch 4 - avg_train_loss: 0.1063  avg_val_loss: 0.1093  time: 149s
[2022-10-25 05:12:54] - Epoch 4 - Score: 0.4687  Scores: [0.507929969160231, 0.45374202034251815, 0.4222118388142667, 0.4726631630673125, 0.4993322733202521, 0.4562721421196272]
[2022-10-25 05:12:54] - Epoch 4 - Save Best Score: 0.4687 Model
Epoch: [5][0/782] Elapsed 0m 0s (remain 4m 39s) Loss: 0.0646(0.0646) Grad: 167427.9062  LR: 0.00000111
Epoch: [5][100/782] Elapsed 0m 16s (remain 1m 49s) Loss: 0.1808(0.1021) Grad: 192980.7344  LR: 0.00000111
Epoch: [5][200/782] Elapsed 0m 33s (remain 1m 38s) Loss: 0.1361(0.1019) Grad: 162299.9062  LR: 0.00000111
Epoch: [5][300/782] Elapsed 0m 50s (remain 1m 20s) Loss: 0.1690(0.1043) Grad: 124381.0703  LR: 0.00000111
Epoch: [5][400/782] Elapsed 1m 6s (remain 1m 2s) Loss: 0.0923(0.1045) Grad: 161753.0469  LR: 0.00000111
Epoch: [5][500/782] Elapsed 1m 21s (remain 0m 45s) Loss: 0.0934(0.1043) Grad: 150224.7344  LR: 0.00000111
Epoch: [5][600/782] Elapsed 1m 39s (remain 0m 29s) Loss: 0.1352(0.1030) Grad: 162026.9688  LR: 0.00000111
Epoch: [5][700/782] Elapsed 1m 56s (remain 0m 13s) Loss: 0.1097(0.1029) Grad: 204270.6719  LR: 0.00000111
Epoch: [5][781/782] Elapsed 2m 9s (remain 0m 0s) Loss: 0.1358(0.1032) Grad: 206097.4531  LR: 0.00000267
EVAL: [0/98] Elapsed 0m 0s (remain 0m 40s) Loss: 0.1536(0.1536)
EVAL: [97/98] Elapsed 0m 18s (remain 0m 0s) Loss: 0.1254(0.1366)
[2022-10-25 05:15:26] - Epoch 5 - avg_train_loss: 0.1032  avg_val_loss: 0.1366  time: 149s
[2022-10-25 05:15:26] - Epoch 5 - Score: 0.5236  Scores: [0.6440455560820026, 0.4717152793839403, 0.46380913255839157, 0.5267690288708653, 0.521361776155798, 0.5136303755019809]
[2022-10-25 05:15:26] - ========== fold: 1 result ==========
[2022-10-25 05:15:26] - Score: 0.4687  Scores: [0.507929969160231, 0.45374202034251815, 0.4222118388142667, 0.4726631630673125, 0.4993322733202521, 0.4562721421196272]
[2022-10-25 05:15:26] - ========== fold: 2 training ==========
[2022-10-25 05:15:26] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Epoch: [1][0/782] Elapsed 0m 0s (remain 4m 4s) Loss: 3.1960(3.1960) Grad: inf  LR: 0.00000299
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][100/782] Elapsed 0m 18s (remain 2m 3s) Loss: 0.8062(2.1601) Grad: 226672.9688  LR: 0.00000299
Epoch: [1][200/782] Elapsed 0m 34s (remain 1m 40s) Loss: 0.1316(1.2088) Grad: 96090.0078  LR: 0.00000299
Epoch: [1][300/782] Elapsed 0m 51s (remain 1m 21s) Loss: 0.1202(0.8553) Grad: 190662.3281  LR: 0.00000299
Epoch: [1][400/782] Elapsed 1m 6s (remain 1m 3s) Loss: 0.0793(0.6727) Grad: 77747.5859  LR: 0.00000299
Epoch: [1][500/782] Elapsed 1m 22s (remain 0m 46s) Loss: 0.1122(0.5634) Grad: 166038.6406  LR: 0.00000299
Epoch: [1][600/782] Elapsed 1m 38s (remain 0m 29s) Loss: 0.1307(0.4902) Grad: 82780.4531  LR: 0.00000299
Epoch: [1][700/782] Elapsed 1m 54s (remain 0m 13s) Loss: 0.1054(0.4380) Grad: 149995.8750  LR: 0.00000299
Epoch: [1][781/782] Elapsed 2m 8s (remain 0m 0s) Loss: 0.0791(0.4045) Grad: 100214.8125  LR: 0.00000225
EVAL: [0/98] Elapsed 0m 0s (remain 0m 37s) Loss: 0.0797(0.0797)
EVAL: [97/98] Elapsed 0m 19s (remain 0m 0s) Loss: 0.0684(0.1182)
[2022-10-25 05:17:57] - Epoch 1 - avg_train_loss: 0.4045  avg_val_loss: 0.1182  time: 149s
[2022-10-25 05:17:57] - Epoch 1 - Score: 0.4878  Scores: [0.5237245960773972, 0.4845854076399757, 0.44522692365606076, 0.46322092253984015, 0.5117875191407791, 0.49837554908406545]
[2022-10-25 05:17:57] - Epoch 1 - Save Best Score: 0.4878 Model
Epoch: [2][0/782] Elapsed 0m 0s (remain 7m 6s) Loss: 0.2319(0.2319) Grad: inf  LR: 0.00000233
Epoch: [2][100/782] Elapsed 0m 17s (remain 1m 58s) Loss: 0.1192(0.1121) Grad: 372163.8125  LR: 0.00000233
Epoch: [2][200/782] Elapsed 0m 32s (remain 1m 34s) Loss: 0.1028(0.1111) Grad: 128965.2500  LR: 0.00000233
Epoch: [2][300/782] Elapsed 0m 48s (remain 1m 17s) Loss: 0.1255(0.1092) Grad: 566427.8125  LR: 0.00000233
Epoch: [2][400/782] Elapsed 1m 6s (remain 1m 2s) Loss: 0.2469(0.1105) Grad: 301677.7500  LR: 0.00000233
Epoch: [2][500/782] Elapsed 1m 21s (remain 0m 45s) Loss: 0.0778(0.1090) Grad: 114069.9922  LR: 0.00000233
Epoch: [2][600/782] Elapsed 1m 37s (remain 0m 29s) Loss: 0.1581(0.1100) Grad: 175338.4688  LR: 0.00000233
Epoch: [2][700/782] Elapsed 1m 54s (remain 0m 13s) Loss: 0.1489(0.1105) Grad: 167416.5312  LR: 0.00000233
Epoch: [2][781/782] Elapsed 2m 7s (remain 0m 0s) Loss: 0.2958(0.1111) Grad: 307054.7500  LR: 0.00000070
EVAL: [0/98] Elapsed 0m 0s (remain 0m 38s) Loss: 0.0814(0.0814)
EVAL: [97/98] Elapsed 0m 20s (remain 0m 0s) Loss: 0.0713(0.1165)
[2022-10-25 05:20:28] - Epoch 2 - avg_train_loss: 0.1111  avg_val_loss: 0.1165  time: 148s
[2022-10-25 05:20:28] - Epoch 2 - Score: 0.4842  Scores: [0.5188712613797528, 0.4866949522883568, 0.4406997457440852, 0.46039910685937674, 0.5088585442666888, 0.48960600692897976]
[2022-10-25 05:20:28] - Epoch 2 - Save Best Score: 0.4842 Model
Epoch: [3][0/782] Elapsed 0m 0s (remain 4m 45s) Loss: 0.0952(0.0952) Grad: inf  LR: 0.00000078
Epoch: [3][100/782] Elapsed 0m 16s (remain 1m 51s) Loss: 0.1440(0.0993) Grad: 475075.3438  LR: 0.00000078
Epoch: [3][200/782] Elapsed 0m 32s (remain 1m 35s) Loss: 0.0711(0.1014) Grad: 162416.2656  LR: 0.00000078
Epoch: [3][300/782] Elapsed 0m 50s (remain 1m 19s) Loss: 0.0860(0.1025) Grad: 143838.5938  LR: 0.00000078
Epoch: [3][400/782] Elapsed 1m 7s (remain 1m 3s) Loss: 0.0546(0.1058) Grad: 120986.3984  LR: 0.00000078
Epoch: [3][500/782] Elapsed 1m 23s (remain 0m 46s) Loss: 0.0502(0.1061) Grad: 52417.5391  LR: 0.00000078
Epoch: [3][600/782] Elapsed 1m 39s (remain 0m 29s) Loss: 0.1021(0.1071) Grad: 57943.8906  LR: 0.00000078
Epoch: [3][700/782] Elapsed 1m 55s (remain 0m 13s) Loss: 0.2353(0.1070) Grad: 107796.9922  LR: 0.00000078
Epoch: [3][781/782] Elapsed 2m 9s (remain 0m 0s) Loss: 0.1117(0.1079) Grad: 53426.1602  LR: 0.00000013
EVAL: [0/98] Elapsed 0m 0s (remain 0m 42s) Loss: 0.0812(0.0812)
[2022-10-25 05:23:02] - Epoch 3 - avg_train_loss: 0.1079  avg_val_loss: 0.1106  time: 150s
[2022-10-25 05:23:02] - Epoch 3 - Score: 0.4715  Scores: [0.5075608746431222, 0.4715608019569918, 0.43041145824193744, 0.44939445947505385, 0.49430824425855624, 0.4755901235535031]
[2022-10-25 05:23:02] - Epoch 3 - Save Best Score: 0.4715 Model
EVAL: [97/98] Elapsed 0m 20s (remain 0m 0s) Loss: 0.0702(0.1106)
Epoch: [4][0/782] Elapsed 0m 0s (remain 4m 39s) Loss: 0.1158(0.1158) Grad: 214485.0469  LR: 0.00000011
Epoch: [4][100/782] Elapsed 0m 17s (remain 2m 0s) Loss: 0.0709(0.1000) Grad: 65744.3750  LR: 0.00000011
Epoch: [4][200/782] Elapsed 0m 33s (remain 1m 36s) Loss: 0.1196(0.1036) Grad: 98847.8906  LR: 0.00000011
Epoch: [4][300/782] Elapsed 0m 49s (remain 1m 19s) Loss: 0.0569(0.1047) Grad: 110411.5469  LR: 0.00000011
Epoch: [4][400/782] Elapsed 1m 6s (remain 1m 3s) Loss: 0.0814(0.1042) Grad: 101211.3750  LR: 0.00000011
Epoch: [4][500/782] Elapsed 1m 23s (remain 0m 46s) Loss: 0.0694(0.1038) Grad: 58415.1992  LR: 0.00000011
Epoch: [4][600/782] Elapsed 1m 39s (remain 0m 29s) Loss: 0.1799(0.1031) Grad: 170988.6875  LR: 0.00000011
Epoch: [4][700/782] Elapsed 1m 55s (remain 0m 13s) Loss: 0.0607(0.1034) Grad: 74160.4688  LR: 0.00000011
Epoch: [4][781/782] Elapsed 2m 9s (remain 0m 0s) Loss: 0.1629(0.1033) Grad: 164200.3594  LR: 0.00000119
EVAL: [0/98] Elapsed 0m 0s (remain 0m 42s) Loss: 0.1136(0.1136)
EVAL: [97/98] Elapsed 0m 20s (remain 0m 0s) Loss: 0.1113(0.1330)
[2022-10-25 05:25:35] - Epoch 4 - avg_train_loss: 0.1033  avg_val_loss: 0.1330  time: 150s
[2022-10-25 05:25:35] - Epoch 4 - Score: 0.5168  Scores: [0.5636514677513431, 0.5145685177455512, 0.4468922557959244, 0.5099903170729425, 0.5818440769464771, 0.4837258434235193]
Epoch: [5][0/782] Elapsed 0m 0s (remain 8m 35s) Loss: 0.0485(0.0485) Grad: 308644.6875  LR: 0.00000111
Epoch: [5][100/782] Elapsed 0m 16s (remain 1m 53s) Loss: 0.0831(0.0952) Grad: 119737.1172  LR: 0.00000111
Epoch: [5][200/782] Elapsed 0m 31s (remain 1m 32s) Loss: 0.0957(0.0950) Grad: 203968.5156  LR: 0.00000111
Epoch: [5][300/782] Elapsed 0m 48s (remain 1m 16s) Loss: 0.1025(0.0961) Grad: 308357.5938  LR: 0.00000111
Epoch: [5][400/782] Elapsed 1m 4s (remain 1m 1s) Loss: 0.0762(0.0970) Grad: 88392.7109  LR: 0.00000111
Epoch: [5][500/782] Elapsed 1m 22s (remain 0m 46s) Loss: 0.1342(0.0981) Grad: 81885.9453  LR: 0.00000111
Epoch: [5][600/782] Elapsed 1m 38s (remain 0m 29s) Loss: 0.0927(0.0985) Grad: 60105.4062  LR: 0.00000111
Epoch: [5][700/782] Elapsed 1m 54s (remain 0m 13s) Loss: 0.1489(0.0991) Grad: 178822.4062  LR: 0.00000111
Epoch: [5][781/782] Elapsed 2m 7s (remain 0m 0s) Loss: 0.0882(0.0997) Grad: 115216.1328  LR: 0.00000267
EVAL: [0/98] Elapsed 0m 0s (remain 0m 36s) Loss: 0.0907(0.0907)
EVAL: [97/98] Elapsed 0m 19s (remain 0m 0s) Loss: 0.0822(0.1174)
[2022-10-25 05:28:03] - Epoch 5 - avg_train_loss: 0.0997  avg_val_loss: 0.1174  time: 148s
[2022-10-25 05:28:03] - Epoch 5 - Score: 0.4856  Scores: [0.5338069821552281, 0.4773957811611756, 0.44499270868045643, 0.4501509595228759, 0.5250133286702447, 0.48209263736484753]
[2022-10-25 05:28:03] - ========== fold: 2 result ==========
[2022-10-25 05:28:03] - Score: 0.4715  Scores: [0.5075608746431222, 0.4715608019569918, 0.43041145824193744, 0.44939445947505385, 0.49430824425855624, 0.4755901235535031]
[2022-10-25 05:28:03] - ========== fold: 3 training ==========
[2022-10-25 05:28:03] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/782] Elapsed 0m 0s (remain 9m 1s) Loss: 2.8435(2.8435) Grad: inf  LR: 0.00000299
Epoch: [1][100/782] Elapsed 0m 17s (remain 1m 59s) Loss: 0.3559(1.7622) Grad: 172797.5312  LR: 0.00000299
Epoch: [1][200/782] Elapsed 0m 35s (remain 1m 41s) Loss: 0.1617(0.9915) Grad: 100615.3750  LR: 0.00000299
Epoch: [1][300/782] Elapsed 0m 51s (remain 1m 22s) Loss: 0.1473(0.7141) Grad: 177809.3281  LR: 0.00000299
Epoch: [1][400/782] Elapsed 1m 6s (remain 1m 3s) Loss: 0.1463(0.5711) Grad: 147035.4844  LR: 0.00000299
Epoch: [1][500/782] Elapsed 1m 24s (remain 0m 47s) Loss: 0.1535(0.4815) Grad: 65733.2969  LR: 0.00000299
Epoch: [1][600/782] Elapsed 1m 40s (remain 0m 30s) Loss: 0.1054(0.4235) Grad: 121489.6953  LR: 0.00000299
Epoch: [1][700/782] Elapsed 1m 57s (remain 0m 13s) Loss: 0.0663(0.3814) Grad: 83768.1484  LR: 0.00000299
Epoch: [1][781/782] Elapsed 2m 10s (remain 0m 0s) Loss: 0.1389(0.3543) Grad: 182481.1406  LR: 0.00000225
EVAL: [0/98] Elapsed 0m 0s (remain 0m 41s) Loss: 0.1702(0.1702)
EVAL: [97/98] Elapsed 0m 19s (remain 0m 0s) Loss: 0.1015(0.1167)
[2022-10-25 05:30:36] - Epoch 1 - avg_train_loss: 0.3543  avg_val_loss: 0.1167  time: 150s
[2022-10-25 05:30:36] - Epoch 1 - Score: 0.4842  Scores: [0.5207012845482046, 0.46688477475363505, 0.45305051769356514, 0.4829718660540287, 0.5200963740412019, 0.46133966806321]
[2022-10-25 05:30:36] - Epoch 1 - Save Best Score: 0.4842 Model
Epoch: [2][0/782] Elapsed 0m 0s (remain 6m 3s) Loss: 0.1095(0.1095) Grad: 467602.6875  LR: 0.00000233
Epoch: [2][100/782] Elapsed 0m 16s (remain 1m 51s) Loss: 0.0837(0.1075) Grad: 175706.0469  LR: 0.00000233
Epoch: [2][200/782] Elapsed 0m 34s (remain 1m 38s) Loss: 0.1237(0.1108) Grad: 239737.3125  LR: 0.00000233
Epoch: [2][300/782] Elapsed 0m 51s (remain 1m 22s) Loss: 0.1087(0.1133) Grad: 229102.2812  LR: 0.00000233
Epoch: [2][400/782] Elapsed 1m 7s (remain 1m 4s) Loss: 0.0598(0.1145) Grad: 156414.7969  LR: 0.00000233
Epoch: [2][500/782] Elapsed 1m 22s (remain 0m 46s) Loss: 0.0485(0.1135) Grad: 73067.9375  LR: 0.00000233
Epoch: [2][600/782] Elapsed 1m 40s (remain 0m 30s) Loss: 0.0764(0.1139) Grad: 120868.8594  LR: 0.00000233
Epoch: [2][700/782] Elapsed 1m 56s (remain 0m 13s) Loss: 0.1468(0.1150) Grad: 76050.7734  LR: 0.00000233
Epoch: [2][781/782] Elapsed 2m 9s (remain 0m 0s) Loss: 0.1472(0.1163) Grad: 39550.5547  LR: 0.00000070
EVAL: [0/98] Elapsed 0m 0s (remain 0m 42s) Loss: 0.1556(0.1556)
EVAL: [97/98] Elapsed 0m 19s (remain 0m 0s) Loss: 0.0845(0.1110)
[2022-10-25 05:33:09] - Epoch 2 - avg_train_loss: 0.1163  avg_val_loss: 0.1110  time: 150s
[2022-10-25 05:33:09] - Epoch 2 - Score: 0.4717  Scores: [0.5034273670701248, 0.46097300524470636, 0.4309337350147465, 0.47105147528805075, 0.5140527750068237, 0.44956887861577116]
[2022-10-25 05:33:09] - Epoch 2 - Save Best Score: 0.4717 Model
Epoch: [3][0/782] Elapsed 0m 0s (remain 5m 33s) Loss: 0.1074(0.1074) Grad: inf  LR: 0.00000078
Epoch: [3][100/782] Elapsed 0m 17s (remain 1m 55s) Loss: 0.0645(0.1211) Grad: 38861.0352  LR: 0.00000078
Epoch: [3][200/782] Elapsed 0m 34s (remain 1m 39s) Loss: 0.1935(0.1187) Grad: 179018.7500  LR: 0.00000078
Epoch: [3][300/782] Elapsed 0m 51s (remain 1m 21s) Loss: 0.1094(0.1194) Grad: 76797.5938  LR: 0.00000078
Epoch: [3][400/782] Elapsed 1m 8s (remain 1m 4s) Loss: 0.0768(0.1168) Grad: 76953.1953  LR: 0.00000078
Epoch: [3][500/782] Elapsed 1m 23s (remain 0m 47s) Loss: 0.1095(0.1155) Grad: 82482.3516  LR: 0.00000078
Epoch: [3][600/782] Elapsed 1m 40s (remain 0m 30s) Loss: 0.0799(0.1139) Grad: 96482.0156  LR: 0.00000078
Epoch: [3][700/782] Elapsed 1m 57s (remain 0m 13s) Loss: 0.0540(0.1138) Grad: 62827.7188  LR: 0.00000078
Epoch: [3][781/782] Elapsed 2m 9s (remain 0m 0s) Loss: 0.1311(0.1133) Grad: 110525.1172  LR: 0.00000013
EVAL: [0/98] Elapsed 0m 0s (remain 0m 47s) Loss: 0.1578(0.1578)
EVAL: [97/98] Elapsed 0m 19s (remain 0m 0s) Loss: 0.0936(0.1168)
[2022-10-25 05:35:42] - Epoch 3 - avg_train_loss: 0.1133  avg_val_loss: 0.1168  time: 149s
[2022-10-25 05:35:42] - Epoch 3 - Score: 0.4846  Scores: [0.514938607985053, 0.48657458995537456, 0.4618585452610817, 0.4665536960640873, 0.5160110624063948, 0.4617278606036659]
Epoch: [4][0/782] Elapsed 0m 0s (remain 5m 41s) Loss: 0.1583(0.1583) Grad: 572107.2500  LR: 0.00000011
Epoch: [4][100/782] Elapsed 0m 16s (remain 1m 51s) Loss: 0.1113(0.1050) Grad: 313959.7500  LR: 0.00000011
Epoch: [4][200/782] Elapsed 0m 34s (remain 1m 40s) Loss: 0.1255(0.1052) Grad: 90897.8906  LR: 0.00000011
Epoch: [4][300/782] Elapsed 0m 51s (remain 1m 23s) Loss: 0.0827(0.1067) Grad: 112129.6094  LR: 0.00000011
Epoch: [4][400/782] Elapsed 1m 8s (remain 1m 5s) Loss: 0.0592(0.1077) Grad: 40523.3008  LR: 0.00000011
Epoch: [4][500/782] Elapsed 1m 24s (remain 0m 47s) Loss: 0.1347(0.1069) Grad: 101263.2891  LR: 0.00000011
Epoch: [4][600/782] Elapsed 1m 40s (remain 0m 30s) Loss: 0.0656(0.1068) Grad: 72839.0781  LR: 0.00000011
Epoch: [4][700/782] Elapsed 1m 56s (remain 0m 13s) Loss: 0.0921(0.1063) Grad: 126474.8672  LR: 0.00000011
Epoch: [4][781/782] Elapsed 2m 9s (remain 0m 0s) Loss: 0.1079(0.1074) Grad: 77247.7891  LR: 0.00000119
EVAL: [0/98] Elapsed 0m 0s (remain 0m 46s) Loss: 0.1963(0.1963)
EVAL: [97/98] Elapsed 0m 19s (remain 0m 0s) Loss: 0.1032(0.1311)
[2022-10-25 05:38:12] - Epoch 4 - avg_train_loss: 0.1074  avg_val_loss: 0.1311  time: 150s
[2022-10-25 05:38:12] - Epoch 4 - Score: 0.5136  Scores: [0.5100171241038763, 0.49909315977508706, 0.49430358164972293, 0.5270992943245602, 0.5808034979018195, 0.4702734721892509]
Epoch: [5][0/782] Elapsed 0m 0s (remain 4m 53s) Loss: 0.0646(0.0646) Grad: 172089.7188  LR: 0.00000111
Epoch: [5][100/782] Elapsed 0m 17s (remain 1m 55s) Loss: 0.0822(0.1027) Grad: 178876.7188  LR: 0.00000111
Epoch: [5][200/782] Elapsed 0m 34s (remain 1m 38s) Loss: 0.0976(0.1028) Grad: 254074.7031  LR: 0.00000111
Epoch: [5][300/782] Elapsed 0m 50s (remain 1m 20s) Loss: 0.0558(0.1028) Grad: 153722.8594  LR: 0.00000111
Epoch: [5][400/782] Elapsed 1m 7s (remain 1m 3s) Loss: 0.0912(0.1032) Grad: 139220.2812  LR: 0.00000111
Epoch: [5][500/782] Elapsed 1m 24s (remain 0m 47s) Loss: 0.0488(0.1042) Grad: 56076.8516  LR: 0.00000111
Epoch: [5][600/782] Elapsed 1m 40s (remain 0m 30s) Loss: 0.0850(0.1035) Grad: 193755.4375  LR: 0.00000111
Epoch: [5][700/782] Elapsed 1m 57s (remain 0m 13s) Loss: 0.0754(0.1029) Grad: 155599.4062  LR: 0.00000111
Epoch: [5][781/782] Elapsed 2m 10s (remain 0m 0s) Loss: 0.0758(0.1025) Grad: 157856.7188  LR: 0.00000267
EVAL: [0/98] Elapsed 0m 0s (remain 0m 45s) Loss: 0.1999(0.1999)
EVAL: [97/98] Elapsed 0m 19s (remain 0m 0s) Loss: 0.0952(0.1188)
[2022-10-25 05:40:43] - Epoch 5 - avg_train_loss: 0.1025  avg_val_loss: 0.1188  time: 151s
[2022-10-25 05:40:43] - Epoch 5 - Score: 0.4873  Scores: [0.4938706928745403, 0.4979375701579072, 0.42383345990565213, 0.5175979275400544, 0.5481588199948338, 0.4426083636643746]
[2022-10-25 05:40:43] - ========== fold: 3 result ==========
[2022-10-25 05:40:43] - Score: 0.4717  Scores: [0.5034273670701248, 0.46097300524470636, 0.4309337350147465, 0.47105147528805075, 0.5140527750068237, 0.44956887861577116]
[2022-10-25 05:40:43] - ========== fold: 4 training ==========
[2022-10-25 05:40:43] - DebertaV2Config {
  "_name_or_path": "microsoft/deberta-v3-base",
  "attention_dropout": 0.0,
  "attention_probs_dropout_prob": 0.0,
  "hidden_act": "gelu",
  "hidden_dropout": 0.0,
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-07,
  "max_position_embeddings": 512,
  "max_relative_positions": -1,
  "model_type": "deberta-v2",
  "norm_rel_ebd": "layer_norm",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "output_hidden_states": true,
  "pad_token_id": 0,
  "pooler_dropout": 0,
  "pooler_hidden_act": "gelu",
  "pooler_hidden_size": 768,
  "pos_att_type": [
    "p2c",
    "c2p"
  ],
  "position_biased_input": false,
  "position_buckets": 256,
  "relative_attention": true,
  "share_att_key": true,
  "transformers_version": "4.21.1",
  "type_vocab_size": 0,
  "vocab_size": 128100
}
Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']
- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Epoch: [1][0/782] Elapsed 0m 0s (remain 5m 32s) Loss: 3.4627(3.4627) Grad: inf  LR: 0.00000299
Epoch: [1][100/782] Elapsed 0m 16s (remain 1m 53s) Loss: 1.4018(2.0916) Grad: 234413.3906  LR: 0.00000299
Epoch: [1][200/782] Elapsed 0m 33s (remain 1m 36s) Loss: 0.3679(1.1684) Grad: 131475.9219  LR: 0.00000299
Epoch: [1][300/782] Elapsed 0m 50s (remain 1m 20s) Loss: 0.1522(0.8283) Grad: 247816.4375  LR: 0.00000299
Epoch: [1][400/782] Elapsed 1m 6s (remain 1m 3s) Loss: 0.1337(0.6550) Grad: 134216.6406  LR: 0.00000299
Epoch: [1][500/782] Elapsed 1m 22s (remain 0m 46s) Loss: 0.0821(0.5492) Grad: 107505.9609  LR: 0.00000299
Epoch: [1][600/782] Elapsed 1m 39s (remain 0m 29s) Loss: 0.0683(0.4785) Grad: 74495.9531  LR: 0.00000299
Epoch: [1][700/782] Elapsed 1m 56s (remain 0m 13s) Loss: 0.1158(0.4290) Grad: 188447.5625  LR: 0.00000299
Epoch: [1][781/782] Elapsed 2m 10s (remain 0m 0s) Loss: 0.1446(0.3962) Grad: 196831.9688  LR: 0.00000225
EVAL: [0/98] Elapsed 0m 0s (remain 1m 9s) Loss: 0.0726(0.0726)
EVAL: [97/98] Elapsed 0m 19s (remain 0m 0s) Loss: 0.0833(0.1244)
[2022-10-25 05:43:15] - Epoch 1 - avg_train_loss: 0.3962  avg_val_loss: 0.1244  time: 150s
[2022-10-25 05:43:15] - Epoch 1 - Score: 0.5010  Scores: [0.5364862241258824, 0.4730831605574477, 0.4715481690386334, 0.4961949440445649, 0.524113311338423, 0.5045171560358311]
[2022-10-25 05:43:15] - Epoch 1 - Save Best Score: 0.5010 Model
Epoch: [2][0/782] Elapsed 0m 0s (remain 5m 18s) Loss: 0.1109(0.1109) Grad: 436710.2188  LR: 0.00000233
Epoch: [2][100/782] Elapsed 0m 16s (remain 1m 52s) Loss: 0.0408(0.1153) Grad: 117748.3203  LR: 0.00000233
Epoch: [2][200/782] Elapsed 0m 33s (remain 1m 35s) Loss: 0.1160(0.1135) Grad: 329723.0938  LR: 0.00000233
Epoch: [2][300/782] Elapsed 0m 50s (remain 1m 20s) Loss: 0.0801(0.1142) Grad: 122108.8984  LR: 0.00000233
Epoch: [2][400/782] Elapsed 1m 6s (remain 1m 3s) Loss: 0.0616(0.1140) Grad: 262494.3750  LR: 0.00000233
Epoch: [2][500/782] Elapsed 1m 22s (remain 0m 46s) Loss: 0.1128(0.1150) Grad: 102781.1797  LR: 0.00000233
Epoch: [2][600/782] Elapsed 1m 39s (remain 0m 29s) Loss: 0.1034(0.1158) Grad: 119106.4922  LR: 0.00000233
Epoch: [2][700/782] Elapsed 1m 56s (remain 0m 13s) Loss: 0.1311(0.1156) Grad: 89114.3984  LR: 0.00000233
Epoch: [2][781/782] Elapsed 2m 9s (remain 0m 0s) Loss: 0.0760(0.1162) Grad: 84900.1094  LR: 0.00000070
EVAL: [0/98] Elapsed 0m 0s (remain 1m 8s) Loss: 0.0756(0.0756)
EVAL: [97/98] Elapsed 0m 19s (remain 0m 0s) Loss: 0.0840(0.1119)
[2022-10-25 05:45:48] - Epoch 2 - avg_train_loss: 0.1162  avg_val_loss: 0.1119  time: 149s
[2022-10-25 05:45:48] - Epoch 2 - Score: 0.4743  Scores: [0.500547875848977, 0.4544402789524996, 0.43899289452517065, 0.4777519360419279, 0.5040974438432351, 0.4697351851697595]
[2022-10-25 05:45:48] - Epoch 2 - Save Best Score: 0.4743 Model
Epoch: [3][0/782] Elapsed 0m 0s (remain 5m 55s) Loss: 0.1696(0.1696) Grad: 393693.7500  LR: 0.00000078
Epoch: [3][100/782] Elapsed 0m 17s (remain 2m 0s) Loss: 0.0951(0.1121) Grad: 125169.8281  LR: 0.00000078
Epoch: [3][200/782] Elapsed 0m 34s (remain 1m 40s) Loss: 0.0728(0.1095) Grad: 182928.2656  LR: 0.00000078
Epoch: [3][300/782] Elapsed 0m 50s (remain 1m 21s) Loss: 0.0626(0.1071) Grad: 128994.3750  LR: 0.00000078
Epoch: [3][400/782] Elapsed 1m 8s (remain 1m 4s) Loss: 0.0962(0.1081) Grad: 123775.0312  LR: 0.00000078
Epoch: [3][500/782] Elapsed 1m 25s (remain 0m 47s) Loss: 0.0682(0.1069) Grad: 187799.1562  LR: 0.00000078
Epoch: [3][600/782] Elapsed 1m 41s (remain 0m 30s) Loss: 0.0757(0.1066) Grad: 122179.5156  LR: 0.00000078
Epoch: [3][700/782] Elapsed 1m 57s (remain 0m 13s) Loss: 0.0689(0.1060) Grad: 161539.9375  LR: 0.00000078
Epoch: [3][781/782] Elapsed 2m 10s (remain 0m 0s) Loss: 0.1419(0.1062) Grad: 215778.0312  LR: 0.00000013
EVAL: [0/98] Elapsed 0m 0s (remain 1m 2s) Loss: 0.0612(0.0612)
EVAL: [97/98] Elapsed 0m 19s (remain 0m 0s) Loss: 0.0758(0.1093)
[2022-10-25 05:48:21] - Epoch 3 - avg_train_loss: 0.1062  avg_val_loss: 0.1093  time: 149s
[2022-10-25 05:48:21] - Epoch 3 - Score: 0.4686  Scores: [0.4959361831374841, 0.44792161295230204, 0.43449502285821257, 0.46547100698025334, 0.5017089658194301, 0.4662144028112464]
[2022-10-25 05:48:21] - Epoch 3 - Save Best Score: 0.4686 Model
Epoch: [4][0/782] Elapsed 0m 0s (remain 4m 15s) Loss: 0.1333(0.1333) Grad: inf  LR: 0.00000011
Epoch: [4][100/782] Elapsed 0m 18s (remain 2m 2s) Loss: 0.2008(0.1052) Grad: 326897.5000  LR: 0.00000011
Epoch: [4][200/782] Elapsed 0m 33s (remain 1m 37s) Loss: 0.1375(0.1082) Grad: 118582.2891  LR: 0.00000011
Epoch: [4][300/782] Elapsed 0m 51s (remain 1m 22s) Loss: 0.0655(0.1065) Grad: 212290.0156  LR: 0.00000011
Epoch: [4][400/782] Elapsed 1m 7s (remain 1m 4s) Loss: 0.0503(0.1046) Grad: 169792.2344  LR: 0.00000011
Epoch: [4][500/782] Elapsed 1m 24s (remain 0m 47s) Loss: 0.1117(0.1049) Grad: 355175.9375  LR: 0.00000011
Epoch: [4][600/782] Elapsed 1m 40s (remain 0m 30s) Loss: 0.0456(0.1050) Grad: 70879.0000  LR: 0.00000011
Epoch: [4][700/782] Elapsed 1m 57s (remain 0m 13s) Loss: 0.1194(0.1044) Grad: 372527.7188  LR: 0.00000011
Epoch: [4][781/782] Elapsed 2m 9s (remain 0m 0s) Loss: 0.0612(0.1048) Grad: 114903.5234  LR: 0.00000119
EVAL: [0/98] Elapsed 0m 0s (remain 1m 1s) Loss: 0.0810(0.0810)
EVAL: [97/98] Elapsed 0m 18s (remain 0m 0s) Loss: 0.1062(0.1155)
Epoch: [5][0/782] Elapsed 0m 0s (remain 7m 10s) Loss: 0.1406(0.1406) Grad: inf  LR: 0.00000111
[2022-10-25 05:50:54] - Epoch 4 - avg_train_loss: 0.1048  avg_val_loss: 0.1155  time: 149s
[2022-10-25 05:50:54] - Epoch 4 - Score: 0.4817  Scores: [0.5390586836822007, 0.44417530133131866, 0.4454678241232185, 0.4898320353749363, 0.4918878008773442, 0.4795219743669652]
Epoch: [5][100/782] Elapsed 0m 17s (remain 1m 59s) Loss: 0.1533(0.0980) Grad: 182875.4688  LR: 0.00000111
Epoch: [5][200/782] Elapsed 0m 34s (remain 1m 38s) Loss: 0.1870(0.0990) Grad: 467802.2188  LR: 0.00000111
Epoch: [5][300/782] Elapsed 0m 49s (remain 1m 19s) Loss: 0.1046(0.1021) Grad: 203286.7344  LR: 0.00000111
Epoch: [5][400/782] Elapsed 1m 6s (remain 1m 3s) Loss: 0.0858(0.1010) Grad: 271367.8438  LR: 0.00000111
Epoch: [5][500/782] Elapsed 1m 23s (remain 0m 47s) Loss: 0.0890(0.1016) Grad: 154628.0156  LR: 0.00000111
Epoch: [5][600/782] Elapsed 1m 41s (remain 0m 30s) Loss: 0.1066(0.1020) Grad: 236053.7188  LR: 0.00000111
Epoch: [5][700/782] Elapsed 1m 58s (remain 0m 13s) Loss: 0.1188(0.1016) Grad: 151533.4844  LR: 0.00000111
Epoch: [5][781/782] Elapsed 2m 11s (remain 0m 0s) Loss: 0.1196(0.1017) Grad: 316760.5938  LR: 0.00000267
EVAL: [0/98] Elapsed 0m 0s (remain 1m 12s) Loss: 0.0619(0.0619)
EVAL: [97/98] Elapsed 0m 19s (remain 0m 0s) Loss: 0.0677(0.1101)
[2022-10-25 05:53:25] - Epoch 5 - avg_train_loss: 0.1017  avg_val_loss: 0.1101  time: 151s
[2022-10-25 05:53:25] - Epoch 5 - Score: 0.4701  Scores: [0.5156949448455004, 0.4475381001895925, 0.4328396444471539, 0.46558838728333507, 0.49141165471021236, 0.46726097341825223]
[2022-10-25 05:53:26] - ========== fold: 4 result ==========
[2022-10-25 05:53:26] - Score: 0.4686  Scores: [0.4959361831374841, 0.44792161295230204, 0.43449502285821257, 0.46547100698025334, 0.5017089658194301, 0.4662144028112464]
[2022-10-25 05:53:26] - ========== CV ==========
[2022-10-25 05:53:26] - Score: 0.4690  Scores: [0.5012819527317298, 0.4616345420637816, 0.4272421265968959, 0.46435120383738576, 0.49924408073658383, 0.46000868771786874]